{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pprint\n",
    "# import sys\n",
    "\n",
    "# pprint.pprint(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fundamental libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/mnt/c/Users/44yos/RacePrediction/resnet_win5')\n",
    "from os import path\n",
    "import time\n",
    "\n",
    "# preporcessing libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt \n",
    "from pickle import dump\n",
    "\n",
    "# tesndorflow libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from tensorflow.python.keras import optimizers, callbacks\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "# from models import resnet\n",
    "from models import transformer\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# from utils import functions\n",
    "from utils import create_time_series_data, smooth_label, categorical_focal_loss, order_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race_id                        int64\n",
      "race_round                     int64\n",
      "ground_condition               int64\n",
      "total_horse_number             int64\n",
      "order                          int64\n",
      "frame_number                   int64\n",
      "horse_number                   int64\n",
      "age                            int64\n",
      "burden_weight                float64\n",
      "goal_time                    float64\n",
      "half_order                   float64\n",
      "last_time                    float64\n",
      "odds                         float64\n",
      "horse_weight                 float64\n",
      "pop                          float64\n",
      "race_rank                      int64\n",
      "distance                       int64\n",
      "ground_type_ダ                  int64\n",
      "ground_type_芝                  int64\n",
      "circle_右                       int64\n",
      "circle_左                       int64\n",
      "weather_circumstance_小雨        int64\n",
      "weather_circumstance_小雪        int64\n",
      "weather_circumstance_晴         int64\n",
      "weather_circumstance_曇         int64\n",
      "weather_circumstance_雨         int64\n",
      "weather_circumstance_雪         int64\n",
      "place_中京                       int64\n",
      "place_中山                       int64\n",
      "place_京都                       int64\n",
      "place_函館                       int64\n",
      "place_小倉                       int64\n",
      "place_新潟                       int64\n",
      "place_札幌                       int64\n",
      "place_東京                       int64\n",
      "place_福島                       int64\n",
      "place_阪神                       int64\n",
      "sex_セ                          int64\n",
      "sex_牝                          int64\n",
      "sex_牡                          int64\n",
      "horse_weight_dif             float64\n",
      "f_grass_win_rate             float64\n",
      "f_dart_win_rate              float64\n",
      "f_win_rate                   float64\n",
      "g_f_grass_win_rate           float64\n",
      "g_f_dart_win_rate            float64\n",
      "g_f_win_rate                 float64\n",
      "m_grass_win_rate             float64\n",
      "m_dart_win_rate              float64\n",
      "m_win_rate                   float64\n",
      "whole_horse_number_1         float64\n",
      "odds_1                       float64\n",
      "order_1                        int64\n",
      "burden_weight_1              float64\n",
      "race_distance_1              float64\n",
      "ground_condition_1           float64\n",
      "goal_time_1                  float64\n",
      "half_order_1                 float64\n",
      "last_time_1                  float64\n",
      "horse_weight_1               float64\n",
      "weather_circumstance_小雨_1    float64\n",
      "weather_circumstance_小雪_1    float64\n",
      "weather_circumstance_晴_1     float64\n",
      "weather_circumstance_曇_1     float64\n",
      "weather_circumstance_雨_1     float64\n",
      "weather_circumstance_雪_1     float64\n",
      "main_place_その他_1             float64\n",
      "main_place_中京_1              float64\n",
      "main_place_中山_1              float64\n",
      "main_place_京都_1              float64\n",
      "main_place_函館_1              float64\n",
      "main_place_小倉_1              float64\n",
      "main_place_新潟_1              float64\n",
      "main_place_札幌_1              float64\n",
      "main_place_東京_1              float64\n",
      "main_place_福島_1              float64\n",
      "main_place_阪神_1              float64\n",
      "race_rank_1                  float64\n",
      "ground_type_ダ_1              float64\n",
      "ground_type_芝_1              float64\n",
      "ground_type_障_1              float64\n",
      "horse_weight_dif_1           float64\n",
      "same_jockey_1                float64\n",
      "whole_horse_number_2         float64\n",
      "odds_2                       float64\n",
      "order_2                        int64\n",
      "burden_weight_2              float64\n",
      "race_distance_2              float64\n",
      "ground_condition_2           float64\n",
      "goal_time_2                  float64\n",
      "half_order_2                 float64\n",
      "last_time_2                  float64\n",
      "horse_weight_2               float64\n",
      "weather_circumstance_小雨_2    float64\n",
      "weather_circumstance_小雪_2    float64\n",
      "weather_circumstance_晴_2     float64\n",
      "weather_circumstance_曇_2     float64\n",
      "weather_circumstance_雨_2     float64\n",
      "weather_circumstance_雪_2     float64\n",
      "main_place_その他_2             float64\n",
      "main_place_中京_2              float64\n",
      "main_place_中山_2              float64\n",
      "main_place_京都_2              float64\n",
      "main_place_函館_2              float64\n",
      "main_place_小倉_2              float64\n",
      "main_place_新潟_2              float64\n",
      "main_place_札幌_2              float64\n",
      "main_place_東京_2              float64\n",
      "main_place_福島_2              float64\n",
      "main_place_阪神_2              float64\n",
      "race_rank_2                  float64\n",
      "ground_type_ダ_2              float64\n",
      "ground_type_芝_2              float64\n",
      "ground_type_障_2              float64\n",
      "horse_weight_dif_2           float64\n",
      "same_jockey_2                float64\n",
      "whole_horse_number_3         float64\n",
      "odds_3                       float64\n",
      "order_3                        int64\n",
      "burden_weight_3              float64\n",
      "race_distance_3              float64\n",
      "ground_condition_3           float64\n",
      "goal_time_3                  float64\n",
      "half_order_3                 float64\n",
      "last_time_3                  float64\n",
      "horse_weight_3               float64\n",
      "weather_circumstance_小雨_3    float64\n",
      "weather_circumstance_小雪_3    float64\n",
      "weather_circumstance_晴_3     float64\n",
      "weather_circumstance_曇_3     float64\n",
      "weather_circumstance_雨_3     float64\n",
      "weather_circumstance_雪_3     float64\n",
      "main_place_その他_3             float64\n",
      "main_place_中京_3              float64\n",
      "main_place_中山_3              float64\n",
      "main_place_京都_3              float64\n",
      "main_place_函館_3              float64\n",
      "main_place_小倉_3              float64\n",
      "main_place_新潟_3              float64\n",
      "main_place_札幌_3              float64\n",
      "main_place_東京_3              float64\n",
      "main_place_福島_3              float64\n",
      "main_place_阪神_3              float64\n",
      "race_rank_3                  float64\n",
      "ground_type_ダ_3              float64\n",
      "ground_type_芝_3              float64\n",
      "ground_type_障_3              float64\n",
      "horse_weight_dif_3           float64\n",
      "same_jockey_3                float64\n",
      "same_jockey                  float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# # load data\n",
    "data = pd.read_csv(\"../data/csv/data.csv\", sep = \",\")\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217032\n"
     ]
    }
   ],
   "source": [
    "data.dropna(inplace=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215967\n"
     ]
    }
   ],
   "source": [
    "data.drop_duplicates(inplace=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adjust columns type\n",
    "data['race_id'] = data['race_id'].astype(str)\n",
    "data['order'] = data['order'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete race day information\n",
    "data.drop([\"horse_number\", \"half_order\", \"goal_time\" ,\"last_time\", \"horse_weight\", \"horse_weight_dif\", \"frame_number\",\"pop\"], axis = 1, inplace=True)\n",
    "# \"race_round\",\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # standarlization \n",
    "# no_scale_data = data[['race_id','order']]\n",
    "# scale_columns = data.drop(['race_id','order'], axis=1).columns.values\n",
    "# standard_scale = StandardScaler()\n",
    "# data = pd.DataFrame(standard_scale.fit_transform(data[scale_columns]))\n",
    "\n",
    "# # PCA\n",
    "# pca = PCA()\n",
    "# data = pd.DataFrame(pca.fit_transform(data))\n",
    "# contrb_rate = pd.DataFrame(pca.explained_variance_ratio_, columns = ['rate'])\n",
    "# sum_rate = 0\n",
    "\n",
    "# #  # to get the colum of the specific contribution rate\n",
    "# # for i in range(len(contrb_rate)):\n",
    "# #     sum_rate += contrb_rate.rate[i]\n",
    "# #     if sum_rate >= 0.9:\n",
    "# #         max_col = i + 1\n",
    "# #         break\n",
    "\n",
    "max_col = 84\n",
    "# # print(max_col)\n",
    "# data = data.loc[:, :max_col-1]\n",
    "# print(data.shape[1])\n",
    "# # print(data.head(5))\n",
    "# # print(len(data), len(no_scale_data))\n",
    "# # print(no_scale_data[no_scale_data['race_id'].isnull()])\n",
    "# data = pd.concat([data, no_scale_data], axis=1)\n",
    "# dump(standard_scale, open(\"standard_scale.pkl\", \"wb\"))\n",
    "# dump(pca, open(\"pca.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sorted(no_scale_data['order'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.shape)\n",
    "# print(data.dtypes)\n",
    "# print(data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_time_series_data(raw_data):\n",
    "#     number_of_race = raw_data.race_id.nunique()\n",
    "#     time_series_data = np.full((number_of_race, 24, max_col), 0.0)#-float('inf')\n",
    "#     label = np.full((number_of_race, 24), 25)\n",
    "#     race_number = 0\n",
    "#     horse_number = 0\n",
    "#     for i in range(len(raw_data)):\n",
    "#         if i == 0:\n",
    "#             label[race_number][horse_number] = float(raw_data.iloc[i].order)\n",
    "#             time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "#             horse_number += 1\n",
    "#             continue\n",
    "#         # add new race\n",
    "#         if data.iloc[i].race_id != data.iloc[i-1].race_id:\n",
    "#             race_number += 1\n",
    "#             horse_number = 0\n",
    "#             label[race_number][horse_number] = float(raw_data.iloc[i].order)\n",
    "#             time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "#             horse_number += 1\n",
    "#         # add new horse to the same race\n",
    "#         else:\n",
    "# #             print(data.iloc[i].race_id ,race_number, horse_number)\n",
    "#             label[race_number][horse_number] = float(raw_data.iloc[i].order)\n",
    "#             time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "#             horse_number += 1\n",
    "#     del raw_data\n",
    "#     return time_series_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20132, 24, 84)\n",
      "(20132, 24)\n"
     ]
    }
   ],
   "source": [
    "# X, y_order = create_time_series_data(data)\n",
    "# np.save('X', X)\n",
    "# np.save('y_order', y_order)\n",
    "X = np.load('X.npy')\n",
    "y_order = np.load('y_order.npy')\n",
    "# del data\n",
    "print(X.shape)\n",
    "print(y_order.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "X = X.astype('float32')\n",
    "print(X.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[ 7 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25]\n"
     ]
    }
   ],
   "source": [
    "print(X[0][11])\n",
    "print(y_order[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26,)\n",
      "[0.00000000e+00 1.21687621e+00 1.21555368e+00 1.23063757e+00\n",
      " 1.24041898e+00 1.25990362e+00 1.26100846e+00 1.28155834e+00\n",
      " 1.32325490e+00 1.37157651e+00 1.45736210e+00 1.56413643e+00\n",
      " 1.73521807e+00 1.98052140e+00 2.27608819e+00 2.71833648e+00\n",
      " 3.58156912e+00 1.49347181e+01 2.00918164e+01 1.25825000e+03\n",
      " 1.43800000e+03 2.51650000e+03 3.35533333e+03 4.02640000e+03\n",
      " 5.03300000e+03 7.53440294e-02]\n"
     ]
    }
   ],
   "source": [
    "alpha = len(y_order) / pd.DataFrame(y_order.flatten()).value_counts()\n",
    "alpha = alpha.sort_index()\n",
    "alpha = np.array(alpha)\n",
    "alpha = np.append(0,alpha)\n",
    "print(alpha.shape)\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]]\n",
      "(20132, 24, 26)\n",
      "(20132, 24, 84)\n"
     ]
    }
   ],
   "source": [
    "# creating X,y (parameters and target)\n",
    "y = np_utils.to_categorical(y_order, dtype='float32')\n",
    "print(y[5])\n",
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def smooth_label(label, factor=0.03):\n",
    "#     # smooth label\n",
    "#     label *= (1 - factor)\n",
    "# #     label[:,:,1:4] += (factor / 3)\n",
    "\n",
    "#     for i in range(label.shape[0]):\n",
    "#         for j in range(label.shape[1]):\n",
    "#             t = np.where(label[i][j] == 1 - factor)\n",
    "#             label[i,j,max(0,t[0][0]-1):min(26,t[0][0]+2)] += (factor / 3)\n",
    "#     return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.   0.   0.   0.01 0.98 0.01 0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.01 0.98 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]]\n",
      "(20132, 24, 26)\n",
      "(20132, 24, 84)\n"
     ]
    }
   ],
   "source": [
    "y = smooth_label(y) \n",
    "print(y[4])\n",
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.race_id.value_counts().plot.hist(bins=25,range=(1,25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAD4CAYAAADRuPC7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAWpklEQVR4nO3df8yd5X3f8fenmMQkgZSfGfFjZgioDaCUBEOQkm1JUTClKpAtdEZT8RZSZ5kjEa1/BKJqoESWwtSEDrHQEWEFWBIgP2FrCHVJ1qxSCpgMlV9htoobHozAxSiQLkDsfPfHuR44No8fn4f4en6+X9LRuc/33Nf9XBdH8OG+rvvcJ1WFJEn726/NdgckSQuTASNJ6sKAkSR1YcBIkrowYCRJXSyZ7Q7MFUcccUStWLFitrshSfPKfffd9w9VdeRk7xkwzYoVK9i0adNsd0OS5pUkf7+395wikyR1YcBIkrowYCRJXbgGM4Vf/OIXjI+P88ILL8x2V6a0dOlSxsbGOPDAA2e7K5L0MgNmCuPj4xx88MGsWLGCJLPdnUlVFc888wzj4+Mce+yxs90dSXqZU2RTeOGFFzj88MPnbLgAJOHwww+f82dZkhYfA2Yf5nK4TJgPfZS0+BgwkqQuXIOZhhWX/vl+Pd7Wz/7uSPt997vf5ZJLLmHXrl185CMf4dJLL92v/ZCkHgyYOW7Xrl2sW7eOjRs3MjY2xmmnnca5557LiSeeONtdkzRHTfd/hkf9n93pcopsjrvnnns4/vjjOe6443jd617H6tWrue2222a7W5K0TwbMHPfEE0+wfPnyl1+PjY3xxBNPzGKPJGk0BswcV1WvqnnVmKT5wICZ48bGxnj88cdffj0+Ps5b3/rWWeyRJI3GgJnjTjvtNDZv3sxjjz3GSy+9xM0338y55547292SpH3yKrJp6HWlxVSWLFnCNddcw6pVq9i1axcf/vCHOemkk2a8H5I0XQbMPHDOOedwzjnnzHY3JGlanCKTJHVhwEiSuugWMEmWJ/l+kkeSPJTkkla/IskTSe5vj3OG2lyWZEuSR5OsGqqfmuSB9t7VadfpJnl9klta/e4kK4barEmyuT3WvNZxTHaZ8FwzH/ooafHpeQazE/ijqno7cAawLsnE/U2uqqpT2uM7AO291cBJwNnAF5Ic0Pa/FlgLnNAeZ7f6xcCzVXU8cBVwZTvWYcDlwLuB04HLkxw63QEsXbqUZ555Zk7/B3zi92CWLl06212RpN10W+SvqieBJ9v280keAZZN0eQ84OaqehF4LMkW4PQkW4FDquqHAEluBM4H7mhtrmjtvw5c085uVgEbq2pHa7ORQSh9dTpjGBsbY3x8nO3bt0+n2Yyb+EVLSZpLZuQqsjZ19U7gbuA9wMeTXARsYnCW8yyD8PmboWbjrfaLtr1nnfb8OEBV7UzyU+Dw4fokbYb7tZbBmRHHHHPMq/p94IEH+iuRkvQadV/kT/Im4BvAJ6rqOQbTXW8DTmFwhvO5iV0naV5T1F9rm1cKVddV1cqqWnnkkUdOOQ5J0vR0DZgkBzIIly9X1TcBquqpqtpVVb8EvshgjQQGZxnLh5qPAdtafWyS+m5tkiwB3gzsmOJYkqQZ0vMqsgDXA49U1eeH6kcP7fZB4MG2fTuwul0ZdiyDxfx72lrO80nOaMe8CLhtqM3EFWIfAr5XgxX5O4GzkhzaFvfPajVJ0gzpuQbzHuAPgAeS3N9qnwIuTHIKgymrrcBHAarqoSS3Ag8zuAJtXVXtau0+BnwJOIjB4v4drX49cFO7IGAHg6vQqKodST4D3Nv2+/TEgr8kaWb0vIrsr5l8LeQ7U7RZD6yfpL4JOHmS+gvABXs51gZgw6j9lSTtX36TX5LUhQEjSerCgJEkdWHASJK6MGAkSV0YMJKkLgwYSVIXBowkqQsDRpLUhQEjSerCgJEkdWHASJK6MGAkSV0YMJKkLgwYSVIXBowkqQsDRpLUhQEjSerCgJEkdWHASJK6MGAkSV0YMJKkLgwYSVIXBowkqQsDRpLUhQEjSerCgJEkdWHASJK6MGAkSV0YMJKkLroFTJLlSb6f5JEkDyW5pNUPS7Ixyeb2fOhQm8uSbEnyaJJVQ/VTkzzQ3rs6SVr99UluafW7k6wYarOm/Y3NSdb0GqckaXI9z2B2An9UVW8HzgDWJTkRuBS4q6pOAO5qr2nvrQZOAs4GvpDkgHasa4G1wAntcXarXww8W1XHA1cBV7ZjHQZcDrwbOB24fDjIJEn9dQuYqnqyqn7Utp8HHgGWAecBN7TdbgDOb9vnATdX1YtV9RiwBTg9ydHAIVX1w6oq4MY92kwc6+vAme3sZhWwsap2VNWzwEZeCSVJ0gyYkTWYNnX1TuBu4C1V9SQMQgg4qu22DHh8qNl4qy1r23vWd2tTVTuBnwKHT3GsPfu1NsmmJJu2b9/+2gcoSXqV7gGT5E3AN4BPVNVzU+06Sa2mqL/WNq8Uqq6rqpVVtfLII4+comuSpOnqGjBJDmQQLl+uqm+28lNt2ov2/HSrjwPLh5qPAdtafWyS+m5tkiwB3gzsmOJYkqQZ0vMqsgDXA49U1eeH3rodmLiqaw1w21B9dbsy7FgGi/n3tGm055Oc0Y550R5tJo71IeB7bZ3mTuCsJIe2xf2zWk2SNEOWdDz2e4A/AB5Icn+rfQr4LHBrkouBnwAXAFTVQ0luBR5mcAXauqra1dp9DPgScBBwR3vAIMBuSrKFwZnL6nasHUk+A9zb9vt0Ve3oNVBJ0qt1C5iq+msmXwsBOHMvbdYD6yepbwJOnqT+Ai2gJnlvA7Bh1P5KkvYvv8kvSerCgJEkdWHASJK6MGAkSV0YMJKkLgwYSVIXBowkqQsDRpLUhQEjSerCgJEkdWHASJK6MGAkSV0YMJKkLgwYSVIXBowkqQsDRpLUhQEjSepipIBJ8qpfk5QkaSqjnsH8WZJ7kvyHJL/etUeSpAVhpICpqvcC/wZYDmxK8pUkH+jaM0nSvDbyGkxVbQb+GPgk8C+Aq5P8OMm/7NU5SdL8NeoazDuSXAU8Avw28HtV9fa2fVXH/kmS5qklI+53DfBF4FNV9fOJYlVtS/LHXXomSZrXRg2Yc4CfV9UugCS/Biytqv9XVTd1650kad4adQ3mL4GDhl6/odUkSZrUqAGztKp+NvGibb+hT5ckSQvBqAHzj0neNfEiyanAz6fYX5K0yI26BvMJ4GtJtrXXRwP/uk+XJEkLwUgBU1X3JvlN4DeAAD+uql907ZkkaV6bzs0uTwPeAbwTuDDJRVPtnGRDkqeTPDhUuyLJE0nub49zht67LMmWJI8mWTVUPzXJA+29q5Ok1V+f5JZWvzvJiqE2a5Jsbo810xijJGk/GfWLljcBfwK8l0HQnAas3EezLwFnT1K/qqpOaY/vtOOfCKwGTmptvpDkgLb/tcBa4IT2mDjmxcCzVXU8gy97XtmOdRhwOfBu4HTg8iSHjjJOSdL+M+oazErgxKqqUQ9cVT8YPqvYh/OAm6vqReCxJFuA05NsBQ6pqh8CJLkROB+4o7W5orX/OnBNO7tZBWysqh2tzUYGofTVUfsuSfrVjTpF9iDwT/bT3/x4kr9tU2gTZxbLgMeH9hlvtWVte8/6bm2qaifwU+DwKY4lSZpBowbMEcDDSe5McvvE4zX8vWuBtwGnAE8Cn2v1TLJvTVF/rW12k2Rtkk1JNm3fvn2qfkuSpmnUKbIr9scfq6qnJraTfBH4n+3lOIOfApgwBmxr9bFJ6sNtxpMsAd4M7Gj19+3R5n/tpT/XAdcBrFy5cuTpP0nSvo36ezB/BWwFDmzb9wI/mu4fS3L00MsPMph6A7gdWN2uDDuWwWL+PVX1JPB8kjPa+spFwG1DbSauEPsQ8L22RnQncFaSQ9sU3FmtJkmaQSOdwST5QwZXch3GYIprGfBnwJlTtPkqgzOJI5KMM7iy631JTmEwZbUV+ChAVT2U5FbgYWAnsG7ixprAxxhckXYQg8X9O1r9euCmdkHADgZXoVFVO5J8hkEIAnx6YsFfkjRzRp0iW8fgkt+7YfDjY0mOmqpBVV04Sfn6KfZfD6yfpL4JOHmS+gvABXs51gZgw1T9kyT1Neoi/4tV9dLEi7bm4ZqFJGmvRg2Yv0ryKeCgJB8Avgb8j37dkiTNd6MGzKXAduABBusm3wH8JUtJ0l6NerPLXzL4yeQv9u2OJGmhGPUqsseYZM2lqo7b7z2SJC0I07kX2YSlDK7eOmz/d0eStFCM+kXLZ4YeT1TVnwK/3blvkqR5bNQpsncNvfw1Bmc0B3fpkSRpQRh1iuxzQ9s7GXwL//f3e28kSQvGqFeRvb93RyRJC8uoU2T/car3q+rz+6c7kqSFYjpXkZ3G4A7GAL8H/IDdf9hLkqSXjRowRwDvqqrnAZJcAXytqj7Sq2OSpPlt1FvFHAO8NPT6JWDFfu+NJGnBGPUM5ibgniTfYvCN/g8CN3brlSRp3hv1KrL1Se4A/lkr/buq+j/9uiVJmu9GnSIDeAPwXFX9F2C8/bSxJEmTGilgklwOfBK4rJUOBP57r05Jkua/Uc9gPgicC/wjQFVtw1vFSJKmMGrAvFRVRbtlf5I39uuSJGkhGDVgbk3y34BfT/KHwF/ij49Jkqawz6vIkgS4BfhN4DngN4D/VFUbO/dNkjSP7TNgqqqSfLuqTgUMFUnSSEadIvubJKd17YkkaUEZ9Zv87wf+fZKtDK4kC4OTm3f06pgkaX6bMmCSHFNVPwF+Z4b6I0laIPZ1BvNtBndR/vsk36iqfzUTnZIkzX/7WoPJ0PZxPTsiSVpY9hUwtZdtSZKmtK8pst9K8hyDM5mD2ja8ssh/SNfeSZLmrSnPYKrqgKo6pKoOrqolbXvi9ZThkmRDkqeTPDhUOyzJxiSb2/OhQ+9dlmRLkkeTrBqqn5rkgfbe1e2LnyR5fZJbWv3uJCuG2qxpf2NzkjXT/8ciSfpVTed2/dP1JeDsPWqXAndV1QnAXe01SU4EVgMntTZfSHJAa3MtsBY4oT0mjnkx8GxVHQ9cBVzZjnUYcDnwbuB04PLhIJMkzYxuAVNVPwB27FE+D7ihbd8AnD9Uv7mqXqyqx4AtwOlJjgYOqaoftptt3rhHm4ljfR04s53drAI2VtWOqnqWwd0H9gw6SVJnPc9gJvOWqnoSoD0f1erLgMeH9htvtWVte8/6bm2qaifwU+DwKY71KknWJtmUZNP27dt/hWFJkvY00wGzN5mkVlPUX2ub3YtV11XVyqpaeeSRR47UUUnSaGY6YJ5q016056dbfRxYPrTfGLCt1ccmqe/WJskS4M0MpuT2dixJ0gya6YC5HZi4qmsNcNtQfXW7MuxYBov597RptOeTnNHWVy7ao83EsT4EfK+t09wJnJXk0La4f1arSZJm0Kg3u5y2JF8F3gcckWScwZVdn2Xw42UXAz8BLgCoqoeS3Ao8DOwE1lXVrnaojzG4Iu0g4I72ALgeuCnJFgZnLqvbsXYk+Qxwb9vv01W158UGkqTOugVMVV24l7fO3Mv+64H1k9Q3ASdPUn+BFlCTvLcB2DByZyVJ+91cWeSXJC0wBowkqQsDRpLUhQEjSerCgJEkdWHASJK6MGAkSV0YMJKkLgwYSVIXBowkqQsDRpLUhQEjSerCgJEkdWHASJK6MGAkSV0YMJKkLgwYSVIXBowkqQsDRpLUhQEjSerCgJEkdWHASJK6MGAkSV0YMJKkLgwYSVIXBowkqQsDRpLUhQEjSerCgJEkdWHASJK6mJWASbI1yQNJ7k+yqdUOS7Ixyeb2fOjQ/pcl2ZLk0SSrhuqntuNsSXJ1krT665Pc0up3J1kx02OUpMVuNs9g3l9Vp1TVyvb6UuCuqjoBuKu9JsmJwGrgJOBs4AtJDmhtrgXWAie0x9mtfjHwbFUdD1wFXDkD45EkDZlLU2TnATe07RuA84fqN1fVi1X1GLAFOD3J0cAhVfXDqirgxj3aTBzr68CZE2c3kqSZMVsBU8BfJLkvydpWe0tVPQnQno9q9WXA40Ntx1ttWdves75bm6raCfwUOHzPTiRZm2RTkk3bt2/fLwOTJA0smaW/+56q2pbkKGBjkh9Pse9kZx41RX2qNrsXqq4DrgNYuXLlq96XJL12s3IGU1Xb2vPTwLeA04Gn2rQX7fnptvs4sHyo+RiwrdXHJqnv1ibJEuDNwI4eY5EkTW7GAybJG5McPLENnAU8CNwOrGm7rQFua9u3A6vblWHHMljMv6dNoz2f5Iy2vnLRHm0mjvUh4HttnUaSNENmY4rsLcC32pr7EuArVfXdJPcCtya5GPgJcAFAVT2U5FbgYWAnsK6qdrVjfQz4EnAQcEd7AFwP3JRkC4Mzl9UzMTBJ0itmPGCq6u+A35qk/gxw5l7arAfWT1LfBJw8Sf0FWkBJkmbHXLpMWZK0gBgwkqQuDBhJUhcGjCSpCwNGktSFASNJ6sKAkSR1YcBIkrowYCRJXRgwkqQuDBhJUhcGjCSpCwNGktSFASNJ6sKAkSR1YcBIkrowYCRJXRgwkqQuDBhJUhcGjCSpiyWz3YGFYsWlfz7bXZjztn72d2e7C5JmkAGjGTPXQtjAk/oyYLRoTTfwDCRpegwYaUSv5QzMUNJiZsBIHXmWpMXMgJHmEANJC4kBI81jBpLmMgNGWkQMJM0kA0bSXhlI+lX4TX5JUhcLOmCSnJ3k0SRbklw62/2RpMVkwQZMkgOA/wr8DnAicGGSE2e3V5K0eCzkNZjTgS1V9XcASW4GzgMentVeSQvYTNwOyHWe+WMhB8wy4PGh1+PAu4d3SLIWWNte/izJo237COAfuvdwblrMY4fFPf55MfZc2e3Q82L8PeTKX2ns/3RvbyzkgMkktdrtRdV1wHWvaphsqqqVvTo2ly3mscPiHv9iHjss7vH3GvuCXYNhcMayfOj1GLBtlvoiSYvOQg6Ye4ETkhyb5HXAauD2We6TJC0aC3aKrKp2Jvk4cCdwALChqh4asfmrps0WkcU8dljc41/MY4fFPf4uY09V7XsvSZKmaSFPkUmSZpEBI0nqwoAZsthvLZNka5IHktyfZNNs96e3JBuSPJ3kwaHaYUk2Jtncng+dzT72spexX5Hkifb535/knNnsYy9Jlif5fpJHkjyU5JJWX/Cf/RRj7/LZuwbTtFvL/F/gAwwucb4XuLCqFs03/5NsBVZW1aL4slmSfw78DLixqk5utf8M7Kiqz7b/yTi0qj45m/3sYS9jvwL4WVX9yWz2rbckRwNHV9WPkhwM3AecD/xbFvhnP8XYf58On71nMK94+dYyVfUSMHFrGS1QVfUDYMce5fOAG9r2DQz+5Vtw9jL2RaGqnqyqH7Xt54FHGNz5Y8F/9lOMvQsD5hWT3Vqm2z/4OaqAv0hyX7uNzmL0lqp6Egb/MgJHzXJ/ZtrHk/xtm0JbcFNEe0qyAngncDeL7LPfY+zQ4bM3YF6xz1vLLALvqap3MbgD9bo2jaLF41rgbcApwJPA52a3O30leRPwDeATVfXcbPdnJk0y9i6fvQHzikV/a5mq2taenwa+xWDacLF5qs1TT8xXPz3L/ZkxVfVUVe2qql8CX2QBf/5JDmTwH9gvV9U3W3lRfPaTjb3XZ2/AvGJR31omyRvboh9J3gicBTw4dasF6XZgTdteA9w2i32ZURP/cW0+yAL9/JMEuB54pKo+P/TWgv/s9zb2Xp+9V5ENaZfm/Smv3Fpm/Sx3acYkOY7BWQsMbiH0lYU+/iRfBd7H4DbtTwGXA98GbgWOAX4CXFBVC24xfC9jfx+DKZICtgIfnViTWEiSvBf438ADwC9b+VMM1iIW9Gc/xdgvpMNnb8BIkrpwikyS1IUBI0nqwoCRJHVhwEiSujBgJEldGDCSpC4MGElSF/8fZMgEahV8994AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(pd.DataFrame(y_order.flatten()).plot.hist(bins=25))## ,ylim=(0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.01, random_state = 0)\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.0841513e+00  7.0771635e-02 -5.8418870e-01  2.4646287e-01\n",
      "  1.8285137e+00 -1.1262701e+00  6.5735537e-01  1.5329328e+00\n",
      "  6.2066919e-01 -6.7069030e-01  1.0873965e+00  1.7795300e-01\n",
      " -2.2035263e+00 -5.5455178e-01  1.5312845e+00  1.9849390e+00\n",
      " -1.9881687e+00 -4.5285341e-01 -1.1557992e+00  3.0845302e-01\n",
      "  6.5695131e-01 -1.6801572e+00  1.2509111e-01 -6.3480353e-01\n",
      "  2.6424465e-01 -1.8350914e+00  1.3959821e+00 -1.1001785e+00\n",
      " -2.4194989e+00  2.9623857e+00  1.4317343e+00  1.9310854e-02\n",
      "  3.4215423e-01  7.4966168e-01 -1.0088371e+00  8.0969352e-01\n",
      " -2.5986239e-01 -8.4669787e-01  1.0779321e+00  6.1363038e-02\n",
      " -1.5148355e+00 -1.8475902e-03 -8.8226789e-01 -6.8742210e-01\n",
      " -2.6793274e-01  1.6574528e+00 -2.0822718e+00 -1.7538213e+00\n",
      "  4.2922177e+00  1.6022534e+00 -4.4454589e-01 -6.7625672e-01\n",
      " -8.8914499e-02 -1.1668312e-01 -3.3394665e-01 -7.5860035e-01\n",
      "  3.7061727e-01 -6.4284933e-01  3.2509527e-01  4.8748016e-01\n",
      "  1.5954223e+00  1.4406800e-01 -8.7014019e-01  6.6753399e-01\n",
      " -1.0578674e+00  4.2397591e-01  6.7731267e-01  3.3324012e-01\n",
      " -1.4028546e+00 -1.2829390e+00 -9.2539579e-01  3.5360447e-01\n",
      " -5.5841304e-02  2.7825090e-01  1.1097189e-01  1.2788044e+00\n",
      "  4.5809287e-01  1.7082896e+00 -6.1947507e-01  1.6214646e+00\n",
      " -8.2937258e-01  6.4784966e-02  9.0706927e-01 -1.0770866e+00]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19930, 24, 84)\n",
      "(202, 24, 84)\n",
      "(19930, 24, 26)\n",
      "(202, 24, 26)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def categorical_focal_loss(alpha, gamma):\n",
    "#     \"\"\"\n",
    "#     Softmax version of focal loss.\n",
    "#     When there is a skew between different categories/labels in your data set, you can try to apply this function as a\n",
    "#     loss.\n",
    "#            m\n",
    "#       FL = ∑  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n",
    "#           c=1\n",
    "#       where m = number of classes, c = class and o = observation\n",
    "#     Parameters:\n",
    "#       alpha -- the same as weighing factor in balanced cross entropy. Alpha is used to specify the weight of different\n",
    "#       categories/labels, the size of the array needs to be consistent with the number of classes.\n",
    "#       gamma -- focusing parameter for modulating factor (1-p)\n",
    "#     Default value:\n",
    "#       gamma -- 2.0 as mentioned in the paper\n",
    "#       alpha -- 0.25 as mentioned in the paper\n",
    "#     References:\n",
    "#         Official paper: https://arxiv.org/pdf/1708.02002.pdf\n",
    "#         https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy\n",
    "#     Usage:\n",
    "#      model.compile(loss=[categorical_focal_loss(alpha=[[.25, .25, .25]], gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "#     \"\"\"\n",
    "\n",
    "#     alpha = np.array(alpha, dtype=np.float32)\n",
    "\n",
    "#     def categorical_focal_loss_fixed(y_true, y_pred):\n",
    "#         \"\"\"\n",
    "#         :param y_true: A tensor of the same shape as `y_pred`\n",
    "#         :param y_pred: A tensor resulting from a softmax\n",
    "#         :return: Output tensor.\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Clip the prediction value to prevent NaN's and Inf's\n",
    "#         epsilon = K.epsilon()\n",
    "#         y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "#         # Calculate Cross Entropy\n",
    "#         cross_entropy = -y_true * K.log(y_pred)\n",
    "\n",
    "#         # Calculate Focal Loss\n",
    "#         loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
    "\n",
    "#         # Compute mean loss in mini_batch\n",
    "#         return K.mean(K.sum(loss, axis=-1))\n",
    "\n",
    "#     return categorical_focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataset with batch size\n",
    "batch_size = 2048 # hyperparameter\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=19930).batch(batch_size)\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "valid_dataset = valid_dataset.shuffle(buffer_size=202).batch(batch_size)\n",
    "\n",
    "\n",
    "# del X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters\n",
    "num_layers = 2 # hyperparameter\n",
    "d_model = max_col # 4*35 84=4*3*7\n",
    "num_heads = 28 # hyperparameter *must be a factor of d_model*\n",
    "d_ffn = 256 # hyperparameter\n",
    "pe_input = 24\n",
    "target_size = 26\n",
    "dropout_rate = 0.1 # hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# set up model\n",
    "# outputs_list = [[pe_input,target_size],[pe_input, d_model]]\n",
    "trans_race = transformer.TransRace(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ffn=d_ffn,\n",
    "    pe_input=pe_input,\n",
    "    target_size=target_size,\n",
    "    rate=dropout_rate,\n",
    "#     outputs=outputs_list,\n",
    ")\n",
    "opt = optimizers.Adam(decay=0.01)\n",
    "loss = categorical_focal_loss(alpha=[alpha], gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method TransRace.call of <models.transformer.TransRace object at 0x7fa077daaf98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TransRace.call of <models.transformer.TransRace object at 0x7fa077daaf98>>, which Python reported as:\n",
      "    def call(self, inputs, training):\n",
      "        inp = inputs\n",
      "\n",
      "        enc_padding_mask = self.create_masks(inp) #, tar\n",
      "        # , look_ahead_mask, dec_padding_mask\n",
      "\n",
      "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
      "#         print(\"enc output shape\", enc_output.shape)\n",
      "\n",
      "        final_output = self.final_layer(enc_output)\n",
      "#         print(\"final output shape\", final_output.shape)\n",
      "        \n",
      "        return final_output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method TransRace.call of <models.transformer.TransRace object at 0x7fa077daaf98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TransRace.call of <models.transformer.TransRace object at 0x7fa077daaf98>>, which Python reported as:\n",
      "    def call(self, inputs, training):\n",
      "        inp = inputs\n",
      "\n",
      "        enc_padding_mask = self.create_masks(inp) #, tar\n",
      "        # , look_ahead_mask, dec_padding_mask\n",
      "\n",
      "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
      "#         print(\"enc output shape\", enc_output.shape)\n",
      "\n",
      "        final_output = self.final_layer(enc_output)\n",
      "#         print(\"final output shape\", final_output.shape)\n",
      "        \n",
      "        return final_output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae22e10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae22e10>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae22e10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae22e10>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae41f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae41f28>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae41f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae41f28>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:From /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 10 steps, validate on 1 steps\n",
      "Epoch 1/10\n",
      "10/10 [==============================] - 9s 949ms/step - loss: 3.6960 - acc: 0.0336 - val_loss: 3.5913 - val_acc: 0.0530\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 1s 141ms/step - loss: 3.0791 - acc: 0.0621 - val_loss: 3.0922 - val_acc: 0.1042\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.8481 - acc: 0.0979 - val_loss: 2.9915 - val_acc: 0.1788\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 2.7471 - acc: 0.1022 - val_loss: 2.9324 - val_acc: 0.1856\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 2.6714 - acc: 0.1186 - val_loss: 2.7927 - val_acc: 0.2391\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.6323 - acc: 0.1391 - val_loss: 2.7314 - val_acc: 0.2900\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.6099 - acc: 0.1735 - val_loss: 2.5981 - val_acc: 0.3129\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.5570 - acc: 0.1874 - val_loss: 2.5412 - val_acc: 0.3490\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.5246 - acc: 0.2212 - val_loss: 2.5344 - val_acc: 0.3608\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.5248 - acc: 0.2494 - val_loss: 2.5094 - val_acc: 0.3927\n",
      "Model: \"trans_race\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Encoder)            multiple                  144488    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             multiple                  2210      \n",
      "=================================================================\n",
      "Total params: 146,698\n",
      "Trainable params: 146,698\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "trans_race.compile(\n",
    "    optimizer=opt,\n",
    "    loss=loss,#'categorical_crossentropy', # 'sigmoid_focal_crossentropy'\n",
    "    metrics=['accuracy'], #['categorical_accuracy']\n",
    ")\n",
    "\n",
    "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath='../models/results/transformer.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "history = trans_race.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=10,\n",
    "    verbose=True, # hide the output because we have so many epochs\n",
    "    callbacks=[model_checkpoint_callback]\n",
    ")\n",
    "print(trans_race.summary())\n",
    "# trans_race.save_weights(\"../models/results/transformer.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f9fcc78a400>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot \n",
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "# for loss\n",
    "axL.plot(history.history['loss'],label=\"loss for training\")\n",
    "axL.plot(history.history['val_loss'],label=\"loss for validation\")\n",
    "axL.set_title('model loss')\n",
    "axL.set_xlabel('epoch')\n",
    "axL.set_ylabel('loss')\n",
    "axL.legend(loc='upper right')\n",
    "axR.plot(history.history['acc'],label=\"acc for training\")\n",
    "axR.plot(history.history['val_acc'],label=\"acc for validation\")\n",
    "axR.set_title('model accuracy')\n",
    "axR.set_xlabel('epoch')\n",
    "axR.set_ylabel('accuracy')\n",
    "axR.legend(loc='lower right')\n",
    "# figureの保存\n",
    "# plt.savefig(\"../models/results/trans_race_result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae22e10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae22e10>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae22e10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae22e10>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae41f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae41f28>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae41f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae41f28>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-e4cef426dca2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdummy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans_race\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train_xgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_train_xgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my_train_xgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_xgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "# predict (transformer + XGBoost)\n",
    "\n",
    "dummy, hidden = trans_race.predict(X_train, training=False)\n",
    "X_train_xgb = hidden.reshape(-1,d_model)\n",
    "y_train_xgb = y_train.reshape(-1,target_size)\n",
    "y_train_xgb = np.argmax(y_train_xgb, axis=1)\n",
    "print(X_train_xgb.shape)\n",
    "print(y_train_xgb.shape)\n",
    "d_train = xgb.DMatrix(X_train_xgb, label=y_train_xgb)\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(objective=\"multi:softmax\", random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:55:14] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1631904754241/work/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=42, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.fit(X_train_xgb, y_train_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae22e10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae22e10>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae22e10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae22e10>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae41f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae41f28>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae41f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae41f28>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "(4848, 84)\n"
     ]
    }
   ],
   "source": [
    "dummy, hidden = trans_race.predict(X_valid, training=False)\n",
    "X_valid_xgb = X_valid.reshape(-1,d_model)\n",
    "# y_valid_xgb = y_valid.reshape(-1,target_size)\n",
    "# y_valid_xgb = np.argmax(y_valid_xgb, axis=1)\n",
    "print(X_valid_xgb.shape)\n",
    "d_valid = xgb.DMatrix(X_valid_xgb)\n",
    "\n",
    "preds = xgb_model.predict(X_valid_xgb)\n",
    "y_pred = preds.reshape(-1, pe_input)\n",
    "y_ans = np.argmax(y_valid, axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202, 24)\n",
      "(202, 24)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)\n",
    "print(y_ans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae22e10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae22e10>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae22e10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae22e10>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae22f28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae41f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae41f28>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae41f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fa07ae41f28>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fa07ae41f98>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot convert a symbolic Tensor (dense_12_27/truediv:0) to a numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-446203ff84b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpreds_trans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans_race\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_trans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_ans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m     \"\"\"\n\u001b[0;32m-> 1188\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    734\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m     raise NotImplementedError(\"Cannot convert a symbolic Tensor ({}) to a numpy\"\n\u001b[0;32m--> 736\u001b[0;31m                               \" array.\".format(self.name))\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot convert a symbolic Tensor (dense_12_27/truediv:0) to a numpy array."
     ]
    }
   ],
   "source": [
    "# predict (transformer)\n",
    "\n",
    "preds_trans, fea = trans_race.predict(X_valid, training=False)\n",
    "y_pred = np.argmax(preds_trans, axis = 2)\n",
    "y_ans = np.argmax(y_valid, axis = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot convert a symbolic Tensor (ArgMax:0) to a numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-aa328bff1b75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_ans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_ans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    734\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m     raise NotImplementedError(\"Cannot convert a symbolic Tensor ({}) to a numpy\"\n\u001b[0;32m--> 736\u001b[0;31m                               \" array.\".format(self.name))\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot convert a symbolic Tensor (ArgMax:0) to a numpy array."
     ]
    }
   ],
   "source": [
    "y_pred = np.array(y_pred)\n",
    "print(y_pred.shape)\n",
    "print(y_ans.shape)\n",
    "print(type(y_pred))\n",
    "print(type(y_ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.7\n"
     ]
    }
   ],
   "source": [
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def order_algorithm(preds):\n",
    "#     num_race = preds.shape[0]\n",
    "#     y_preds = np.full((num_race, 24), 25)\n",
    "#     for i in range(num_race):\n",
    "#         one_race = preds[i,:,:]\n",
    "#         init_preds = np.argmax(one_race, axis = -1)\n",
    "#         exist_horse = np.delete(one_race, np.where(init_preds == 25)[0], 0)\n",
    "#         for j in range(1,exist_horse.shape[0]+1):\n",
    "#             one_order = np.argmax(exist_horse[:,j])\n",
    "#             for k in range(one_race.shape[0]):\n",
    "#                 if np.array_equal(one_race[k], exist_horse[one_order]):\n",
    "#                     y_preds[i][k] = j\n",
    "#                     exist_horse = np.delete(exist_horse, one_order, 0)\n",
    "#                     break\n",
    "#     return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.DataFrame(preds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def order_algorithm(preds):\n",
    "#     num_race = preds.shape[0]\n",
    "#     y_preds = np.full((num_race, 24), 25)\n",
    "#     for i in range(num_race): # iterate all race\n",
    "#         one_race = preds[i,:,:] # shape = (24, 26) ,so (num of horse, num of target 0-25)\n",
    "#         init_preds = np.argmax(one_race, axis = -1)\n",
    "#         exist_horse = np.delete(one_race, np.where(init_preds == 25)[0], 0) # shape = (num of exist horse, 26)\n",
    "#         for j in range(1,exist_horse.shape[0]+1): # iterate 1-num of exist horse\n",
    "#             one_order = np.argmax(exist_horse[:,j]) # this is a target order\n",
    "#             for k in range(one_race.shape[0]): # search the horse k = (0, 23)\n",
    "#                 if np.array_equal(one_race[k], exist_horse[one_order]):\n",
    "#                     y_preds[i][k] = j\n",
    "#                     exist_horse = np.delete(exist_horse, one_order, 0)\n",
    "#                     exist_horse[:,j+1] += exist_horse[:,j]\n",
    "#                     one_race[:,j+1] += one_race[:,j]\n",
    "#                     break\n",
    "#     return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot convert a symbolic Tensor (strided_slice_27:0) to a numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-e61af3e96668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morder_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/44yos/RacePrediction/resnet_win5/training/utils.py\u001b[0m in \u001b[0;36morder_algorithm\u001b[0;34m(preds)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_race\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# iterate all race\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mone_race\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# shape = (24, 26) ,so (num of horse, num of target 0-25)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0minit_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_race\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (24,1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;31m#         print(one_race)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;31m#         print(init_preds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m     \"\"\"\n\u001b[0;32m-> 1188\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    734\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m     raise NotImplementedError(\"Cannot convert a symbolic Tensor ({}) to a numpy\"\n\u001b[0;32m--> 736\u001b[0;31m                               \" array.\".format(self.name))\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot convert a symbolic Tensor (strided_slice_27:0) to a numpy array."
     ]
    }
   ],
   "source": [
    "y_preds = order_algorithm(preds)\n",
    "print(y_preds.shape)\n",
    "print(y_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202, 24, 26)\n",
      "(202, 24)\n",
      "(202, 24)\n"
     ]
    }
   ],
   "source": [
    "print(preds.shape)\n",
    "print(y_preds.shape)\n",
    "print(y_ans.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11  4  5 12  8  1  2  9  6 10 25 25 25 25 25 25 25 25 25 25 25 25 25 25]\n",
      "[ 5  4  8  7  3  1  2  6  9 10 25 25 25 25 25 25 25 25 25 25 25 25 25 25]\n",
      "[ 1  1  6  8  3  2  6  5 10  6 25 25 25 25 25 25 25 25 25 25 25 25 25 25]\n"
     ]
    }
   ],
   "source": [
    "print(y_ans[0])\n",
    "print(y_preds[0])\n",
    "print(y_pred[0])\n",
    "# print(np.mean(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  13.450495049504951\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(y_preds.shape[0]):\n",
    "    for j in range(y_preds.shape[1]):\n",
    "        if (y_preds[i][j] == y_ans[i][j]):\n",
    "            correct += 1\n",
    "accuracy = correct / len(y_pred)\n",
    "print(\"accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the distribution of prediction\n",
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "label = ['0','1','2','3','4','5','6','7','8','9',' 10',' 11',' 12',' 13',' 14',' 15',' 16',' 17',' 18',' 19',' 20',' 21',' 22',' 23',' 24']\n",
    "\n",
    "axL.hist(y_preds.flatten(), bins = 25, label = \"pred_order\")##, range = (1,21)\n",
    "axL.set_xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24])\n",
    "axL.set_xticklabels(label)\n",
    "axL.set_title('pred order distribution')\n",
    "axL.legend()\n",
    "axR.hist(y_ans.flatten(), bins = 25, label = \"ans_order\")##, range = (1,21)\n",
    "axR.set_xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24])\n",
    "axR.set_xticklabels(label)\n",
    "axR.set_title('ans order distribution')\n",
    "axR.legend()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_f:  202\n",
      "correct_first:  83\n",
      "precision:  0.41089108910891087\n"
     ]
    }
   ],
   "source": [
    "# precision = TP / (TP + FP)\n",
    "# the accuracy of predected True\n",
    "i = 0\n",
    "correct_first = 0\n",
    "all_f = 0\n",
    "increase = 0\n",
    "# X_valid_inv = standard_scale.inverse_transform(X_valid)\n",
    "# X_valid_inv_df = pd.DataFrame(X_valid_inv)\n",
    "# odds = X_test_inv_df['odds'].values\n",
    "# hit_odds = []\n",
    "# select = []\n",
    "\n",
    "for i in range(y_preds.shape[0]):\n",
    "    for j in range(y_preds.shape[1]):\n",
    "        if (y_preds[i][j] == 1):  # total nubber of predicted 1st (TP + FP)  & (pred[i][1]*odds[i] > 1.0)) | (pred[i][1]> 0.25)\n",
    "            all_f += 1\n",
    "            if (y_ans[i][j] == 1) or (y_ans[i][j] == 2) or (y_ans[i][j] == 3):\n",
    "                correct_first += 1   #　True Positive\n",
    "            \n",
    "# for i in range(len(y_ans)):\n",
    "#     if (y_preds[i] == 1):  # total nubber of predicted 1st (TP + FP)  & (pred[i][1]*odds[i] > 1.0)) | (pred[i][1]> 0.25)\n",
    "#         all_f = all_f + 1\n",
    "#         if (y_ans[i] == 1):\n",
    "#             correct_first = correct_first + 1   #　True Positive\n",
    "# #             increase += odds[i]\n",
    "# #             hit_odds.append(odds[i])\n",
    "            \n",
    "            \n",
    "            \n",
    "print(\"all_f: \", all_f)\n",
    "print(\"correct_first: \", correct_first)\n",
    "# print(\"hit odds average: \", np.array(hit_odds).mean())\n",
    "# print(\"spent money:\", all_f * 100)\n",
    "# revenue = (increase - all_f) * 100\n",
    "# retrive = increase / all_f\n",
    " \n",
    "# print(\"retrive rate: \", retrive) \n",
    "# print(\"revenue: \", revenue)\n",
    "precision = correct_first / all_f\n",
    "print(\"precision: \",precision)\n",
    "#print(\"\\n\".join(map(str,hit_odds)))\n",
    "# print(\"min: \", min(hit_odds))\n",
    "# print(\"mid: \", np.median(np.array(hit_odds)))\n",
    "# print(\"max: \", max(hit_odds))\n",
    "\n",
    "# fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "# axL.hist(hit_odds, bins = 50, label = \"hit_odds\")\n",
    "# axL.set_title('hit odds distribution')\n",
    "# axL.legend()\n",
    "# axR.hist(odds, bins = 50, label = \"odds\", range = (0,40))\n",
    "# axR.set_title('all odds distribution')\n",
    "# axR.legend()\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.2138728323699422\n"
     ]
    }
   ],
   "source": [
    "# Recall = TP / (TP + FN)\n",
    "# the accuracy of label True\n",
    "i = 0\n",
    "correct_first = 0\n",
    "all_f = 0\n",
    "# odds_f = []\n",
    "p_rate_f = []\n",
    "\n",
    "# all_f_odds = []\n",
    "\n",
    "for i in range(y_preds.shape[0]):\n",
    "    for j in range(y_preds.shape[1]):\n",
    "        if (y_ans[i][j] == 1):  # total nubber of predicted 1st (TP + FP)  & (pred[i][1]*odds[i] > 1.0)) | (pred[i][1]> 0.25)\n",
    "            all_f += 1\n",
    "            if (y_preds[i][j] == 1):\n",
    "                correct_first += 1   #　True Positive\n",
    "                \n",
    "                \n",
    "# for i in range(len(y_ans)):\n",
    "#     if (y_ans[i] == 1):  # TP + FN\n",
    "#         all_f = all_f + 1\n",
    "# #         all_f_odds.append(odds[i])\n",
    "#         if (y_preds[i] == 1):\n",
    "#             correct_first = correct_first + 1   #　TP\n",
    "#             odds_f.append(odds[i])\n",
    "#             p_rate_f.append(pred[i][1])\n",
    "\n",
    "# fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "# axL.scatter(p_rate_f, odds_f)  \n",
    "# axL.set_title('correlation odss and prediction')\n",
    "# #axL.xlabel('prediction rate first')\n",
    "# #axL.ylabel('odds')\n",
    "# axR.hist(odds_f, bins = 50, label = \"odds\")\n",
    "# axR.set_title('all first odds distribution')\n",
    "# axR.legend()\n",
    "\n",
    "# fig.show()\n",
    "Recall = correct_first / all_f\n",
    "print(\"Recall: \",Recall)\n",
    "# print(\"all_f_odds average: \", np.array(all_f_odds).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision of 3 renpuku:  0.0558993399339934\n"
     ]
    }
   ],
   "source": [
    "# 3 renpuku\n",
    "within_3 = 0\n",
    "hit = 0\n",
    "\n",
    "for i in range(y_preds.shape[0]):\n",
    "    for j in range(y_preds.shape[1]):\n",
    "        within_3 += 1\n",
    "        if (y_ans[i][j] == 1) or (y_ans[i][j] == 2) or (y_ans[i][j] == 3):  # total nubber of predicted 1st (TP + FP)  & (pred[i][1]*odds[i] > 1.0)) | (pred[i][1]> 0.25)\n",
    "            if (y_preds[i][j] == 1) or (y_preds[i][j] == 2) or (y_preds[i][j] == 3) or (y_preds[i][j] == 4):\n",
    "                hit += 1   #　True Positive\n",
    "                \n",
    "precision = hit / within_3\n",
    "print(\"precision of 3 renpuku: \", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
