{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pprint\n",
    "# import sys\n",
    "\n",
    "# pprint.pprint(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "# pd.set_option('display.max_rows', 200)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/mnt/c/Users/44yos/RacePrediction/resnet_win5')\n",
    "from os import path\n",
    "import time\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import tensorflow as tf\n",
    "# from tensorflow.python import keras\n",
    "# from tensorflow.python.keras.models import Sequential\n",
    "# from tensorflow.python.keras.layers import Dense, Dropout\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from tensorflow.python.keras import optimizers\n",
    "# from tensorflow.python.keras.models import load_model\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "# from scipy.stats import norm\n",
    "\n",
    "from pickle import dump\n",
    "\n",
    "# from models import resnet\n",
    "from models import transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race_id                 int64\n",
      "race_round              int64\n",
      "ground_condition        int64\n",
      "total_horse_number      int64\n",
      "order                   int64\n",
      "                       ...   \n",
      "ground_type_芝_3       float64\n",
      "ground_type_障_3       float64\n",
      "horse_weight_dif_3    float64\n",
      "same_jockey_3         float64\n",
      "same_jockey           float64\n",
      "Length: 150, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = pd.read_csv(\"../data/csv/data.csv\", sep = \",\")\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust columns type\n",
    "data['race_id'] = data['race_id'].astype(str)\n",
    "# data['race_round'] = data['race_round'].astype(str)\n",
    "# #data['total_horse_number'] = data['total_horse_number'].astype(str)\n",
    "data['order'] = data['order'].astype(str)\n",
    "# data['frame_number'] = data['frame_number'].astype(str)\n",
    "# data['horse_number'] = data['horse_number'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete race day information\n",
    "data = data.drop([\"horse_number\", \"half_order\", \"goal_time\" ,\"last_time\", \"horse_weight\", \"horse_weight_dif\", \"frame_number\",\"pop\"], axis = 1)\n",
    "# \"race_round\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standarlization \n",
    "no_scale_data = data[['race_id','order']]\n",
    "scale_columns = data.drop(['race_id','order'], axis=1).columns.values\n",
    "standard_scale = StandardScaler()\n",
    "data = pd.DataFrame(standard_scale.fit_transform(data[scale_columns]))\n",
    "data = pd.concat([data, no_scale_data], axis=1)\n",
    "dump(standard_scale, open(\"standard_scale.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating time series data\n",
    "# def return_one_race(all_horse):\n",
    "#     print(len(all_horse))\n",
    "#     one_race = np.full((18, 139), -float('inf'))\n",
    "# #     print(len(one_race))\n",
    "# #     print(len(one_race[0]))\n",
    "#     for i, one_horse in all_horse.iterrows():\n",
    "#         print(i)\n",
    "# #         print(one_horse)\n",
    "#         one_race[i] = one_horse.drop(['race_id']).values\n",
    "#     print(one_race)\n",
    "#     return one_race\n",
    "\n",
    "# def create_time_series_data(raw_data):\n",
    "#     time_series_data = []\n",
    "#     time_series_data.append(raw_data.groupby(['race_id']).apply(return_one_race))\n",
    "#     return time_series_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92033, 142)\n",
      "0          float64\n",
      "1          float64\n",
      "2          float64\n",
      "3          float64\n",
      "4          float64\n",
      "            ...   \n",
      "137        float64\n",
      "138        float64\n",
      "139        float64\n",
      "race_id     object\n",
      "order       object\n",
      "Length: 142, dtype: object\n",
      "           0         1         2         3         4         5         6  \\\n",
      "0   0.434374 -0.509915 -0.220628  1.545825 -0.874906  2.530312 -0.385375   \n",
      "1   0.434374 -0.509915 -0.220628  0.089023 -0.874906 -0.643375 -0.385375   \n",
      "2   0.434374 -0.509915 -0.220628  0.817424 -0.255801 -0.554265 -0.385375   \n",
      "3   0.434374 -0.509915 -0.220628  1.545825  0.363305 -0.491203 -0.385375   \n",
      "4   0.434374 -0.509915 -0.220628  0.817424 -0.874906 -0.591280 -0.385375   \n",
      "5   0.434374 -0.509915 -0.220628  0.089023 -0.874906 -0.563862 -0.385375   \n",
      "6   0.434374 -0.509915 -0.220628  0.089023 -0.874906 -0.484348 -0.385375   \n",
      "7   0.434374 -0.509915 -0.220628  0.089023 -0.874906 -0.456930 -0.385375   \n",
      "8   0.434374 -0.509915 -0.220628  0.089023 -0.255801 -0.439108 -0.385375   \n",
      "9   0.434374 -0.509915 -0.220628  0.817424 -0.874906 -0.067587 -0.385375   \n",
      "10  0.434374 -0.509915 -0.220628  0.817424 -2.732221  0.099665 -0.385375   \n",
      "11  1.308833 -0.509915 -1.436765  0.089023 -0.255801 -0.574829 -0.385375   \n",
      "12  1.308833 -0.509915 -1.436765 -0.639378 -0.255801  0.180550 -0.385375   \n",
      "13  1.308833 -0.509915 -1.436765 -0.639378 -0.255801 -0.639262 -0.385375   \n",
      "14  1.308833 -0.509915 -1.436765 -1.367779 -2.113116 -0.562491 -0.385375   \n",
      "15  1.308833 -0.509915 -1.436765 -0.639378 -0.255801 -0.519992 -0.385375   \n",
      "16  1.308833 -0.509915 -1.436765 -0.639378 -0.255801  0.620616 -0.385375   \n",
      "17  1.308833 -0.509915 -1.436765 -1.367779 -2.113116 -0.314354 -0.385375   \n",
      "18  1.308833 -0.509915 -1.436765 -0.639378 -0.255801 -0.063475 -0.385375   \n",
      "19  1.308833 -0.509915 -1.436765 -0.639378 -0.255801 -0.618698 -0.385375   \n",
      "\n",
      "           7         8         9        10       11        12        13  \\\n",
      "0  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "1  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "2  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "3  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "4  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "5  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "6  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "7  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "8  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "9  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "10 -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "11  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "12  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "13  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "14  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "15  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "16  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "17  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "18  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "19  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "\n",
      "          14        15        16        17        18        19        20  \\\n",
      "0   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "1   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "2   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "3   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "4   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "5   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "6   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "7   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "8   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "9   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "10  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "11  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "12  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "13  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "14  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "15  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "16  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "17  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "18  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "19  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "\n",
      "          21        22        23       24        25        26        27  \\\n",
      "0  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "1  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "2  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "3  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "4  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "5  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "6  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "7  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "8  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "9  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "10 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "11 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "12 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "13 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "14 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "15 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "16 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "17 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "18 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "19 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "\n",
      "          28        29        30        31        32        33        34  \\\n",
      "0  -0.224288 -0.712530  0.790600 -0.149783 -0.464338 -0.226190 -0.291658   \n",
      "1  -0.224288  1.403449 -1.264862 -0.507064  0.494861 -0.509254 -0.291658   \n",
      "2  -0.224288 -0.712530  0.790600 -1.152477 -0.464338 -1.249982 -0.291658   \n",
      "3   4.458546 -0.712530 -1.264862 -1.152477 -0.464338 -1.249982 -0.291658   \n",
      "4  -0.224288  1.403449 -1.264862 -1.152477 -0.464338 -1.249982 -0.291658   \n",
      "5  -0.224288  1.403449 -1.264862 -0.762540  0.934495 -0.155093 -0.291658   \n",
      "6  -0.224288  1.403449 -1.264862 -1.152477 -0.464338 -1.249982 -0.291658   \n",
      "7  -0.224288  1.403449 -1.264862  0.767211 -0.464338  0.710099 -0.291658   \n",
      "8  -0.224288 -0.712530  0.790600  0.047328 -0.464338 -0.024932 -0.291658   \n",
      "9  -0.224288 -0.712530  0.790600  0.719219  1.214261  0.570093 -0.291658   \n",
      "10 -0.224288  1.403449 -1.264862 -1.152477 -0.464338 -1.249982 -0.291658   \n",
      "11 -0.224288  1.403449 -1.264862 -0.677771 -0.464338 -0.765288 -0.291658   \n",
      "12 -0.224288  1.403449 -1.264862  0.682519 -0.464338  0.623625 -0.291658   \n",
      "13 -0.224288  1.403449 -1.264862 -0.677771 -0.464338 -0.765288 -0.291658   \n",
      "14 -0.224288  1.403449 -1.264862  0.511253 -0.464338  0.448755 -0.291658   \n",
      "15 -0.224288  1.403449 -1.264862  1.577080 -0.464338  1.537008 -0.291658   \n",
      "16 -0.224288  1.403449 -1.264862 -1.152477 -0.464338 -1.249982 -0.291658   \n",
      "17 -0.224288  1.403449 -1.264862  0.047328 -0.464338 -0.024932 -0.291658   \n",
      "18 -0.224288  1.403449 -1.264862  1.967016 -0.464338  1.935149 -0.291658   \n",
      "19 -0.224288  1.403449 -1.264862  0.407270  2.892861  0.661097 -0.291658   \n",
      "\n",
      "          35        36        37        38        39        40        41  \\\n",
      "0  -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -1.734136  0.323909   \n",
      "1  -0.135474 -0.294017  0.583889 -0.472497  0.316915 -0.153655 -0.558962   \n",
      "2  -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -1.734136 -0.558962   \n",
      "3  -0.135474 -0.294017  0.967650  0.684846  0.828029  0.636585 -0.367440   \n",
      "4  -0.135474 -0.294017  0.967650 -0.472497  0.611193 -1.339016 -0.397024   \n",
      "5  -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -0.153655 -0.406367   \n",
      "6  -0.135474 -0.294017  0.080202 -0.472497 -0.114693 -0.153655 -0.512249   \n",
      "7  -0.135474 -0.294017 -0.567396 -0.472497 -0.762105  1.426825 -0.379896   \n",
      "8  -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -0.153655 -0.562076   \n",
      "9  -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -0.153655  0.963874   \n",
      "10 -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -0.548775  0.518546   \n",
      "11 -0.135474 -0.294017 -0.567396  0.684846  0.077132 -2.129256 -0.563633   \n",
      "12 -0.135474 -0.294017  0.008247  0.862899  0.258589 -0.153655  0.560587   \n",
      "13 -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -0.153655 -0.580761   \n",
      "14 -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -0.943896 -0.538720   \n",
      "15 -0.135474 -0.294017 -0.567396  0.684846 -0.181094 -0.153655 -0.523149   \n",
      "16 -0.135474 -0.294017  0.583889 -0.472497  0.496751 -0.153655 -0.006194   \n",
      "17 -0.135474 -0.294017  0.623588  0.354177  0.496751 -1.339016 -0.557405   \n",
      "18 -0.135474 -0.294017  1.316525 -0.472497  1.126179 -0.153655 -0.174360   \n",
      "19 -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -2.129256 -0.527820   \n",
      "\n",
      "          42        43        44        45        46        47        48  \\\n",
      "0   0.183389  0.987617 -1.182260  3.314465 -1.080738 -0.845098  0.660362   \n",
      "1  -0.904803 -0.242359 -1.182260 -0.520814 -1.230091 -1.500203 -0.368036   \n",
      "2  -0.904803  0.987617 -1.182260  3.314465 -1.103715  0.255478  0.099418   \n",
      "3  -1.122441  0.372629 -0.675256  0.757613 -0.747564 -0.615812 -0.928981   \n",
      "4  -0.904803 -2.702310 -1.182260 -0.520814 -1.222432  1.072573 -0.368036   \n",
      "5  -0.687165 -0.242359 -1.182260 -0.520814 -1.226262 -0.058972 -0.601763   \n",
      "6   0.401028 -0.242359 -1.182260 -0.520814 -1.210943  1.775321 -0.695254   \n",
      "7  -0.251888 -0.242359 -0.675256 -0.520814 -0.728416 -1.354624 -0.835490   \n",
      "8   0.183389  0.987617 -1.182260 -0.520814 -1.218603 -1.107140 -0.368036   \n",
      "9  -1.122441  0.987617 -1.182260 -0.520814 -1.207114  1.644300 -0.648508   \n",
      "10 -1.122441 -3.317298 -1.182260 -0.520814 -1.233921 -1.480046 -0.508272   \n",
      "11 -0.904803 -0.242359  0.845756  3.314465  1.044681  0.581575  1.127816   \n",
      "12  1.271582 -0.242359  0.338752 -0.520814  0.408970  1.120217 -0.134309   \n",
      "13 -0.904803 -0.242359  0.338752 -0.520814  0.374504 -1.500203 -0.181055   \n",
      "14 -1.122441 -2.087322  0.338752 -0.520814  0.320890 -1.456530 -0.274545   \n",
      "15 -0.034249 -0.242359  0.338752 -0.520814  0.393652 -0.386525 -0.134309   \n",
      "16  1.053943 -0.242359  0.338752 -0.520814  0.405141  0.203070 -0.040818   \n",
      "17 -1.122441 -2.087322 -0.168252  0.757613 -0.242059 -0.205875 -0.368036   \n",
      "18 -0.469526 -0.242359  0.338752 -0.520814  0.385993 -0.976119 -0.181055   \n",
      "19 -0.251888 -0.242359  0.338752 -0.520814  0.259616  1.498722 -1.022471   \n",
      "\n",
      "          49        50        51        52        53        54       55  \\\n",
      "0   0.129562 -0.183373 -0.030583 -1.303789 -0.627405  4.190320 -0.02423   \n",
      "1   0.129562 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "2  -0.893405 -0.183373 -0.030583 -1.303789 -0.627405  4.190320 -0.02423   \n",
      "3   0.860253 -0.183373 -0.030583 -1.303789 -0.627405  4.190320 -0.02423   \n",
      "4  -0.528060 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "5  -1.039544 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "6  -1.331820 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "7   0.567976 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "8  -0.528060 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "9   0.641045 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "10 -0.162715 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "11 -0.820336 -0.183373 -0.030583 -1.303789 -0.627405  4.190320 -0.02423   \n",
      "12 -1.477958 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "13 -2.062511 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "14  0.348769 -0.183373 -0.030583 -1.303789  1.593867 -0.238645 -0.02423   \n",
      "15 -1.551027 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "16 -0.089645 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "17 -1.112613 -0.183373 -0.030583 -1.303789  1.593867 -0.238645 -0.02423   \n",
      "18 -2.062511 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "19 -1.697165 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "\n",
      "          56        57        58        59        60        61        62  \\\n",
      "0  -0.113524 -0.250644 -0.437630  2.052381 -0.184981 -0.241153 -0.264626   \n",
      "1  -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "2  -0.113524 -0.250644 -0.437630  2.052381 -0.184981 -0.241153 -0.264626   \n",
      "3  -0.113524 -0.250644 -0.437630 -0.487239 -0.184981 -0.241153 -0.264626   \n",
      "4  -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "5  -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "6  -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "7  -0.113524 -0.250644 -0.437630  2.052381 -0.184981 -0.241153 -0.264626   \n",
      "8  -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "9  -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "10 -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "11 -0.113524 -0.250644 -0.437630  2.052381 -0.184981 -0.241153 -0.264626   \n",
      "12 -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "13 -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "14 -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "15 -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "16 -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "17 -0.113524 -0.250644  2.285036 -0.487239 -0.184981 -0.241153 -0.264626   \n",
      "18 -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "19 -0.113524 -0.250644 -0.437630  2.052381 -0.184981 -0.241153 -0.264626   \n",
      "\n",
      "          63        64        65       66        67        68        69  \\\n",
      "0  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "1  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "2  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "3  -0.184068  2.104756 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "4  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "5  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "6  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "7  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "8  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "9  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "10 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "11 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "12 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "13 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "14 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "15 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "16 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "17 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "18 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "19 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "\n",
      "          70        71        72        73        74        75        76  \\\n",
      "0  -0.052607 -0.413735 -0.685882 -0.138346  2.266158  0.256571 -0.843735   \n",
      "1  -0.052607  0.818296 -0.685882  0.641898 -0.056646  0.032424 -1.460288   \n",
      "2  -0.052607 -0.413735 -0.685882 -0.138346 -0.377615  0.032424 -0.227182   \n",
      "3  -0.052607 -0.413735 -0.685882  1.032020 -0.490799  0.032424  1.005924   \n",
      "4  -0.052607  2.358335  1.457977 -0.138346 -0.330314  0.480717 -0.227182   \n",
      "5  -0.052607 -0.413735  1.457977  0.641898  0.250809  1.153157 -0.843735   \n",
      "6  -0.052607  1.742319 -0.685882  0.251776 -0.281324  0.256571 -0.843735   \n",
      "7  -0.052607  0.818296 -0.685882  0.251776 -0.497556  1.825598 -0.843735   \n",
      "8  -0.052607  0.202280  1.457977 -0.918591 -0.509381 -1.088310  1.005924   \n",
      "9  -0.052607 -1.645767 -0.685882  0.641898 -0.124218  1.153157  1.005924   \n",
      "10 -0.052607 -1.029751 -0.685882  0.251776  4.431855 -1.088310 -0.227182   \n",
      "11 -0.052607 -0.105728 -0.685882 -2.869202 -0.203616 -0.640016 -0.227182   \n",
      "12 -0.052607  0.510288  1.457977 -0.528468 -0.304975  1.377304 -0.227182   \n",
      "13 -0.052607  0.202280  1.457977 -2.088957 -0.585401 -1.088310 -0.227182   \n",
      "14 -0.052607 -0.105728  1.457977 -2.479079 -0.517828 -0.415870 -2.076841   \n",
      "15 -0.052607  0.818296  1.457977 -0.528468 -0.573575 -1.088310 -0.227182   \n",
      "16 -0.052607  0.510288  1.457977 -1.308713 -0.397887  0.704864 -1.460288   \n",
      "17 -0.052607  0.202280 -0.685882  0.641898 -0.526275 -0.640016 -0.843735   \n",
      "18 -0.052607  0.202280  1.457977 -0.528468 -0.406333 -1.088310 -0.227182   \n",
      "19 -0.052607 -0.413735 -0.685882 -2.088957 -0.531343 -0.640016 -0.227182   \n",
      "\n",
      "          77        78        79        80        81        82        83  \\\n",
      "0  -1.701010  0.763486 -1.736451  0.101292 -1.086340  0.227290 -0.183151   \n",
      "1  -1.189353 -0.521153 -1.276990 -1.515574 -0.446991 -0.064402 -0.183151   \n",
      "2  -1.701010  0.763486 -1.736451  0.365270 -1.132007 -0.793633 -0.183151   \n",
      "3  -0.677696  2.048125 -0.682394 -1.529162 -0.127317  0.956521  5.459962   \n",
      "4  -0.677696 -0.521153 -0.709421 -0.030697 -0.995004 -1.085325 -0.183151   \n",
      "5  -0.677696  0.763486 -0.751892 -0.591651 -0.949336 -0.939479 -0.183151   \n",
      "6  -0.166039 -0.521153 -0.276987 -0.761037 -0.492659 -1.741633 -0.183151   \n",
      "7  -0.166039  0.763486 -0.199767  0.840431  0.192357  0.373136 -0.183151   \n",
      "8  -1.189353 -0.521153 -1.242241 -1.130606 -0.538327 -0.574864 -0.183151   \n",
      "9  -1.189353 -0.521153 -1.099384 -0.014198  0.923041  1.029444 -0.183151   \n",
      "10 -1.189353  3.332764 -1.118689  1.579570 -0.127317  0.081444 -0.183151   \n",
      "11  0.345618 -0.521153  0.271277 -0.690643 -0.995004 -0.793633 -0.183151   \n",
      "12 -1.189353 -0.521153 -1.168882  1.238429  0.055354 -1.595787 -0.183151   \n",
      "13  0.857275  3.332764  1.031897 -1.335923  1.014376 -2.106248 -0.183151   \n",
      "14  0.345618 -0.521153  0.352359 -1.284594 -0.081649  0.373136 -0.183151   \n",
      "15  0.345618 -0.521153  0.278999 -0.751561 -0.583994 -1.741633 -0.183151   \n",
      "16  0.345618  0.763486  0.475911  0.017299  1.014376 -0.210248 -0.183151   \n",
      "17 -0.166039 -0.521153 -0.234516 -0.245179  0.009686 -1.158248 -0.183151   \n",
      "18  0.345618 -0.521153  0.325331  0.243434 -0.309988 -2.106248 -0.183151   \n",
      "19  0.345618 -0.521153  0.255833 -0.309341 -0.949336 -1.595787 -0.183151   \n",
      "\n",
      "          84        85        86        87        88        89       90  \\\n",
      "0  -0.030405 -1.305431  1.594038 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "1  -0.030405 -1.305431  1.594038 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "2  -0.030405 -1.305431  1.594038 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "3  -0.030405 -1.305431 -0.627338 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "4  -0.030405  0.766031 -0.627338 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "5  -0.030405 -1.305431 -0.627338  4.208741 -0.024004 -0.122104 -0.25466   \n",
      "6  -0.030405 -1.305431  1.594038 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "7  -0.030405 -1.305431  1.594038 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "8  -0.030405  0.766031 -0.627338 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "9  -0.030405  0.766031 -0.627338 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "10 -0.030405 -1.305431 -0.627338  4.208741 -0.024004 -0.122104 -0.25466   \n",
      "11 -0.030405  0.766031 -0.627338 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "12 -0.030405  0.766031 -0.627338 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "13 -0.030405 -1.305431 -0.627338  4.208741 -0.024004 -0.122104 -0.25466   \n",
      "14 -0.030405 -1.305431  1.594038 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "15 -0.030405 -1.305431  1.594038 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "16 -0.030405 -1.305431 -0.627338  4.208741 -0.024004 -0.122104 -0.25466   \n",
      "17 -0.030405  0.766031 -0.627338 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "18 -0.030405  0.766031 -0.627338 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "19 -0.030405 -1.305431  1.594038 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "\n",
      "          91        92        93        94        95        96        97  \\\n",
      "0  -0.424337 -0.485053 -0.192544 -0.250032  3.699370 -0.188904 -0.469419   \n",
      "1   2.356620 -0.485053 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "2  -0.424337 -0.485053 -0.192544 -0.250032  3.699370 -0.188904 -0.469419   \n",
      "3  -0.424337 -0.485053 -0.192544 -0.250032 -0.270316 -0.188904  2.130294   \n",
      "4  -0.424337  2.061632 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "5  -0.424337 -0.485053 -0.192544 -0.250032 -0.270316 -0.188904  2.130294   \n",
      "6  -0.424337 -0.485053 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "7  -0.424337  2.061632 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "8  -0.424337 -0.485053  5.193607 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "9   2.356620 -0.485053 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "10 -0.424337  2.061632 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "11 -0.424337  2.061632 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "12 -0.424337 -0.485053  5.193607 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "13 -0.424337  2.061632 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "14 -0.424337 -0.485053  5.193607 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "15  2.356620 -0.485053 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "16 -0.424337 -0.485053  5.193607 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "17 -0.424337 -0.485053 -0.192544 -0.250032 -0.270316 -0.188904  2.130294   \n",
      "18 -0.424337 -0.485053  5.193607 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "19 -0.424337 -0.485053 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "\n",
      "          98        99       100       101       102       103       104  \\\n",
      "0  -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  1.124895   \n",
      "1  -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  0.198312   \n",
      "2  -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803 -1.345994   \n",
      "3  -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803 -0.419411   \n",
      "4  -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803 -0.110549   \n",
      "5  -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  0.816034   \n",
      "6  -0.210135  2.279713 -0.362571 -0.792162  0.797549 -0.056803 -1.037133   \n",
      "7  -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803 -1.037133   \n",
      "8  -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803 -0.419411   \n",
      "9  -0.210135 -0.438652 -0.362571  1.262369 -1.253842 -0.056803  1.124895   \n",
      "10 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803 -0.419411   \n",
      "11 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  1.742617   \n",
      "12 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803 -1.654855   \n",
      "13 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  0.198312   \n",
      "14 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  2.360340   \n",
      "15 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  1.742617   \n",
      "16 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  0.507173   \n",
      "17 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  0.198312   \n",
      "18 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803 -0.728272   \n",
      "19 -0.210135  2.279713 -0.362571 -0.792162  0.797549 -0.056803  0.507173   \n",
      "\n",
      "         105       106       107       108       109       110       111  \\\n",
      "0  -0.566421 -0.130578 -0.258403  0.776344  2.893107  3.394031  0.765135   \n",
      "1  -0.566421  1.420362 -0.107048  2.614119 -0.835530 -1.197872 -0.520352   \n",
      "2  -0.566421  0.644892 -0.206150  1.235787 -0.214090 -1.197872 -0.520352   \n",
      "3  -0.566421  0.644892 -0.388135 -0.831709  1.028789 -1.197872 -0.520352   \n",
      "4  -0.566421  1.032627 -0.143085  0.316900 -0.214090 -0.679013  2.050622   \n",
      "5  -0.566421  0.644892 -0.357504 -0.142544 -0.835530 -1.197872 -0.520352   \n",
      "6  -0.566421  1.032627 -0.348495  2.154675 -0.214090 -0.679013  2.050622   \n",
      "7  -0.566421  0.644892 -0.534085 -1.061431 -0.214090 -0.679013  0.765135   \n",
      "8   1.765471 -0.518313  0.105569 -0.831709  1.028789 -1.197872 -0.520352   \n",
      "9  -0.566421 -0.906048 -0.292638  0.316900  1.028789 -1.197872 -0.520352   \n",
      "10 -0.566421  0.644892  5.100272  1.695231 -0.214090 -0.679013 -0.520352   \n",
      "11 -0.566421  1.032627 -0.357504  0.776344 -0.214090  0.358705 -0.520352   \n",
      "12 -0.566421  0.644892  2.320031  1.924953 -2.699848 -0.160154  0.765135   \n",
      "13  1.765471  0.257157 -0.429578 -0.831709 -0.214090  0.877565  2.050622   \n",
      "14  1.765471 -1.293783 -0.415163  0.087178 -0.835530  0.358705 -0.520352   \n",
      "15  1.765471  0.644892 -0.526877  2.384397 -0.835530  0.877565 -0.520352   \n",
      "16  1.765471 -0.518313 -0.532283 -1.061431 -0.214090  0.358705 -0.520352   \n",
      "17 -0.566421  1.420362 -0.247592 -0.831709 -0.835530 -0.679013  0.765135   \n",
      "18  1.765471 -0.130578 -0.508859  0.546622 -0.214090  0.877565 -0.520352   \n",
      "19 -0.566421 -1.293783 -0.391739 -0.831709 -0.214090  0.358705 -0.520352   \n",
      "\n",
      "         112       113        114       115       116       117       118  \\\n",
      "0   4.122613 -0.802297 -10.160172 -0.038195 -0.182993 -0.029125 -1.306527   \n",
      "1  -1.223508 -1.422108  -0.226458 -0.110973 -0.182993 -0.029125  0.765388   \n",
      "2  -1.250924  0.359849  -0.688491 -0.474865 -0.182993 -0.029125  0.765388   \n",
      "3  -1.297923 -1.499584  -0.596085  1.053481 -0.182993 -0.029125  0.765388   \n",
      "4  -0.686937  0.127420  -0.503678 -1.057091  5.464691 -0.029125 -1.306527   \n",
      "5  -1.266590 -0.221224  -0.780898 -1.129870 -0.182993 -0.029125  0.765388   \n",
      "6  -0.655605  1.439961  -0.318865 -1.493761  5.464691 -0.029125 -1.306527   \n",
      "7  -0.698687 -0.918511   0.281778  0.616811 -0.182993 -0.029125 -1.306527   \n",
      "8  -1.262673 -0.873813  -0.642288 -0.474865 -0.182993 -0.029125  0.765388   \n",
      "9  -1.278340  0.437325  -0.596085  0.762367 -0.182993 -0.029125  0.765388   \n",
      "10 -0.522441  0.476064   0.974828  0.180140 -0.182993 -0.029125 -1.306527   \n",
      "11  0.253040  1.002447  -0.411271 -1.202648 -0.182993 -0.029125  0.765388   \n",
      "12 -0.166033 -1.499584   1.113438 -1.202648 -0.182993 -0.029125 -1.306527   \n",
      "13  0.832692 -1.298145  -0.134052 -2.148766 -0.182993 -0.029125 -1.306527   \n",
      "14  0.323538 -1.393934  -0.688491 -0.183751 -0.182993 -0.029125  0.765388   \n",
      "15  0.836609  0.650385   1.159641 -2.148766 -0.182993 -0.029125  0.765388   \n",
      "16  0.311788 -0.730780  -0.134052 -0.329308 -0.182993 -0.029125  0.765388   \n",
      "17 -0.702604 -1.112202  -0.411271 -1.202648 -0.182993 -0.029125  0.765388   \n",
      "18  0.820943 -0.536663  -0.134052 -1.930431 -0.182993 -0.029125  0.765388   \n",
      "19  0.162959  1.310695  -1.011915 -1.712096 -0.182993 -0.029125 -1.306527   \n",
      "\n",
      "         119       120      121       122      123       124       125  \\\n",
      "0   1.595663 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "1  -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864  2.064688   \n",
      "2  -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "3  -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "4  -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "5  -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "6  -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "7   1.595663 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "8  -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "9  -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "10  1.595663 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "11 -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "12  1.595663 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864  2.064688   \n",
      "13 -0.626699  4.202876 -0.02423 -0.126786 -0.25707 -0.412864  2.064688   \n",
      "14 -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864  2.064688   \n",
      "15 -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "16 -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "17 -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "18 -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "19  1.595663 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864  2.064688   \n",
      "\n",
      "         126       127       128       129       130       131       132  \\\n",
      "0  -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518  2.281729   \n",
      "1  -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "2  -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518  2.281729   \n",
      "3  -0.196634  3.960014 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "4  -0.196634 -0.252524 -0.274734 -0.191876  2.147662 -0.215518 -0.438264   \n",
      "5  -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518  2.281729   \n",
      "6  -0.196634 -0.252524 -0.274734 -0.191876  2.147662 -0.215518 -0.438264   \n",
      "7  -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518  2.281729   \n",
      "8   5.085578 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "9  -0.196634  3.960014 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "10 -0.196634 -0.252524 -0.274734 -0.191876  2.147662 -0.215518 -0.438264   \n",
      "11 -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518  2.281729   \n",
      "12 -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "13 -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "14 -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "15 -0.196634  3.960014 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "16  5.085578 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "17 -0.196634 -0.252524 -0.274734 -0.191876  2.147662 -0.215518 -0.438264   \n",
      "18 -0.196634 -0.252524  3.639890 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "19 -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "\n",
      "         133       134       135        136       137       138       139  \\\n",
      "0  -0.350214 -0.793361 -1.252524  18.233291  1.449144 -0.513521 -0.685882   \n",
      "1  -0.350214 -0.793361  0.798388  -0.054845 -0.111482 -0.513521 -0.685882   \n",
      "2  -0.350214 -0.793361  0.798388  -0.054845 -0.111482 -0.513521 -0.685882   \n",
      "3  -0.350214 -0.793361  0.798388  -0.054845 -0.423607 -0.513521 -0.685882   \n",
      "4  -0.350214 -0.793361  0.798388  -0.054845 -1.672108 -0.513521  1.457977   \n",
      "5  -0.350214 -0.793361  0.798388  -0.054845 -1.047857 -0.513521  1.457977   \n",
      "6  -0.350214 -0.793361  0.798388  -0.054845 -0.423607 -0.513521 -0.685882   \n",
      "7  -0.350214 -0.793361  0.798388  -0.054845  0.512768 -0.513521 -0.685882   \n",
      "8  -0.350214 -0.793361  0.798388  -0.054845  2.073394  1.947341  1.457977   \n",
      "9  -0.350214 -0.793361  0.798388  -0.054845  0.512768 -0.513521 -0.685882   \n",
      "10 -0.350214  1.260460 -1.252524  -0.054845  3.009769 -0.513521 -0.685882   \n",
      "11 -0.350214 -0.793361  0.798388  -0.054845 -0.735732 -0.513521 -0.685882   \n",
      "12 -0.350214 -0.793361  0.798388  -0.054845  0.824893 -0.513521  1.457977   \n",
      "13 -0.350214 -0.793361  0.798388  -0.054845 -0.111482  1.947341  1.457977   \n",
      "14 -0.350214 -0.793361  0.798388  -0.054845 -0.423607 -0.513521  1.457977   \n",
      "15 -0.350214 -0.793361  0.798388  -0.054845 -0.735732 -0.513521  1.457977   \n",
      "16 -0.350214 -0.793361  0.798388  -0.054845 -0.111482  1.947341  1.457977   \n",
      "17 -0.350214 -0.793361  0.798388  -0.054845  1.449144 -0.513521 -0.685882   \n",
      "18 -0.350214 -0.793361  0.798388  -0.054845  0.824893 -0.513521  1.457977   \n",
      "19 -0.350214 -0.793361  0.798388  -0.054845 -0.111482 -0.513521 -0.685882   \n",
      "\n",
      "         race_id order  \n",
      "0   201101010111     5  \n",
      "1   201101010111     2  \n",
      "2   201101010111    11  \n",
      "3   201101010111     7  \n",
      "4   201101010111     1  \n",
      "5   201101010111     6  \n",
      "6   201101010111     3  \n",
      "7   201101010111     8  \n",
      "8   201101010111     4  \n",
      "9   201101010111    12  \n",
      "10  201101010111    14  \n",
      "11  201101010112     8  \n",
      "12  201101010112     4  \n",
      "13  201101010112     6  \n",
      "14  201101010112     9  \n",
      "15  201101010112     3  \n",
      "16  201101010112     5  \n",
      "17  201101010112     2  \n",
      "18  201101010112    10  \n",
      "19  201101010112     7  \n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data.dtypes)\n",
    "print(data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_series_data(raw_data):\n",
    "    number_of_race = raw_data.race_id.nunique()\n",
    "    time_series_data = np.full((number_of_race, 18, 140), -float('inf'))\n",
    "    label = np.full((number_of_race, 18), 19)\n",
    "    race_number = 0\n",
    "    horse_number = 0\n",
    "    for i in range(len(raw_data)):\n",
    "        if i == 0:\n",
    "#             print(race_number)\n",
    "#             print(horse_number)\n",
    "#             print(raw_data.iloc[i].order)\n",
    "            label[race_number][horse_number] = float(raw_data.iloc[i].order)\n",
    "            time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "            horse_number += 1\n",
    "            continue\n",
    "        if data.iloc[i].race_id != data.iloc[i-1].race_id:\n",
    "            race_number += 1\n",
    "            horse_number = 0\n",
    "            label[race_number][horse_number] = float(raw_data.iloc[i].order)\n",
    "            time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "            horse_number += 1\n",
    "        else:\n",
    "            label[race_number][horse_number] = float(raw_data.iloc[i].order)\n",
    "            time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "            horse_number += 1\n",
    "    return time_series_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8489, 18, 140)\n",
      "(8489, 18)\n"
     ]
    }
   ],
   "source": [
    "X, y_order = create_time_series_data(data)\n",
    "print(X.shape)\n",
    "print(y_order.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "X = X.astype('float32')\n",
    "print(X.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf]\n",
      "[ 5  2 11  7  1  6  3  8  4 12 14 19 19 19 19 19 19 19]\n"
     ]
    }
   ],
   "source": [
    "print(X[0][11])\n",
    "print(y_order[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "(8489, 18, 140)\n"
     ]
    }
   ],
   "source": [
    "# creating X,y (parameters and target)\n",
    "y = np_utils.to_categorical(y_order, dtype='float32')\n",
    "print(y[0])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(data.race_id.value_counts().plot.hist(bins=18,range=(1,18)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(pd.DataFrame(y_order).plot.hist(bins=18, ylim=(0,1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUFFER_SIZE = 1000\n",
    "# BATCH_SIZE = 128\n",
    "# def make_batches(ds):\n",
    "#     return (\n",
    "#       ds\n",
    "# #       .cache()\n",
    "# #       .shuffle(BUFFER_SIZE)\n",
    "#       .batch(BATCH_SIZE)\n",
    "# #       .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#       .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "# X = tf.convert_to_tensor(X)\n",
    "# y = tf.convert_to_tensor(y)\n",
    "# X = make_batches(X)\n",
    "# y = make_batches(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.3145431  -0.50991476  0.5901306   0.089023    0.3633046  -0.555636\n",
      " -0.38537532  0.8550409  -0.7798238   0.7798238   0.6798608  -0.66338956\n",
      " -0.18337315 -0.02931084 -1.3091208   1.597206   -0.23655261 -0.02236226\n",
      " -0.24513794  2.2255282  -0.4990219  -0.17550196 -0.2203592  -0.25591546\n",
      " -0.17979759 -0.49679595 -0.19606745 -0.4557965  -0.22428839 -0.71253043\n",
      "  0.7906001  -1.1524768  -0.4643385  -1.249982   -0.29165798 -0.1354737\n",
      " -0.29401696  0.5838889  -0.4724972   0.4967511  -1.3390157  -0.53560555\n",
      "  0.6186662   0.98761696  0.8457565  -0.5208136   0.7919285  -1.0952293\n",
      "  0.4266353   1.6640126  -0.18337315 -0.03058303 -1.3037888   1.5938668\n",
      " -0.23864526 -0.02422995 -0.1135239  -0.25064442  2.2850358  -0.487239\n",
      " -0.18498135 -0.24115273 -0.26462567 -0.18406838 -0.47511455 -0.2061855\n",
      " -0.44352028 -0.37101352 -0.7890041   0.79361564 -0.0526072   0.20228021\n",
      " -0.685882   -0.52846843 -0.49079907 -0.6400163   1.0059242   1.368932\n",
      " -0.52115315  1.2905855  -1.249058    0.10102149  1.6128289  -0.18315147\n",
      " -0.03040453  0.76603085 -0.6273377  -0.23760074 -0.02400442 -0.12210409\n",
      " -0.25465992  2.3566198  -0.48505253 -0.1925444  -0.25003174 -0.27031633\n",
      " -0.18890353 -0.46941882 -0.21013454 -0.4386517  -0.36257088 -0.7921616\n",
      "  0.7975488  -0.0568033   0.19831172 -0.5664212  -0.9060477  -0.54129195\n",
      " -0.37226558  1.0287887   0.8775645  -0.52035177  0.80527645  0.97966\n",
      " -0.27266148  1.5629289  -0.18299298 -0.02912458 -1.3065268   1.5956632\n",
      " -0.23793232 -0.02422995 -0.12678613 -0.25707033  2.4221058  -0.4843346\n",
      " -0.19663447 -0.25252435 -0.27473357 -0.19187577 -0.46562257 -0.21551774\n",
      " -0.43826422 -0.35021392 -0.79336107  0.79838777 -0.05484473  0.51276827\n",
      " -0.5135207  -0.685882  ]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6791, 18, 140)\n",
      "(1698, 18, 140)\n",
      "(6791, 18, 20)\n",
      "(1698, 18, 20)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataset with batch size\n",
    "batch_size = 2048\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=6791).batch(batch_size)\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "valid_dataset = valid_dataset.shuffle(buffer_size=1698).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters\n",
    "num_layers = 1\n",
    "d_model = 140\n",
    "num_heads = 1\n",
    "d_ffn = 8\n",
    "pe_input = 18\n",
    "target_size = 20\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model\n",
    "trans_race = transformer.TransRace(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ffn=d_ffn,\n",
    "    pe_input=pe_input,\n",
    "    target_size=target_size,\n",
    "    rate=dropout_rate,\n",
    ")\n",
    "\n",
    "# trans_race.build(input_shape=(None,18,139))\n",
    "opt = optimizers.Adam(lr = 0.0001, clipvalue = 2.)\n",
    "trans_race.compile(\n",
    "    optimizer=opt,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "#     patience=10,\n",
    "#     min_delta=0.001,\n",
    "#     restore_best_weights=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask shape after creating (?, 1, 1, 18, 140)\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 1, 140) split_head\n",
      "(?, 18, 1, 140) split_head\n",
      "(?, 18, 1, 140) split_head\n",
      "(?, 1, 18, 140) scaled_dot_\n",
      "(?, 1, 18, 18) scaled_attention_shape\n",
      "Train on 6791 samples, validate on 1698 samples\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "2 root error(s) found.\n  (0) Invalid argument: Incompatible shapes: [32,18,20] vs. [32,18,18]\n\t [[{{node training_6/gradients/loss_3/output_1_loss/mul_grad/BroadcastGradientArgs}}]]\n  (1) Invalid argument: Incompatible shapes: [32,18,20] vs. [32,18,18]\n\t [[{{node training_6/gradients/loss_3/output_1_loss/mul_grad/BroadcastGradientArgs}}]]\n\t [[loss_3/mul/_1443]]\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-a63f679d84dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#                         callbacks=[early_stopping],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# hide the output because we have so many epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                         )\n\u001b[1;32m      9\u001b[0m \u001b[0mtrans_race\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../models/results/transfomer1.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument: Incompatible shapes: [32,18,20] vs. [32,18,18]\n\t [[{{node training_6/gradients/loss_3/output_1_loss/mul_grad/BroadcastGradientArgs}}]]\n  (1) Invalid argument: Incompatible shapes: [32,18,20] vs. [32,18,18]\n\t [[{{node training_6/gradients/loss_3/output_1_loss/mul_grad/BroadcastGradientArgs}}]]\n\t [[loss_3/mul/_1443]]\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "history = trans_race.fit(\n",
    "                        X_train, \n",
    "                        y_train,\n",
    "                        validation_data=([X_valid, y_valid]),\n",
    "                        epochs=1,\n",
    "#                         callbacks=[early_stopping],\n",
    "                        verbose=True, # hide the output because we have so many epochs\n",
    "                        )\n",
    "trans_race.save_weights(\"../models/results/transfomer1.h5\")\n",
    "\n",
    "# the problem is gradient exploding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \n",
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "# for loss\n",
    "axL.plot(history.history['loss'],label=\"loss for training\")\n",
    "axL.plot(history.history['val_loss'],label=\"loss for validation\")\n",
    "axL.set_title('model loss')\n",
    "axL.set_xlabel('epoch')\n",
    "axL.set_ylabel('loss')\n",
    "axL.legend(loc='upper right')\n",
    "axR.plot(history.history['acc'],label=\"acc for training\")\n",
    "axR.plot(history.history['val_acc'],label=\"acc for validation\")\n",
    "axR.set_title('model accuracy')\n",
    "axR.set_xlabel('epoch')\n",
    "axR.set_ylabel('accuracy')\n",
    "axR.legend(loc='upper right')\n",
    "# figureの保存\n",
    "plt.savefig(\"../models/results/trans_race_result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = trans_race.layers[0]\n",
    "print(l1.get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "pred = trans_race(X_valid, training=False)\n",
    "# print(backend.eval(pred))\n",
    "pred = backend.eval(pred)\n",
    "y_pred = np.argmax(pred, axis = 2)\n",
    "y_ans = np.argmax(y_valid, axis = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred.shape)\n",
    "print(y_pred.shape)\n",
    "print(y_ans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8 10  9  3  4  7  5  6  1  2 19 19 19 19 19 19 19 19]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0.08532914539981677\n"
     ]
    }
   ],
   "source": [
    "print(y_ans[0])\n",
    "print(y_pred[1])\n",
    "print(np.mean(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.43437406, -0.50991476,  1.400889  , ...,  1.7612689 ,\n",
       "        -0.5135207 , -0.685882  ],\n",
       "       [ 0.43437406, -0.50991476,  1.400889  , ...,  0.20064315,\n",
       "        -0.5135207 , -0.685882  ],\n",
       "       [ 0.43437406, -0.50991476,  1.400889  , ..., -0.11148198,\n",
       "        -0.5135207 , -0.685882  ],\n",
       "       ...,\n",
       "       [ 0.43437406, -0.50991476,  1.400889  , ...,  1.4491436 ,\n",
       "        -0.5135207 , -0.685882  ],\n",
       "       [ 0.43437406, -0.50991476,  1.400889  , ...,  0.20064315,\n",
       "         1.9473411 ,  1.4579768 ],\n",
       "       [       -inf,        -inf,        -inf, ...,        -inf,\n",
       "               -inf,        -inf]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.007067137809187279\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(y_pred)):\n",
    "    for j in range(len(y_pred[0])):\n",
    "        if (y_pred[i][j] == y_ans[i][j]):\n",
    "            correct += 1\n",
    "accuracy = correct / len(y_pred)\n",
    "print(\"accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 何位に予想した？　何位が含まれていた？\n",
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "label = ['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19']\n",
    "\n",
    "axL.hist(y_pred.flatten(), bins = 20, label = \"pred_order\")##, range = (1,21)\n",
    "axL.set_xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19])\n",
    "axL.set_xticklabels(label)\n",
    "axL.set_title('pred order distribution')\n",
    "axL.legend()\n",
    "axR.hist(y_ans.flatten(), bins = 20, label = \"ans_order\", range = (1,21))\n",
    "axR.set_xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19])\n",
    "axR.set_xticklabels(label)\n",
    "axR.set_title('ans order distribution')\n",
    "axR.legend()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.unique(pred_order))\n",
    "# print(np.unique(Y_ans))\n",
    "# u, c = np.unique(pred_order, return_counts = True)\n",
    "# print(u)\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy of the first horse\n",
    "i = 0\n",
    "correct_first = 0\n",
    "all_f = 0\n",
    "increase = 0\n",
    "X_test_inv = standard_scale.inverse_transform(X_test)\n",
    "X_test_inv_df = pd.DataFrame(X_test_inv)\n",
    "odds = X_test_inv_df[4].values\n",
    "hit_odds = []\n",
    "select = []\n",
    "\n",
    "for i in range(len(Y_ans)):\n",
    "    if (pred_order[i] == 1):  # いちい予想した総数  & (pred[i][1]*odds[i] > 1.0)) | (pred[i][1]> 0.25)\n",
    "        all_f = all_f + 1\n",
    "        if (Y_ans[i] == 1):\n",
    "            correct_first = correct_first + 1   #　一致した総数\n",
    "            increase += odds[i]\n",
    "            hit_odds.append(odds[i])\n",
    "            \n",
    "            \n",
    "            \n",
    "print(\"all_f: \", all_f)\n",
    "print(\"correct_first: \", correct_first)\n",
    "print(\"hit odds average: \", np.array(hit_odds).mean())\n",
    "print(\"spent money:\", all_f * 100)\n",
    "revenue = (increase - all_f) * 100\n",
    "retrive = increase / all_f\n",
    " \n",
    "print(\"retrive rate: \", retrive) \n",
    "print(\"revenue: \", revenue)\n",
    "accurate_rate = correct_first / all_f\n",
    "print(\"accuracy: \",accurate_rate)\n",
    "#print(\"\\n\".join(map(str,hit_odds)))\n",
    "print(\"min: \", min(hit_odds))\n",
    "print(\"mid: \", np.median(np.array(hit_odds)))\n",
    "print(\"max: \", max(hit_odds))\n",
    "\n",
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "axL.hist(hit_odds, bins = 50, label = \"hit_odds\")\n",
    "axL.set_title('hit odds distribution')\n",
    "axL.legend()\n",
    "axR.hist(odds, bins = 50, label = \"odds\", range = (0,40))\n",
    "axR.set_title('all odds distribution')\n",
    "axR.legend()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一位だった時一位予想していた確率\n",
    "i = 0\n",
    "correct_first = 0\n",
    "all_f = 0\n",
    "odds_f = []\n",
    "p_rate_f = []\n",
    "\n",
    "all_f_odds = []\n",
    "\n",
    "for i in range(len(Y_ans)):\n",
    "    if (Y_ans[i] == 1):  # 一位の総数\n",
    "        all_f = all_f + 1\n",
    "        all_f_odds.append(odds[i])\n",
    "        if (pred_order[i] == 1):\n",
    "            correct_first = correct_first + 1   #　一致した総数\n",
    "            odds_f.append(odds[i])\n",
    "            p_rate_f.append(pred[i][1])\n",
    "\n",
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "axL.scatter(p_rate_f, odds_f)  \n",
    "axL.set_title('correlation odss and prediction')\n",
    "#axL.xlabel('prediction rate first')\n",
    "#axL.ylabel('odds')\n",
    "axR.hist(odds_f, bins = 50, label = \"odds\")\n",
    "axR.set_title('all first odds distribution')\n",
    "axR.legend()\n",
    "\n",
    "fig.show()\n",
    "accurate_rate = correct_first / all_f\n",
    "print(\"accuracy: \",accurate_rate)\n",
    "print(\"all_f_odds average: \", np.array(all_f_odds).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ensamble log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = resnet.resrace(X_test.shape[1], 19)\n",
    "model1.load_weights(\"model/win5_resrace_model_best1.h5\")\n",
    "pred1 = model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = resnet.resrace(X_test.shape[1], 19)\n",
    "model2.load_weights(\"model/win5_resrace_model_best2.h5\")\n",
    "pred2 = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = resnet.resrace(X_test.shape[1], 19)\n",
    "model3.load_weights(\"model/win5_resrace_model_best3.h5\")\n",
    "pred3 = model3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = resnet.resrace(X_test.shape[1], 19)\n",
    "model4.load_weights(\"model/win5_resrace_model_best4.h5\")\n",
    "pred4 = model4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = resnet.resrace(X_test.shape[1], 19)\n",
    "model5.load_weights(\"model/win5_resrace_model_best5.h5\")\n",
    "pred5 = model5.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = resnet.resrace(X_test.shape[1], 19)\n",
    "model6.load_weights(\"model/win5_resrace_model_best6.h5\")\n",
    "pred6 = model6.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = resnet.resrace(X_test.shape[1], 19)\n",
    "model7.load_weights(\"model/win5_resrace_model_best7.h5\")\n",
    "pred7 = model7.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = resnet.resrace(X_test.shape[1], 19)\n",
    "model8.load_weights(\"model/win5_resrace_model_best8.h5\")\n",
    "pred8 = model8.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model9 = resnet.resrace(X_test.shape[1], 19)\n",
    "model9.load_weights(\"model/win5_resrace_model_best9.h5\")\n",
    "pred9 = model9.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10 = resnet.resrace(X_test.shape[1], 19)\n",
    "model10.load_weights(\"model/win5_resrace_model_best10.h5\")\n",
    "pred10 = model10.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pred1 = np.log(pred1)\n",
    "log_pred2 = np.log(pred2)\n",
    "log_pred3 = np.log(pred3)\n",
    "log_pred4 = np.log(pred4)\n",
    "log_pred5 = np.log(pred5)\n",
    "# log_pred6 = np.log(pred6)\n",
    "# log_pred7 = np.log(pred7)\n",
    "# log_pred8 = np.log(pred8)\n",
    "# log_pred9 = np.log(pred9)\n",
    "# log_pred10 = np.log(pred10)\n",
    "\n",
    "sum_pred = log_pred1 + log_pred2 + log_pred3 + log_pred4 + log_pred5 #+ log_pred6 + log_pred7 + log_pred8 + log_pred9 + log_pred10\n",
    "pred_order = np.argmax(sum_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sum_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pred_order[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
