{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pprint\n",
    "# import sys\n",
    "\n",
    "# pprint.pprint(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "# pd.set_option('display.max_rows', 200)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/mnt/c/Users/44yos/RacePrediction/resnet_win5')\n",
    "from os import path\n",
    "import time\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import tensorflow as tf\n",
    "# from tensorflow.python import keras\n",
    "# from tensorflow.python.keras.models import Sequential\n",
    "# from tensorflow.python.keras.layers import Dense, Dropout\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from tensorflow.python.keras import optimizers\n",
    "# from tensorflow.python.keras.models import load_model\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "# from scipy.stats import norm\n",
    "\n",
    "from pickle import dump\n",
    "\n",
    "# from models import resnet\n",
    "from models import transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race_id                 int64\n",
      "race_round              int64\n",
      "ground_condition        int64\n",
      "total_horse_number      int64\n",
      "order                   int64\n",
      "                       ...   \n",
      "ground_type_芝_3       float64\n",
      "ground_type_障_3       float64\n",
      "horse_weight_dif_3    float64\n",
      "same_jockey_3         float64\n",
      "same_jockey           float64\n",
      "Length: 150, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = pd.read_csv(\"../data/csv/data.csv\", sep = \",\")\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust columns type\n",
    "data['race_id'] = data['race_id'].astype(str)\n",
    "# data['race_round'] = data['race_round'].astype(str)\n",
    "# #data['total_horse_number'] = data['total_horse_number'].astype(str)\n",
    "data['order'] = data['order'].astype(str)\n",
    "# data['frame_number'] = data['frame_number'].astype(str)\n",
    "# data['horse_number'] = data['horse_number'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete race day information\n",
    "data = data.drop([\"horse_number\", \"half_order\", \"goal_time\" ,\"last_time\", \"horse_weight\", \"horse_weight_dif\", \"frame_number\",\"pop\"], axis = 1)\n",
    "# \"race_round\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standarlization \n",
    "no_scale_data = data[['race_id','order']]\n",
    "scale_columns = data.drop(['race_id','order'], axis=1).columns.values\n",
    "standard_scale = StandardScaler()\n",
    "data = pd.DataFrame(standard_scale.fit_transform(data[scale_columns]))\n",
    "data = pd.concat([data, no_scale_data], axis=1)\n",
    "dump(standard_scale, open(\"standard_scale.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating time series data\n",
    "# def return_one_race(all_horse):\n",
    "#     print(len(all_horse))\n",
    "#     one_race = np.full((18, 139), -float('inf'))\n",
    "# #     print(len(one_race))\n",
    "# #     print(len(one_race[0]))\n",
    "#     for i, one_horse in all_horse.iterrows():\n",
    "#         print(i)\n",
    "# #         print(one_horse)\n",
    "#         one_race[i] = one_horse.drop(['race_id']).values\n",
    "#     print(one_race)\n",
    "#     return one_race\n",
    "\n",
    "# def create_time_series_data(raw_data):\n",
    "#     time_series_data = []\n",
    "#     time_series_data.append(raw_data.groupby(['race_id']).apply(return_one_race))\n",
    "#     return time_series_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92033, 142)\n",
      "0          float64\n",
      "1          float64\n",
      "2          float64\n",
      "3          float64\n",
      "4          float64\n",
      "            ...   \n",
      "137        float64\n",
      "138        float64\n",
      "139        float64\n",
      "race_id     object\n",
      "order       object\n",
      "Length: 142, dtype: object\n",
      "           0         1         2         3         4         5         6  \\\n",
      "0   0.434374 -0.509915 -0.220628  1.545825 -0.874906  2.530312 -0.385375   \n",
      "1   0.434374 -0.509915 -0.220628  0.089023 -0.874906 -0.643375 -0.385375   \n",
      "2   0.434374 -0.509915 -0.220628  0.817424 -0.255801 -0.554265 -0.385375   \n",
      "3   0.434374 -0.509915 -0.220628  1.545825  0.363305 -0.491203 -0.385375   \n",
      "4   0.434374 -0.509915 -0.220628  0.817424 -0.874906 -0.591280 -0.385375   \n",
      "5   0.434374 -0.509915 -0.220628  0.089023 -0.874906 -0.563862 -0.385375   \n",
      "6   0.434374 -0.509915 -0.220628  0.089023 -0.874906 -0.484348 -0.385375   \n",
      "7   0.434374 -0.509915 -0.220628  0.089023 -0.874906 -0.456930 -0.385375   \n",
      "8   0.434374 -0.509915 -0.220628  0.089023 -0.255801 -0.439108 -0.385375   \n",
      "9   0.434374 -0.509915 -0.220628  0.817424 -0.874906 -0.067587 -0.385375   \n",
      "10  0.434374 -0.509915 -0.220628  0.817424 -2.732221  0.099665 -0.385375   \n",
      "11  1.308833 -0.509915 -1.436765  0.089023 -0.255801 -0.574829 -0.385375   \n",
      "12  1.308833 -0.509915 -1.436765 -0.639378 -0.255801  0.180550 -0.385375   \n",
      "13  1.308833 -0.509915 -1.436765 -0.639378 -0.255801 -0.639262 -0.385375   \n",
      "14  1.308833 -0.509915 -1.436765 -1.367779 -2.113116 -0.562491 -0.385375   \n",
      "15  1.308833 -0.509915 -1.436765 -0.639378 -0.255801 -0.519992 -0.385375   \n",
      "16  1.308833 -0.509915 -1.436765 -0.639378 -0.255801  0.620616 -0.385375   \n",
      "17  1.308833 -0.509915 -1.436765 -1.367779 -2.113116 -0.314354 -0.385375   \n",
      "18  1.308833 -0.509915 -1.436765 -0.639378 -0.255801 -0.063475 -0.385375   \n",
      "19  1.308833 -0.509915 -1.436765 -0.639378 -0.255801 -0.618698 -0.385375   \n",
      "\n",
      "           7         8         9        10       11        12        13  \\\n",
      "0  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "1  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "2  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "3  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "4  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "5  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "6  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "7  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "8  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "9  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "10 -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "11  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "12  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "13  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "14  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "15  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "16  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "17  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "18  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "19  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "\n",
      "          14        15        16        17        18        19        20  \\\n",
      "0   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "1   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "2   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "3   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "4   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "5   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "6   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "7   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "8   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "9   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "10  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "11  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "12  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "13  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "14  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "15  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "16  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "17  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "18  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "19  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "\n",
      "          21        22        23       24        25        26        27  \\\n",
      "0  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "1  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "2  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "3  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "4  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "5  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "6  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "7  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "8  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "9  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "10 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "11 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "12 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "13 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "14 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "15 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "16 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "17 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "18 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "19 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "\n",
      "          28        29        30        31        32        33        34  \\\n",
      "0  -0.224288 -0.712530  0.790600 -0.149783 -0.464338 -0.226190 -0.291658   \n",
      "1  -0.224288  1.403449 -1.264862 -0.507064  0.494861 -0.509254 -0.291658   \n",
      "2  -0.224288 -0.712530  0.790600 -1.152477 -0.464338 -1.249982 -0.291658   \n",
      "3   4.458546 -0.712530 -1.264862 -1.152477 -0.464338 -1.249982 -0.291658   \n",
      "4  -0.224288  1.403449 -1.264862 -1.152477 -0.464338 -1.249982 -0.291658   \n",
      "5  -0.224288  1.403449 -1.264862 -0.762540  0.934495 -0.155093 -0.291658   \n",
      "6  -0.224288  1.403449 -1.264862 -1.152477 -0.464338 -1.249982 -0.291658   \n",
      "7  -0.224288  1.403449 -1.264862  0.767211 -0.464338  0.710099 -0.291658   \n",
      "8  -0.224288 -0.712530  0.790600  0.047328 -0.464338 -0.024932 -0.291658   \n",
      "9  -0.224288 -0.712530  0.790600  0.719219  1.214261  0.570093 -0.291658   \n",
      "10 -0.224288  1.403449 -1.264862 -1.152477 -0.464338 -1.249982 -0.291658   \n",
      "11 -0.224288  1.403449 -1.264862 -0.677771 -0.464338 -0.765288 -0.291658   \n",
      "12 -0.224288  1.403449 -1.264862  0.682519 -0.464338  0.623625 -0.291658   \n",
      "13 -0.224288  1.403449 -1.264862 -0.677771 -0.464338 -0.765288 -0.291658   \n",
      "14 -0.224288  1.403449 -1.264862  0.511253 -0.464338  0.448755 -0.291658   \n",
      "15 -0.224288  1.403449 -1.264862  1.577080 -0.464338  1.537008 -0.291658   \n",
      "16 -0.224288  1.403449 -1.264862 -1.152477 -0.464338 -1.249982 -0.291658   \n",
      "17 -0.224288  1.403449 -1.264862  0.047328 -0.464338 -0.024932 -0.291658   \n",
      "18 -0.224288  1.403449 -1.264862  1.967016 -0.464338  1.935149 -0.291658   \n",
      "19 -0.224288  1.403449 -1.264862  0.407270  2.892861  0.661097 -0.291658   \n",
      "\n",
      "          35        36        37        38        39        40        41  \\\n",
      "0  -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -1.734136  0.323909   \n",
      "1  -0.135474 -0.294017  0.583889 -0.472497  0.316915 -0.153655 -0.558962   \n",
      "2  -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -1.734136 -0.558962   \n",
      "3  -0.135474 -0.294017  0.967650  0.684846  0.828029  0.636585 -0.367440   \n",
      "4  -0.135474 -0.294017  0.967650 -0.472497  0.611193 -1.339016 -0.397024   \n",
      "5  -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -0.153655 -0.406367   \n",
      "6  -0.135474 -0.294017  0.080202 -0.472497 -0.114693 -0.153655 -0.512249   \n",
      "7  -0.135474 -0.294017 -0.567396 -0.472497 -0.762105  1.426825 -0.379896   \n",
      "8  -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -0.153655 -0.562076   \n",
      "9  -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -0.153655  0.963874   \n",
      "10 -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -0.548775  0.518546   \n",
      "11 -0.135474 -0.294017 -0.567396  0.684846  0.077132 -2.129256 -0.563633   \n",
      "12 -0.135474 -0.294017  0.008247  0.862899  0.258589 -0.153655  0.560587   \n",
      "13 -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -0.153655 -0.580761   \n",
      "14 -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -0.943896 -0.538720   \n",
      "15 -0.135474 -0.294017 -0.567396  0.684846 -0.181094 -0.153655 -0.523149   \n",
      "16 -0.135474 -0.294017  0.583889 -0.472497  0.496751 -0.153655 -0.006194   \n",
      "17 -0.135474 -0.294017  0.623588  0.354177  0.496751 -1.339016 -0.557405   \n",
      "18 -0.135474 -0.294017  1.316525 -0.472497  1.126179 -0.153655 -0.174360   \n",
      "19 -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -2.129256 -0.527820   \n",
      "\n",
      "          42        43        44        45        46        47        48  \\\n",
      "0   0.183389  0.987617 -1.182260  3.314465 -1.080738 -0.845098  0.660362   \n",
      "1  -0.904803 -0.242359 -1.182260 -0.520814 -1.230091 -1.500203 -0.368036   \n",
      "2  -0.904803  0.987617 -1.182260  3.314465 -1.103715  0.255478  0.099418   \n",
      "3  -1.122441  0.372629 -0.675256  0.757613 -0.747564 -0.615812 -0.928981   \n",
      "4  -0.904803 -2.702310 -1.182260 -0.520814 -1.222432  1.072573 -0.368036   \n",
      "5  -0.687165 -0.242359 -1.182260 -0.520814 -1.226262 -0.058972 -0.601763   \n",
      "6   0.401028 -0.242359 -1.182260 -0.520814 -1.210943  1.775321 -0.695254   \n",
      "7  -0.251888 -0.242359 -0.675256 -0.520814 -0.728416 -1.354624 -0.835490   \n",
      "8   0.183389  0.987617 -1.182260 -0.520814 -1.218603 -1.107140 -0.368036   \n",
      "9  -1.122441  0.987617 -1.182260 -0.520814 -1.207114  1.644300 -0.648508   \n",
      "10 -1.122441 -3.317298 -1.182260 -0.520814 -1.233921 -1.480046 -0.508272   \n",
      "11 -0.904803 -0.242359  0.845756  3.314465  1.044681  0.581575  1.127816   \n",
      "12  1.271582 -0.242359  0.338752 -0.520814  0.408970  1.120217 -0.134309   \n",
      "13 -0.904803 -0.242359  0.338752 -0.520814  0.374504 -1.500203 -0.181055   \n",
      "14 -1.122441 -2.087322  0.338752 -0.520814  0.320890 -1.456530 -0.274545   \n",
      "15 -0.034249 -0.242359  0.338752 -0.520814  0.393652 -0.386525 -0.134309   \n",
      "16  1.053943 -0.242359  0.338752 -0.520814  0.405141  0.203070 -0.040818   \n",
      "17 -1.122441 -2.087322 -0.168252  0.757613 -0.242059 -0.205875 -0.368036   \n",
      "18 -0.469526 -0.242359  0.338752 -0.520814  0.385993 -0.976119 -0.181055   \n",
      "19 -0.251888 -0.242359  0.338752 -0.520814  0.259616  1.498722 -1.022471   \n",
      "\n",
      "          49        50        51        52        53        54       55  \\\n",
      "0   0.129562 -0.183373 -0.030583 -1.303789 -0.627405  4.190320 -0.02423   \n",
      "1   0.129562 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "2  -0.893405 -0.183373 -0.030583 -1.303789 -0.627405  4.190320 -0.02423   \n",
      "3   0.860253 -0.183373 -0.030583 -1.303789 -0.627405  4.190320 -0.02423   \n",
      "4  -0.528060 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "5  -1.039544 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "6  -1.331820 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "7   0.567976 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "8  -0.528060 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "9   0.641045 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "10 -0.162715 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "11 -0.820336 -0.183373 -0.030583 -1.303789 -0.627405  4.190320 -0.02423   \n",
      "12 -1.477958 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "13 -2.062511 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "14  0.348769 -0.183373 -0.030583 -1.303789  1.593867 -0.238645 -0.02423   \n",
      "15 -1.551027 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "16 -0.089645 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "17 -1.112613 -0.183373 -0.030583 -1.303789  1.593867 -0.238645 -0.02423   \n",
      "18 -2.062511 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "19 -1.697165 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "\n",
      "          56        57        58        59        60        61        62  \\\n",
      "0  -0.113524 -0.250644 -0.437630  2.052381 -0.184981 -0.241153 -0.264626   \n",
      "1  -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "2  -0.113524 -0.250644 -0.437630  2.052381 -0.184981 -0.241153 -0.264626   \n",
      "3  -0.113524 -0.250644 -0.437630 -0.487239 -0.184981 -0.241153 -0.264626   \n",
      "4  -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "5  -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "6  -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "7  -0.113524 -0.250644 -0.437630  2.052381 -0.184981 -0.241153 -0.264626   \n",
      "8  -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "9  -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "10 -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "11 -0.113524 -0.250644 -0.437630  2.052381 -0.184981 -0.241153 -0.264626   \n",
      "12 -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "13 -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "14 -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "15 -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "16 -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "17 -0.113524 -0.250644  2.285036 -0.487239 -0.184981 -0.241153 -0.264626   \n",
      "18 -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "19 -0.113524 -0.250644 -0.437630  2.052381 -0.184981 -0.241153 -0.264626   \n",
      "\n",
      "          63        64        65       66        67        68        69  \\\n",
      "0  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "1  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "2  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "3  -0.184068  2.104756 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "4  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "5  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "6  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "7  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "8  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "9  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "10 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "11 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "12 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "13 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "14 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "15 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "16 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "17 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "18 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "19 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "\n",
      "          70        71        72        73        74        75        76  \\\n",
      "0  -0.052607 -0.413735 -0.685882 -0.138346  2.266158  0.256571 -0.843735   \n",
      "1  -0.052607  0.818296 -0.685882  0.641898 -0.056646  0.032424 -1.460288   \n",
      "2  -0.052607 -0.413735 -0.685882 -0.138346 -0.377615  0.032424 -0.227182   \n",
      "3  -0.052607 -0.413735 -0.685882  1.032020 -0.490799  0.032424  1.005924   \n",
      "4  -0.052607  2.358335  1.457977 -0.138346 -0.330314  0.480717 -0.227182   \n",
      "5  -0.052607 -0.413735  1.457977  0.641898  0.250809  1.153157 -0.843735   \n",
      "6  -0.052607  1.742319 -0.685882  0.251776 -0.281324  0.256571 -0.843735   \n",
      "7  -0.052607  0.818296 -0.685882  0.251776 -0.497556  1.825598 -0.843735   \n",
      "8  -0.052607  0.202280  1.457977 -0.918591 -0.509381 -1.088310  1.005924   \n",
      "9  -0.052607 -1.645767 -0.685882  0.641898 -0.124218  1.153157  1.005924   \n",
      "10 -0.052607 -1.029751 -0.685882  0.251776  4.431855 -1.088310 -0.227182   \n",
      "11 -0.052607 -0.105728 -0.685882 -2.869202 -0.203616 -0.640016 -0.227182   \n",
      "12 -0.052607  0.510288  1.457977 -0.528468 -0.304975  1.377304 -0.227182   \n",
      "13 -0.052607  0.202280  1.457977 -2.088957 -0.585401 -1.088310 -0.227182   \n",
      "14 -0.052607 -0.105728  1.457977 -2.479079 -0.517828 -0.415870 -2.076841   \n",
      "15 -0.052607  0.818296  1.457977 -0.528468 -0.573575 -1.088310 -0.227182   \n",
      "16 -0.052607  0.510288  1.457977 -1.308713 -0.397887  0.704864 -1.460288   \n",
      "17 -0.052607  0.202280 -0.685882  0.641898 -0.526275 -0.640016 -0.843735   \n",
      "18 -0.052607  0.202280  1.457977 -0.528468 -0.406333 -1.088310 -0.227182   \n",
      "19 -0.052607 -0.413735 -0.685882 -2.088957 -0.531343 -0.640016 -0.227182   \n",
      "\n",
      "          77        78        79        80        81        82        83  \\\n",
      "0  -1.701010  0.763486 -1.736451  0.101292 -1.086340  0.227290 -0.183151   \n",
      "1  -1.189353 -0.521153 -1.276990 -1.515574 -0.446991 -0.064402 -0.183151   \n",
      "2  -1.701010  0.763486 -1.736451  0.365270 -1.132007 -0.793633 -0.183151   \n",
      "3  -0.677696  2.048125 -0.682394 -1.529162 -0.127317  0.956521  5.459962   \n",
      "4  -0.677696 -0.521153 -0.709421 -0.030697 -0.995004 -1.085325 -0.183151   \n",
      "5  -0.677696  0.763486 -0.751892 -0.591651 -0.949336 -0.939479 -0.183151   \n",
      "6  -0.166039 -0.521153 -0.276987 -0.761037 -0.492659 -1.741633 -0.183151   \n",
      "7  -0.166039  0.763486 -0.199767  0.840431  0.192357  0.373136 -0.183151   \n",
      "8  -1.189353 -0.521153 -1.242241 -1.130606 -0.538327 -0.574864 -0.183151   \n",
      "9  -1.189353 -0.521153 -1.099384 -0.014198  0.923041  1.029444 -0.183151   \n",
      "10 -1.189353  3.332764 -1.118689  1.579570 -0.127317  0.081444 -0.183151   \n",
      "11  0.345618 -0.521153  0.271277 -0.690643 -0.995004 -0.793633 -0.183151   \n",
      "12 -1.189353 -0.521153 -1.168882  1.238429  0.055354 -1.595787 -0.183151   \n",
      "13  0.857275  3.332764  1.031897 -1.335923  1.014376 -2.106248 -0.183151   \n",
      "14  0.345618 -0.521153  0.352359 -1.284594 -0.081649  0.373136 -0.183151   \n",
      "15  0.345618 -0.521153  0.278999 -0.751561 -0.583994 -1.741633 -0.183151   \n",
      "16  0.345618  0.763486  0.475911  0.017299  1.014376 -0.210248 -0.183151   \n",
      "17 -0.166039 -0.521153 -0.234516 -0.245179  0.009686 -1.158248 -0.183151   \n",
      "18  0.345618 -0.521153  0.325331  0.243434 -0.309988 -2.106248 -0.183151   \n",
      "19  0.345618 -0.521153  0.255833 -0.309341 -0.949336 -1.595787 -0.183151   \n",
      "\n",
      "          84        85        86        87        88        89       90  \\\n",
      "0  -0.030405 -1.305431  1.594038 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "1  -0.030405 -1.305431  1.594038 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "2  -0.030405 -1.305431  1.594038 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "3  -0.030405 -1.305431 -0.627338 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "4  -0.030405  0.766031 -0.627338 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "5  -0.030405 -1.305431 -0.627338  4.208741 -0.024004 -0.122104 -0.25466   \n",
      "6  -0.030405 -1.305431  1.594038 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "7  -0.030405 -1.305431  1.594038 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "8  -0.030405  0.766031 -0.627338 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "9  -0.030405  0.766031 -0.627338 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "10 -0.030405 -1.305431 -0.627338  4.208741 -0.024004 -0.122104 -0.25466   \n",
      "11 -0.030405  0.766031 -0.627338 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "12 -0.030405  0.766031 -0.627338 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "13 -0.030405 -1.305431 -0.627338  4.208741 -0.024004 -0.122104 -0.25466   \n",
      "14 -0.030405 -1.305431  1.594038 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "15 -0.030405 -1.305431  1.594038 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "16 -0.030405 -1.305431 -0.627338  4.208741 -0.024004 -0.122104 -0.25466   \n",
      "17 -0.030405  0.766031 -0.627338 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "18 -0.030405  0.766031 -0.627338 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "19 -0.030405 -1.305431  1.594038 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "\n",
      "          91        92        93        94        95        96        97  \\\n",
      "0  -0.424337 -0.485053 -0.192544 -0.250032  3.699370 -0.188904 -0.469419   \n",
      "1   2.356620 -0.485053 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "2  -0.424337 -0.485053 -0.192544 -0.250032  3.699370 -0.188904 -0.469419   \n",
      "3  -0.424337 -0.485053 -0.192544 -0.250032 -0.270316 -0.188904  2.130294   \n",
      "4  -0.424337  2.061632 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "5  -0.424337 -0.485053 -0.192544 -0.250032 -0.270316 -0.188904  2.130294   \n",
      "6  -0.424337 -0.485053 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "7  -0.424337  2.061632 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "8  -0.424337 -0.485053  5.193607 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "9   2.356620 -0.485053 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "10 -0.424337  2.061632 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "11 -0.424337  2.061632 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "12 -0.424337 -0.485053  5.193607 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "13 -0.424337  2.061632 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "14 -0.424337 -0.485053  5.193607 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "15  2.356620 -0.485053 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "16 -0.424337 -0.485053  5.193607 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "17 -0.424337 -0.485053 -0.192544 -0.250032 -0.270316 -0.188904  2.130294   \n",
      "18 -0.424337 -0.485053  5.193607 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "19 -0.424337 -0.485053 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "\n",
      "          98        99       100       101       102       103       104  \\\n",
      "0  -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  1.124895   \n",
      "1  -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  0.198312   \n",
      "2  -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803 -1.345994   \n",
      "3  -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803 -0.419411   \n",
      "4  -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803 -0.110549   \n",
      "5  -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  0.816034   \n",
      "6  -0.210135  2.279713 -0.362571 -0.792162  0.797549 -0.056803 -1.037133   \n",
      "7  -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803 -1.037133   \n",
      "8  -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803 -0.419411   \n",
      "9  -0.210135 -0.438652 -0.362571  1.262369 -1.253842 -0.056803  1.124895   \n",
      "10 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803 -0.419411   \n",
      "11 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  1.742617   \n",
      "12 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803 -1.654855   \n",
      "13 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  0.198312   \n",
      "14 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  2.360340   \n",
      "15 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  1.742617   \n",
      "16 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  0.507173   \n",
      "17 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  0.198312   \n",
      "18 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803 -0.728272   \n",
      "19 -0.210135  2.279713 -0.362571 -0.792162  0.797549 -0.056803  0.507173   \n",
      "\n",
      "         105       106       107       108       109       110       111  \\\n",
      "0  -0.566421 -0.130578 -0.258403  0.776344  2.893107  3.394031  0.765135   \n",
      "1  -0.566421  1.420362 -0.107048  2.614119 -0.835530 -1.197872 -0.520352   \n",
      "2  -0.566421  0.644892 -0.206150  1.235787 -0.214090 -1.197872 -0.520352   \n",
      "3  -0.566421  0.644892 -0.388135 -0.831709  1.028789 -1.197872 -0.520352   \n",
      "4  -0.566421  1.032627 -0.143085  0.316900 -0.214090 -0.679013  2.050622   \n",
      "5  -0.566421  0.644892 -0.357504 -0.142544 -0.835530 -1.197872 -0.520352   \n",
      "6  -0.566421  1.032627 -0.348495  2.154675 -0.214090 -0.679013  2.050622   \n",
      "7  -0.566421  0.644892 -0.534085 -1.061431 -0.214090 -0.679013  0.765135   \n",
      "8   1.765471 -0.518313  0.105569 -0.831709  1.028789 -1.197872 -0.520352   \n",
      "9  -0.566421 -0.906048 -0.292638  0.316900  1.028789 -1.197872 -0.520352   \n",
      "10 -0.566421  0.644892  5.100272  1.695231 -0.214090 -0.679013 -0.520352   \n",
      "11 -0.566421  1.032627 -0.357504  0.776344 -0.214090  0.358705 -0.520352   \n",
      "12 -0.566421  0.644892  2.320031  1.924953 -2.699848 -0.160154  0.765135   \n",
      "13  1.765471  0.257157 -0.429578 -0.831709 -0.214090  0.877565  2.050622   \n",
      "14  1.765471 -1.293783 -0.415163  0.087178 -0.835530  0.358705 -0.520352   \n",
      "15  1.765471  0.644892 -0.526877  2.384397 -0.835530  0.877565 -0.520352   \n",
      "16  1.765471 -0.518313 -0.532283 -1.061431 -0.214090  0.358705 -0.520352   \n",
      "17 -0.566421  1.420362 -0.247592 -0.831709 -0.835530 -0.679013  0.765135   \n",
      "18  1.765471 -0.130578 -0.508859  0.546622 -0.214090  0.877565 -0.520352   \n",
      "19 -0.566421 -1.293783 -0.391739 -0.831709 -0.214090  0.358705 -0.520352   \n",
      "\n",
      "         112       113        114       115       116       117       118  \\\n",
      "0   4.122613 -0.802297 -10.160172 -0.038195 -0.182993 -0.029125 -1.306527   \n",
      "1  -1.223508 -1.422108  -0.226458 -0.110973 -0.182993 -0.029125  0.765388   \n",
      "2  -1.250924  0.359849  -0.688491 -0.474865 -0.182993 -0.029125  0.765388   \n",
      "3  -1.297923 -1.499584  -0.596085  1.053481 -0.182993 -0.029125  0.765388   \n",
      "4  -0.686937  0.127420  -0.503678 -1.057091  5.464691 -0.029125 -1.306527   \n",
      "5  -1.266590 -0.221224  -0.780898 -1.129870 -0.182993 -0.029125  0.765388   \n",
      "6  -0.655605  1.439961  -0.318865 -1.493761  5.464691 -0.029125 -1.306527   \n",
      "7  -0.698687 -0.918511   0.281778  0.616811 -0.182993 -0.029125 -1.306527   \n",
      "8  -1.262673 -0.873813  -0.642288 -0.474865 -0.182993 -0.029125  0.765388   \n",
      "9  -1.278340  0.437325  -0.596085  0.762367 -0.182993 -0.029125  0.765388   \n",
      "10 -0.522441  0.476064   0.974828  0.180140 -0.182993 -0.029125 -1.306527   \n",
      "11  0.253040  1.002447  -0.411271 -1.202648 -0.182993 -0.029125  0.765388   \n",
      "12 -0.166033 -1.499584   1.113438 -1.202648 -0.182993 -0.029125 -1.306527   \n",
      "13  0.832692 -1.298145  -0.134052 -2.148766 -0.182993 -0.029125 -1.306527   \n",
      "14  0.323538 -1.393934  -0.688491 -0.183751 -0.182993 -0.029125  0.765388   \n",
      "15  0.836609  0.650385   1.159641 -2.148766 -0.182993 -0.029125  0.765388   \n",
      "16  0.311788 -0.730780  -0.134052 -0.329308 -0.182993 -0.029125  0.765388   \n",
      "17 -0.702604 -1.112202  -0.411271 -1.202648 -0.182993 -0.029125  0.765388   \n",
      "18  0.820943 -0.536663  -0.134052 -1.930431 -0.182993 -0.029125  0.765388   \n",
      "19  0.162959  1.310695  -1.011915 -1.712096 -0.182993 -0.029125 -1.306527   \n",
      "\n",
      "         119       120      121       122      123       124       125  \\\n",
      "0   1.595663 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "1  -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864  2.064688   \n",
      "2  -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "3  -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "4  -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "5  -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "6  -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "7   1.595663 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "8  -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "9  -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "10  1.595663 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "11 -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "12  1.595663 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864  2.064688   \n",
      "13 -0.626699  4.202876 -0.02423 -0.126786 -0.25707 -0.412864  2.064688   \n",
      "14 -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864  2.064688   \n",
      "15 -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "16 -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "17 -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "18 -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "19  1.595663 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864  2.064688   \n",
      "\n",
      "         126       127       128       129       130       131       132  \\\n",
      "0  -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518  2.281729   \n",
      "1  -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "2  -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518  2.281729   \n",
      "3  -0.196634  3.960014 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "4  -0.196634 -0.252524 -0.274734 -0.191876  2.147662 -0.215518 -0.438264   \n",
      "5  -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518  2.281729   \n",
      "6  -0.196634 -0.252524 -0.274734 -0.191876  2.147662 -0.215518 -0.438264   \n",
      "7  -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518  2.281729   \n",
      "8   5.085578 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "9  -0.196634  3.960014 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "10 -0.196634 -0.252524 -0.274734 -0.191876  2.147662 -0.215518 -0.438264   \n",
      "11 -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518  2.281729   \n",
      "12 -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "13 -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "14 -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "15 -0.196634  3.960014 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "16  5.085578 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "17 -0.196634 -0.252524 -0.274734 -0.191876  2.147662 -0.215518 -0.438264   \n",
      "18 -0.196634 -0.252524  3.639890 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "19 -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "\n",
      "         133       134       135        136       137       138       139  \\\n",
      "0  -0.350214 -0.793361 -1.252524  18.233291  1.449144 -0.513521 -0.685882   \n",
      "1  -0.350214 -0.793361  0.798388  -0.054845 -0.111482 -0.513521 -0.685882   \n",
      "2  -0.350214 -0.793361  0.798388  -0.054845 -0.111482 -0.513521 -0.685882   \n",
      "3  -0.350214 -0.793361  0.798388  -0.054845 -0.423607 -0.513521 -0.685882   \n",
      "4  -0.350214 -0.793361  0.798388  -0.054845 -1.672108 -0.513521  1.457977   \n",
      "5  -0.350214 -0.793361  0.798388  -0.054845 -1.047857 -0.513521  1.457977   \n",
      "6  -0.350214 -0.793361  0.798388  -0.054845 -0.423607 -0.513521 -0.685882   \n",
      "7  -0.350214 -0.793361  0.798388  -0.054845  0.512768 -0.513521 -0.685882   \n",
      "8  -0.350214 -0.793361  0.798388  -0.054845  2.073394  1.947341  1.457977   \n",
      "9  -0.350214 -0.793361  0.798388  -0.054845  0.512768 -0.513521 -0.685882   \n",
      "10 -0.350214  1.260460 -1.252524  -0.054845  3.009769 -0.513521 -0.685882   \n",
      "11 -0.350214 -0.793361  0.798388  -0.054845 -0.735732 -0.513521 -0.685882   \n",
      "12 -0.350214 -0.793361  0.798388  -0.054845  0.824893 -0.513521  1.457977   \n",
      "13 -0.350214 -0.793361  0.798388  -0.054845 -0.111482  1.947341  1.457977   \n",
      "14 -0.350214 -0.793361  0.798388  -0.054845 -0.423607 -0.513521  1.457977   \n",
      "15 -0.350214 -0.793361  0.798388  -0.054845 -0.735732 -0.513521  1.457977   \n",
      "16 -0.350214 -0.793361  0.798388  -0.054845 -0.111482  1.947341  1.457977   \n",
      "17 -0.350214 -0.793361  0.798388  -0.054845  1.449144 -0.513521 -0.685882   \n",
      "18 -0.350214 -0.793361  0.798388  -0.054845  0.824893 -0.513521  1.457977   \n",
      "19 -0.350214 -0.793361  0.798388  -0.054845 -0.111482 -0.513521 -0.685882   \n",
      "\n",
      "         race_id order  \n",
      "0   201101010111     5  \n",
      "1   201101010111     2  \n",
      "2   201101010111    11  \n",
      "3   201101010111     7  \n",
      "4   201101010111     1  \n",
      "5   201101010111     6  \n",
      "6   201101010111     3  \n",
      "7   201101010111     8  \n",
      "8   201101010111     4  \n",
      "9   201101010111    12  \n",
      "10  201101010111    14  \n",
      "11  201101010112     8  \n",
      "12  201101010112     4  \n",
      "13  201101010112     6  \n",
      "14  201101010112     9  \n",
      "15  201101010112     3  \n",
      "16  201101010112     5  \n",
      "17  201101010112     2  \n",
      "18  201101010112    10  \n",
      "19  201101010112     7  \n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data.dtypes)\n",
    "print(data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_series_data(raw_data):\n",
    "    number_of_race = raw_data.race_id.nunique()\n",
    "    time_series_data = np.full((number_of_race, 18, 140), -float('inf'))\n",
    "    label = np.full((number_of_race, 18), 19)\n",
    "    race_number = 0\n",
    "    horse_number = 0\n",
    "    for i in range(len(raw_data)):\n",
    "        if i == 0:\n",
    "#             print(race_number)\n",
    "#             print(horse_number)\n",
    "#             print(raw_data.iloc[i].order)\n",
    "            label[race_number][horse_number] = float(raw_data.iloc[i].order)\n",
    "            time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "            horse_number += 1\n",
    "            continue\n",
    "        if data.iloc[i].race_id != data.iloc[i-1].race_id:\n",
    "            race_number += 1\n",
    "            horse_number = 0\n",
    "            label[race_number][horse_number] = float(raw_data.iloc[i].order)\n",
    "            time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "            horse_number += 1\n",
    "        else:\n",
    "            label[race_number][horse_number] = float(raw_data.iloc[i].order)\n",
    "            time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "            horse_number += 1\n",
    "    return time_series_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8489, 18, 140)\n",
      "(8489, 18)\n"
     ]
    }
   ],
   "source": [
    "X, y_order = create_time_series_data(data)\n",
    "print(X.shape)\n",
    "print(y_order.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "X = X.astype('float32')\n",
    "print(X.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf]\n",
      "[ 5  2 11  7  1  6  3  8  4 12 14 19 19 19 19 19 19 19]\n"
     ]
    }
   ],
   "source": [
    "print(X[0][11])\n",
    "print(y_order[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "(8489, 18, 20)\n",
      "(8489, 18, 140)\n"
     ]
    }
   ],
   "source": [
    "# creating X,y (parameters and target)\n",
    "y = np_utils.to_categorical(y_order, dtype='float32')\n",
    "print(y[0])\n",
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(data.race_id.value_counts().plot.hist(bins=18,range=(1,18)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(pd.DataFrame(y_order.flatten()).plot.hist(bins=18))## ,ylim=(0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUFFER_SIZE = 1000\n",
    "# BATCH_SIZE = 128\n",
    "# def make_batches(ds):\n",
    "#     return (\n",
    "#       ds\n",
    "# #       .cache()\n",
    "# #       .shuffle(BUFFER_SIZE)\n",
    "#       .batch(BATCH_SIZE)\n",
    "# #       .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#       .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "# X = tf.convert_to_tensor(X)\n",
    "# y = tf.convert_to_tensor(y)\n",
    "# X = make_batches(X)\n",
    "# y = make_batches(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.3145431  -0.50991476  0.5901306   0.089023    0.3633046  -0.555636\n",
      " -0.38537532  0.8550409  -0.7798238   0.7798238   0.6798608  -0.66338956\n",
      " -0.18337315 -0.02931084 -1.3091208   1.597206   -0.23655261 -0.02236226\n",
      " -0.24513794  2.2255282  -0.4990219  -0.17550196 -0.2203592  -0.25591546\n",
      " -0.17979759 -0.49679595 -0.19606745 -0.4557965  -0.22428839 -0.71253043\n",
      "  0.7906001  -1.1524768  -0.4643385  -1.249982   -0.29165798 -0.1354737\n",
      " -0.29401696  0.5838889  -0.4724972   0.4967511  -1.3390157  -0.53560555\n",
      "  0.6186662   0.98761696  0.8457565  -0.5208136   0.7919285  -1.0952293\n",
      "  0.4266353   1.6640126  -0.18337315 -0.03058303 -1.3037888   1.5938668\n",
      " -0.23864526 -0.02422995 -0.1135239  -0.25064442  2.2850358  -0.487239\n",
      " -0.18498135 -0.24115273 -0.26462567 -0.18406838 -0.47511455 -0.2061855\n",
      " -0.44352028 -0.37101352 -0.7890041   0.79361564 -0.0526072   0.20228021\n",
      " -0.685882   -0.52846843 -0.49079907 -0.6400163   1.0059242   1.368932\n",
      " -0.52115315  1.2905855  -1.249058    0.10102149  1.6128289  -0.18315147\n",
      " -0.03040453  0.76603085 -0.6273377  -0.23760074 -0.02400442 -0.12210409\n",
      " -0.25465992  2.3566198  -0.48505253 -0.1925444  -0.25003174 -0.27031633\n",
      " -0.18890353 -0.46941882 -0.21013454 -0.4386517  -0.36257088 -0.7921616\n",
      "  0.7975488  -0.0568033   0.19831172 -0.5664212  -0.9060477  -0.54129195\n",
      " -0.37226558  1.0287887   0.8775645  -0.52035177  0.80527645  0.97966\n",
      " -0.27266148  1.5629289  -0.18299298 -0.02912458 -1.3065268   1.5956632\n",
      " -0.23793232 -0.02422995 -0.12678613 -0.25707033  2.4221058  -0.4843346\n",
      " -0.19663447 -0.25252435 -0.27473357 -0.19187577 -0.46562257 -0.21551774\n",
      " -0.43826422 -0.35021392 -0.79336107  0.79838777 -0.05484473  0.51276827\n",
      " -0.5135207  -0.685882  ]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6791, 18, 140)\n",
      "(1698, 18, 140)\n",
      "(6791, 18, 20)\n",
      "(1698, 18, 20)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataset with batch size\n",
    "batch_size = 2048\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=6791).batch(batch_size)\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "valid_dataset = valid_dataset.shuffle(buffer_size=1698).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters\n",
    "num_layers = 1\n",
    "d_model = 140\n",
    "num_heads = 1\n",
    "d_ffn = 8\n",
    "pe_input = 18\n",
    "target_size = 20\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# set up model\n",
    "trans_race = transformer.TransRace(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ffn=d_ffn,\n",
    "    pe_input=pe_input,\n",
    "    target_size=target_size,\n",
    "    rate=dropout_rate,\n",
    ")\n",
    "\n",
    "# trans_race.build(input_shape=(None,18,139))\n",
    "opt = optimizers.Adam(lr = 0.0001, clipvalue = 2.)\n",
    "trans_race.compile(\n",
    "    optimizer=opt,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "#     patience=10,\n",
    "#     min_delta=0.001,\n",
    "#     restore_best_weights=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask shape after creating (?, 18)\n",
      "mask shape after creating (?, 18)\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 1, 140) split_head\n",
      "(?, 18, 1, 140) split_head\n",
      "(?, 18, 1, 140) split_head\n",
      "(?, 1, 18, 140) q_shape\n",
      "(?, 1, 18, 18) scaled_attention_shape\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in converted code:\n\n    /mnt/c/Users/44yos/RacePrediction/resnet_win5/models/transformer.py:201 call  *\n        enc_output = self.encoder(inp, training, enc_padding_mask)\n    /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:854 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /mnt/c/Users/44yos/RacePrediction/resnet_win5/models/transformer.py:182 call  *\n        x = self.enc_layers[i](x, training, mask)\n    /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:854 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /mnt/c/Users/44yos/RacePrediction/resnet_win5/models/transformer.py:149 call  *\n        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n    /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:854 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /mnt/c/Users/44yos/RacePrediction/resnet_win5/models/transformer.py:116 call  *\n        scaled_attention = scaled_dot_product_attention(\n    /mnt/c/Users/44yos/RacePrediction/resnet_win5/models/transformer.py:63 scaled_dot_product_attention  *\n        mask = mask[:, scaled_attention_logits.shape[1], mask.shape[1]]\n    /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:802 _slice_helper\n        name=name)\n    /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:968 strided_slice\n        shrink_axis_mask=shrink_axis_mask)\n    /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py:10392 strided_slice\n        shrink_axis_mask=shrink_axis_mask, name=name)\n    /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py:794 _apply_op_helper\n        op_def=op_def)\n    /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py:507 new_func\n        return func(*args, **kwargs)\n    /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:3371 create_op\n        attrs, op_def, compute_device)\n    /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:3440 _create_op_internal\n        op_def=op_def)\n    /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1784 __init__\n        control_input_ops)\n    /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1624 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Index out of range using input dim 2; input has only 2 dims for 'trans_race/encoder/encoder_layer/multi_head_attention/strided_slice_2' (op: 'StridedSlice') with input shapes: [?,18], [3], [3], [3] and with computed input tensors: input[3] = <1 1 1>.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-43d3d2af3485>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#                         callbacks=[early_stopping],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# hide the output because we have so many epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                         )\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# trans_race.save_weights(\"../models/results/transfomer1.h5\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2416\u001b[0m     \u001b[0;31m# First, we build the model on the fly if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m       \u001b[0mall_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_model_with_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m       \u001b[0mis_build_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_build_model_with_inputs\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m   2619\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2620\u001b[0m       \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2621\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2622\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_dict_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[0;34m(self, inputs, outputs, training)\u001b[0m\n\u001b[1;32m   2706\u001b[0m           \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2707\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2708\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2709\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2710\u001b[0m         \u001b[0;31m# This Model or a submodel is dynamic and hasn't overridden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m                   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in converted code:\n\n    /mnt/c/Users/44yos/RacePrediction/resnet_win5/models/transformer.py:201 call  *\n        enc_output = self.encoder(inp, training, enc_padding_mask)\n    /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:854 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /mnt/c/Users/44yos/RacePrediction/resnet_win5/models/transformer.py:182 call  *\n        x = self.enc_layers[i](x, training, mask)\n    /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:854 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /mnt/c/Users/44yos/RacePrediction/resnet_win5/models/transformer.py:149 call  *\n        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n    /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:854 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /mnt/c/Users/44yos/RacePrediction/resnet_win5/models/transformer.py:116 call  *\n        scaled_attention = scaled_dot_product_attention(\n    /mnt/c/Users/44yos/RacePrediction/resnet_win5/models/transformer.py:63 scaled_dot_product_attention  *\n        mask = mask[:, scaled_attention_logits.shape[1], mask.shape[1]]\n    /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:802 _slice_helper\n        name=name)\n    /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:968 strided_slice\n        shrink_axis_mask=shrink_axis_mask)\n    /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py:10392 strided_slice\n        shrink_axis_mask=shrink_axis_mask, name=name)\n    /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py:794 _apply_op_helper\n        op_def=op_def)\n    /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py:507 new_func\n        return func(*args, **kwargs)\n    /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:3371 create_op\n        attrs, op_def, compute_device)\n    /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:3440 _create_op_internal\n        op_def=op_def)\n    /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1784 __init__\n        control_input_ops)\n    /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1624 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Index out of range using input dim 2; input has only 2 dims for 'trans_race/encoder/encoder_layer/multi_head_attention/strided_slice_2' (op: 'StridedSlice') with input shapes: [?,18], [3], [3], [3] and with computed input tensors: input[3] = <1 1 1>.\n"
     ]
    }
   ],
   "source": [
    "history = trans_race.fit(\n",
    "                        X_train, \n",
    "                        y_train,\n",
    "                        validation_data=([X_valid, y_valid]),\n",
    "                        epochs=2,\n",
    "#                         callbacks=[early_stopping],\n",
    "                        verbose=True, # hide the output because we have so many epochs\n",
    "                        )\n",
    "# trans_race.save_weights(\"../models/results/transfomer1.h5\")\n",
    "\n",
    "# the problem is gradient exploding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \n",
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "# for loss\n",
    "axL.plot(history.history['loss'],label=\"loss for training\")\n",
    "axL.plot(history.history['val_loss'],label=\"loss for validation\")\n",
    "axL.set_title('model loss')\n",
    "axL.set_xlabel('epoch')\n",
    "axL.set_ylabel('loss')\n",
    "axL.legend(loc='upper right')\n",
    "axR.plot(history.history['acc'],label=\"acc for training\")\n",
    "axR.plot(history.history['val_acc'],label=\"acc for validation\")\n",
    "axR.set_title('model accuracy')\n",
    "axR.set_xlabel('epoch')\n",
    "axR.set_ylabel('accuracy')\n",
    "axR.legend(loc='upper right')\n",
    "# figureの保存\n",
    "# plt.savefig(\"../models/results/trans_race_result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03687921  0.0128139   0.01421263 ... -0.0176291  -0.05276658\n",
      "  -0.12177539]\n",
      " [ 0.03647849 -0.16659288  0.03827428 ...  0.06829555 -0.16240771\n",
      "  -0.11143278]\n",
      " [-0.03427426  0.06320532  0.09840133 ... -0.02185821  0.04390921\n",
      "   0.0838508 ]\n",
      " ...\n",
      " [-0.11340646 -0.09481942  0.00417771 ...  0.07333632 -0.09708207\n",
      "  -0.13780057]\n",
      " [-0.12116326 -0.00438668 -0.16915095 ... -0.16030876  0.0212271\n",
      "   0.07967956]\n",
      " [-0.16381697 -0.08192709  0.01520679 ... -0.17817253 -0.15416843\n",
      "   0.06735547]]\n"
     ]
    }
   ],
   "source": [
    "l1 = trans_race.layers[0]\n",
    "print(l1.get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "pred = trans_race.predict(X_valid)\n",
    "y_pred = np.argmax(pred, axis = 2)\n",
    "y_ans = np.argmax(y_valid, axis = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1698, 18, 20)\n",
      "(1698, 18)\n",
      "(1698, 18)\n"
     ]
    }
   ],
   "source": [
    "print(pred.shape)\n",
    "print(y_pred.shape)\n",
    "print(y_ans.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8 10  9  3  4  7  5  6  1  2 19 19 19 19 19 19 19 19]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0.07603716790995943\n"
     ]
    }
   ],
   "source": [
    "print(y_ans[0])\n",
    "print(y_pred[1])\n",
    "print(np.mean(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.43437406, -0.50991476,  1.400889  , ...,  1.7612689 ,\n",
       "        -0.5135207 , -0.685882  ],\n",
       "       [ 0.43437406, -0.50991476,  1.400889  , ...,  0.20064315,\n",
       "        -0.5135207 , -0.685882  ],\n",
       "       [ 0.43437406, -0.50991476,  1.400889  , ..., -0.11148198,\n",
       "        -0.5135207 , -0.685882  ],\n",
       "       ...,\n",
       "       [ 0.43437406, -0.50991476,  1.400889  , ...,  1.4491436 ,\n",
       "        -0.5135207 , -0.685882  ],\n",
       "       [ 0.43437406, -0.50991476,  1.400889  , ...,  0.20064315,\n",
       "         1.9473411 ,  1.4579768 ],\n",
       "       [       -inf,        -inf,        -inf, ...,        -inf,\n",
       "               -inf,        -inf]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.0076560659599528855\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(y_pred)):\n",
    "    for j in range(len(y_pred[0])):\n",
    "        if (y_pred[i][j] == y_ans[i][j]):\n",
    "            correct += 1\n",
    "accuracy = correct / len(y_pred)\n",
    "print(\"accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 何位に予想した？　何位が含まれていた？\n",
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "label = ['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19']\n",
    "\n",
    "axL.hist(y_pred.flatten(), bins = 20, label = \"pred_order\")##, range = (1,21)\n",
    "axL.set_xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19])\n",
    "axL.set_xticklabels(label)\n",
    "axL.set_title('pred order distribution')\n",
    "axL.legend()\n",
    "axR.hist(y_ans.flatten(), bins = 20, label = \"ans_order\")##, range = (1,21)\n",
    "axR.set_xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19])\n",
    "axR.set_xticklabels(label)\n",
    "axR.set_title('ans order distribution')\n",
    "axR.legend()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.unique(pred_order))\n",
    "# print(np.unique(Y_ans))\n",
    "# u, c = np.unique(pred_order, return_counts = True)\n",
    "# print(u)\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy of the first horse\n",
    "i = 0\n",
    "correct_first = 0\n",
    "all_f = 0\n",
    "increase = 0\n",
    "X_test_inv = standard_scale.inverse_transform(X_test)\n",
    "X_test_inv_df = pd.DataFrame(X_test_inv)\n",
    "odds = X_test_inv_df[4].values\n",
    "hit_odds = []\n",
    "select = []\n",
    "\n",
    "for i in range(len(Y_ans)):\n",
    "    if (pred_order[i] == 1):  # いちい予想した総数  & (pred[i][1]*odds[i] > 1.0)) | (pred[i][1]> 0.25)\n",
    "        all_f = all_f + 1\n",
    "        if (Y_ans[i] == 1):\n",
    "            correct_first = correct_first + 1   #　一致した総数\n",
    "            increase += odds[i]\n",
    "            hit_odds.append(odds[i])\n",
    "            \n",
    "            \n",
    "            \n",
    "print(\"all_f: \", all_f)\n",
    "print(\"correct_first: \", correct_first)\n",
    "print(\"hit odds average: \", np.array(hit_odds).mean())\n",
    "print(\"spent money:\", all_f * 100)\n",
    "revenue = (increase - all_f) * 100\n",
    "retrive = increase / all_f\n",
    " \n",
    "print(\"retrive rate: \", retrive) \n",
    "print(\"revenue: \", revenue)\n",
    "accurate_rate = correct_first / all_f\n",
    "print(\"accuracy: \",accurate_rate)\n",
    "#print(\"\\n\".join(map(str,hit_odds)))\n",
    "print(\"min: \", min(hit_odds))\n",
    "print(\"mid: \", np.median(np.array(hit_odds)))\n",
    "print(\"max: \", max(hit_odds))\n",
    "\n",
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "axL.hist(hit_odds, bins = 50, label = \"hit_odds\")\n",
    "axL.set_title('hit odds distribution')\n",
    "axL.legend()\n",
    "axR.hist(odds, bins = 50, label = \"odds\", range = (0,40))\n",
    "axR.set_title('all odds distribution')\n",
    "axR.legend()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一位だった時一位予想していた確率\n",
    "i = 0\n",
    "correct_first = 0\n",
    "all_f = 0\n",
    "odds_f = []\n",
    "p_rate_f = []\n",
    "\n",
    "all_f_odds = []\n",
    "\n",
    "for i in range(len(Y_ans)):\n",
    "    if (Y_ans[i] == 1):  # 一位の総数\n",
    "        all_f = all_f + 1\n",
    "        all_f_odds.append(odds[i])\n",
    "        if (pred_order[i] == 1):\n",
    "            correct_first = correct_first + 1   #　一致した総数\n",
    "            odds_f.append(odds[i])\n",
    "            p_rate_f.append(pred[i][1])\n",
    "\n",
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "axL.scatter(p_rate_f, odds_f)  \n",
    "axL.set_title('correlation odss and prediction')\n",
    "#axL.xlabel('prediction rate first')\n",
    "#axL.ylabel('odds')\n",
    "axR.hist(odds_f, bins = 50, label = \"odds\")\n",
    "axR.set_title('all first odds distribution')\n",
    "axR.legend()\n",
    "\n",
    "fig.show()\n",
    "accurate_rate = correct_first / all_f\n",
    "print(\"accuracy: \",accurate_rate)\n",
    "print(\"all_f_odds average: \", np.array(all_f_odds).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ensamble log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = resnet.resrace(X_test.shape[1], 19)\n",
    "model1.load_weights(\"model/win5_resrace_model_best1.h5\")\n",
    "pred1 = model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = resnet.resrace(X_test.shape[1], 19)\n",
    "model2.load_weights(\"model/win5_resrace_model_best2.h5\")\n",
    "pred2 = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = resnet.resrace(X_test.shape[1], 19)\n",
    "model3.load_weights(\"model/win5_resrace_model_best3.h5\")\n",
    "pred3 = model3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = resnet.resrace(X_test.shape[1], 19)\n",
    "model4.load_weights(\"model/win5_resrace_model_best4.h5\")\n",
    "pred4 = model4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = resnet.resrace(X_test.shape[1], 19)\n",
    "model5.load_weights(\"model/win5_resrace_model_best5.h5\")\n",
    "pred5 = model5.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = resnet.resrace(X_test.shape[1], 19)\n",
    "model6.load_weights(\"model/win5_resrace_model_best6.h5\")\n",
    "pred6 = model6.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = resnet.resrace(X_test.shape[1], 19)\n",
    "model7.load_weights(\"model/win5_resrace_model_best7.h5\")\n",
    "pred7 = model7.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = resnet.resrace(X_test.shape[1], 19)\n",
    "model8.load_weights(\"model/win5_resrace_model_best8.h5\")\n",
    "pred8 = model8.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model9 = resnet.resrace(X_test.shape[1], 19)\n",
    "model9.load_weights(\"model/win5_resrace_model_best9.h5\")\n",
    "pred9 = model9.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10 = resnet.resrace(X_test.shape[1], 19)\n",
    "model10.load_weights(\"model/win5_resrace_model_best10.h5\")\n",
    "pred10 = model10.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pred1 = np.log(pred1)\n",
    "log_pred2 = np.log(pred2)\n",
    "log_pred3 = np.log(pred3)\n",
    "log_pred4 = np.log(pred4)\n",
    "log_pred5 = np.log(pred5)\n",
    "# log_pred6 = np.log(pred6)\n",
    "# log_pred7 = np.log(pred7)\n",
    "# log_pred8 = np.log(pred8)\n",
    "# log_pred9 = np.log(pred9)\n",
    "# log_pred10 = np.log(pred10)\n",
    "\n",
    "sum_pred = log_pred1 + log_pred2 + log_pred3 + log_pred4 + log_pred5 #+ log_pred6 + log_pred7 + log_pred8 + log_pred9 + log_pred10\n",
    "pred_order = np.argmax(sum_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sum_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pred_order[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
