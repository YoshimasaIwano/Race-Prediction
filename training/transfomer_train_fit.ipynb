{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pprint\n",
    "# import sys\n",
    "\n",
    "# pprint.pprint(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "# pd.set_option('display.max_rows', 200)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/mnt/c/Users/44yos/RacePrediction/resnet_win5')\n",
    "from os import path\n",
    "import time\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import tensorflow as tf\n",
    "# from tensorflow.python import keras\n",
    "# from tensorflow.python.keras.models import Sequential\n",
    "# from tensorflow.python.keras.layers import Dense, Dropout\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from tensorflow.python.keras import optimizers\n",
    "# from tensorflow.python.keras.models import load_model\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "# from scipy.stats import norm\n",
    "\n",
    "from pickle import dump\n",
    "\n",
    "# from models import resnet\n",
    "from models import transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race_id                 int64\n",
      "race_round              int64\n",
      "ground_condition        int64\n",
      "total_horse_number      int64\n",
      "order                   int64\n",
      "                       ...   \n",
      "ground_type_芝_3       float64\n",
      "ground_type_障_3       float64\n",
      "horse_weight_dif_3    float64\n",
      "same_jockey_3         float64\n",
      "same_jockey           float64\n",
      "Length: 150, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = pd.read_csv(\"../data/csv/data.csv\", sep = \",\")\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust columns type\n",
    "data['race_id'] = data['race_id'].astype(str)\n",
    "# data['race_round'] = data['race_round'].astype(str)\n",
    "# #data['total_horse_number'] = data['total_horse_number'].astype(str)\n",
    "data['order'] = data['order'].astype(str)\n",
    "# data['frame_number'] = data['frame_number'].astype(str)\n",
    "# data['horse_number'] = data['horse_number'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete race day information\n",
    "data = data.drop([\"horse_number\", \"half_order\", \"goal_time\" ,\"last_time\", \"horse_weight\", \"horse_weight_dif\", \"frame_number\",\"pop\"], axis = 1)\n",
    "# \"race_round\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standarlization \n",
    "no_scale_data = data[['race_id','order']]\n",
    "scale_columns = data.drop(['race_id','order'], axis=1).columns.values\n",
    "standard_scale = StandardScaler()\n",
    "data = pd.DataFrame(standard_scale.fit_transform(data[scale_columns]))\n",
    "data = pd.concat([data, no_scale_data], axis=1)\n",
    "dump(standard_scale, open(\"standard_scale.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating time series data\n",
    "# def return_one_race(all_horse):\n",
    "#     print(len(all_horse))\n",
    "#     one_race = np.full((18, 139), -float('inf'))\n",
    "# #     print(len(one_race))\n",
    "# #     print(len(one_race[0]))\n",
    "#     for i, one_horse in all_horse.iterrows():\n",
    "#         print(i)\n",
    "# #         print(one_horse)\n",
    "#         one_race[i] = one_horse.drop(['race_id']).values\n",
    "#     print(one_race)\n",
    "#     return one_race\n",
    "\n",
    "# def create_time_series_data(raw_data):\n",
    "#     time_series_data = []\n",
    "#     time_series_data.append(raw_data.groupby(['race_id']).apply(return_one_race))\n",
    "#     return time_series_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92033, 142)\n",
      "0          float64\n",
      "1          float64\n",
      "2          float64\n",
      "3          float64\n",
      "4          float64\n",
      "            ...   \n",
      "137        float64\n",
      "138        float64\n",
      "139        float64\n",
      "race_id     object\n",
      "order       object\n",
      "Length: 142, dtype: object\n",
      "           0         1         2         3         4         5         6  \\\n",
      "0   0.434374 -0.509915 -0.220628  1.545825 -0.874906  2.530312 -0.385375   \n",
      "1   0.434374 -0.509915 -0.220628  0.089023 -0.874906 -0.643375 -0.385375   \n",
      "2   0.434374 -0.509915 -0.220628  0.817424 -0.255801 -0.554265 -0.385375   \n",
      "3   0.434374 -0.509915 -0.220628  1.545825  0.363305 -0.491203 -0.385375   \n",
      "4   0.434374 -0.509915 -0.220628  0.817424 -0.874906 -0.591280 -0.385375   \n",
      "5   0.434374 -0.509915 -0.220628  0.089023 -0.874906 -0.563862 -0.385375   \n",
      "6   0.434374 -0.509915 -0.220628  0.089023 -0.874906 -0.484348 -0.385375   \n",
      "7   0.434374 -0.509915 -0.220628  0.089023 -0.874906 -0.456930 -0.385375   \n",
      "8   0.434374 -0.509915 -0.220628  0.089023 -0.255801 -0.439108 -0.385375   \n",
      "9   0.434374 -0.509915 -0.220628  0.817424 -0.874906 -0.067587 -0.385375   \n",
      "10  0.434374 -0.509915 -0.220628  0.817424 -2.732221  0.099665 -0.385375   \n",
      "11  1.308833 -0.509915 -1.436765  0.089023 -0.255801 -0.574829 -0.385375   \n",
      "12  1.308833 -0.509915 -1.436765 -0.639378 -0.255801  0.180550 -0.385375   \n",
      "13  1.308833 -0.509915 -1.436765 -0.639378 -0.255801 -0.639262 -0.385375   \n",
      "14  1.308833 -0.509915 -1.436765 -1.367779 -2.113116 -0.562491 -0.385375   \n",
      "15  1.308833 -0.509915 -1.436765 -0.639378 -0.255801 -0.519992 -0.385375   \n",
      "16  1.308833 -0.509915 -1.436765 -0.639378 -0.255801  0.620616 -0.385375   \n",
      "17  1.308833 -0.509915 -1.436765 -1.367779 -2.113116 -0.314354 -0.385375   \n",
      "18  1.308833 -0.509915 -1.436765 -0.639378 -0.255801 -0.063475 -0.385375   \n",
      "19  1.308833 -0.509915 -1.436765 -0.639378 -0.255801 -0.618698 -0.385375   \n",
      "\n",
      "           7         8         9        10       11        12        13  \\\n",
      "0  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "1  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "2  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "3  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "4  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "5  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "6  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "7  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "8  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "9  -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "10 -1.192543 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "11  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "12  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "13  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "14  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "15  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "16  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "17  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "18  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "19  0.343145 -0.779824  0.779824  0.679861 -0.66339 -0.183373 -0.029311   \n",
      "\n",
      "          14        15        16        17        18        19        20  \\\n",
      "0   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "1   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "2   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "3   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "4   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "5   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "6   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "7   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "8   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "9   0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "10  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "11  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "12  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "13  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "14  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "15  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "16  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "17  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "18  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "19  0.763871 -0.626093 -0.236553 -0.022362 -0.245138 -0.449332 -0.499022   \n",
      "\n",
      "          21        22        23       24        25        26        27  \\\n",
      "0  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "1  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "2  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "3  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "4  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "5  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "6  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "7  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "8  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "9  -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "10 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "11 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "12 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "13 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "14 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "15 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "16 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "17 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "18 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "19 -0.175502 -0.220359 -0.255915  5.56181 -0.496796 -0.196067 -0.455797   \n",
      "\n",
      "          28        29        30        31        32        33        34  \\\n",
      "0  -0.224288 -0.712530  0.790600 -0.149783 -0.464338 -0.226190 -0.291658   \n",
      "1  -0.224288  1.403449 -1.264862 -0.507064  0.494861 -0.509254 -0.291658   \n",
      "2  -0.224288 -0.712530  0.790600 -1.152477 -0.464338 -1.249982 -0.291658   \n",
      "3   4.458546 -0.712530 -1.264862 -1.152477 -0.464338 -1.249982 -0.291658   \n",
      "4  -0.224288  1.403449 -1.264862 -1.152477 -0.464338 -1.249982 -0.291658   \n",
      "5  -0.224288  1.403449 -1.264862 -0.762540  0.934495 -0.155093 -0.291658   \n",
      "6  -0.224288  1.403449 -1.264862 -1.152477 -0.464338 -1.249982 -0.291658   \n",
      "7  -0.224288  1.403449 -1.264862  0.767211 -0.464338  0.710099 -0.291658   \n",
      "8  -0.224288 -0.712530  0.790600  0.047328 -0.464338 -0.024932 -0.291658   \n",
      "9  -0.224288 -0.712530  0.790600  0.719219  1.214261  0.570093 -0.291658   \n",
      "10 -0.224288  1.403449 -1.264862 -1.152477 -0.464338 -1.249982 -0.291658   \n",
      "11 -0.224288  1.403449 -1.264862 -0.677771 -0.464338 -0.765288 -0.291658   \n",
      "12 -0.224288  1.403449 -1.264862  0.682519 -0.464338  0.623625 -0.291658   \n",
      "13 -0.224288  1.403449 -1.264862 -0.677771 -0.464338 -0.765288 -0.291658   \n",
      "14 -0.224288  1.403449 -1.264862  0.511253 -0.464338  0.448755 -0.291658   \n",
      "15 -0.224288  1.403449 -1.264862  1.577080 -0.464338  1.537008 -0.291658   \n",
      "16 -0.224288  1.403449 -1.264862 -1.152477 -0.464338 -1.249982 -0.291658   \n",
      "17 -0.224288  1.403449 -1.264862  0.047328 -0.464338 -0.024932 -0.291658   \n",
      "18 -0.224288  1.403449 -1.264862  1.967016 -0.464338  1.935149 -0.291658   \n",
      "19 -0.224288  1.403449 -1.264862  0.407270  2.892861  0.661097 -0.291658   \n",
      "\n",
      "          35        36        37        38        39        40        41  \\\n",
      "0  -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -1.734136  0.323909   \n",
      "1  -0.135474 -0.294017  0.583889 -0.472497  0.316915 -0.153655 -0.558962   \n",
      "2  -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -1.734136 -0.558962   \n",
      "3  -0.135474 -0.294017  0.967650  0.684846  0.828029  0.636585 -0.367440   \n",
      "4  -0.135474 -0.294017  0.967650 -0.472497  0.611193 -1.339016 -0.397024   \n",
      "5  -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -0.153655 -0.406367   \n",
      "6  -0.135474 -0.294017  0.080202 -0.472497 -0.114693 -0.153655 -0.512249   \n",
      "7  -0.135474 -0.294017 -0.567396 -0.472497 -0.762105  1.426825 -0.379896   \n",
      "8  -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -0.153655 -0.562076   \n",
      "9  -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -0.153655  0.963874   \n",
      "10 -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -0.548775  0.518546   \n",
      "11 -0.135474 -0.294017 -0.567396  0.684846  0.077132 -2.129256 -0.563633   \n",
      "12 -0.135474 -0.294017  0.008247  0.862899  0.258589 -0.153655  0.560587   \n",
      "13 -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -0.153655 -0.580761   \n",
      "14 -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -0.943896 -0.538720   \n",
      "15 -0.135474 -0.294017 -0.567396  0.684846 -0.181094 -0.153655 -0.523149   \n",
      "16 -0.135474 -0.294017  0.583889 -0.472497  0.496751 -0.153655 -0.006194   \n",
      "17 -0.135474 -0.294017  0.623588  0.354177  0.496751 -1.339016 -0.557405   \n",
      "18 -0.135474 -0.294017  1.316525 -0.472497  1.126179 -0.153655 -0.174360   \n",
      "19 -0.135474 -0.294017 -0.567396 -0.472497 -0.762105 -2.129256 -0.527820   \n",
      "\n",
      "          42        43        44        45        46        47        48  \\\n",
      "0   0.183389  0.987617 -1.182260  3.314465 -1.080738 -0.845098  0.660362   \n",
      "1  -0.904803 -0.242359 -1.182260 -0.520814 -1.230091 -1.500203 -0.368036   \n",
      "2  -0.904803  0.987617 -1.182260  3.314465 -1.103715  0.255478  0.099418   \n",
      "3  -1.122441  0.372629 -0.675256  0.757613 -0.747564 -0.615812 -0.928981   \n",
      "4  -0.904803 -2.702310 -1.182260 -0.520814 -1.222432  1.072573 -0.368036   \n",
      "5  -0.687165 -0.242359 -1.182260 -0.520814 -1.226262 -0.058972 -0.601763   \n",
      "6   0.401028 -0.242359 -1.182260 -0.520814 -1.210943  1.775321 -0.695254   \n",
      "7  -0.251888 -0.242359 -0.675256 -0.520814 -0.728416 -1.354624 -0.835490   \n",
      "8   0.183389  0.987617 -1.182260 -0.520814 -1.218603 -1.107140 -0.368036   \n",
      "9  -1.122441  0.987617 -1.182260 -0.520814 -1.207114  1.644300 -0.648508   \n",
      "10 -1.122441 -3.317298 -1.182260 -0.520814 -1.233921 -1.480046 -0.508272   \n",
      "11 -0.904803 -0.242359  0.845756  3.314465  1.044681  0.581575  1.127816   \n",
      "12  1.271582 -0.242359  0.338752 -0.520814  0.408970  1.120217 -0.134309   \n",
      "13 -0.904803 -0.242359  0.338752 -0.520814  0.374504 -1.500203 -0.181055   \n",
      "14 -1.122441 -2.087322  0.338752 -0.520814  0.320890 -1.456530 -0.274545   \n",
      "15 -0.034249 -0.242359  0.338752 -0.520814  0.393652 -0.386525 -0.134309   \n",
      "16  1.053943 -0.242359  0.338752 -0.520814  0.405141  0.203070 -0.040818   \n",
      "17 -1.122441 -2.087322 -0.168252  0.757613 -0.242059 -0.205875 -0.368036   \n",
      "18 -0.469526 -0.242359  0.338752 -0.520814  0.385993 -0.976119 -0.181055   \n",
      "19 -0.251888 -0.242359  0.338752 -0.520814  0.259616  1.498722 -1.022471   \n",
      "\n",
      "          49        50        51        52        53        54       55  \\\n",
      "0   0.129562 -0.183373 -0.030583 -1.303789 -0.627405  4.190320 -0.02423   \n",
      "1   0.129562 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "2  -0.893405 -0.183373 -0.030583 -1.303789 -0.627405  4.190320 -0.02423   \n",
      "3   0.860253 -0.183373 -0.030583 -1.303789 -0.627405  4.190320 -0.02423   \n",
      "4  -0.528060 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "5  -1.039544 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "6  -1.331820 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "7   0.567976 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "8  -0.528060 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "9   0.641045 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "10 -0.162715 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "11 -0.820336 -0.183373 -0.030583 -1.303789 -0.627405  4.190320 -0.02423   \n",
      "12 -1.477958 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "13 -2.062511 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "14  0.348769 -0.183373 -0.030583 -1.303789  1.593867 -0.238645 -0.02423   \n",
      "15 -1.551027 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "16 -0.089645 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "17 -1.112613 -0.183373 -0.030583 -1.303789  1.593867 -0.238645 -0.02423   \n",
      "18 -2.062511 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "19 -1.697165 -0.183373 -0.030583  0.766995 -0.627405 -0.238645 -0.02423   \n",
      "\n",
      "          56        57        58        59        60        61        62  \\\n",
      "0  -0.113524 -0.250644 -0.437630  2.052381 -0.184981 -0.241153 -0.264626   \n",
      "1  -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "2  -0.113524 -0.250644 -0.437630  2.052381 -0.184981 -0.241153 -0.264626   \n",
      "3  -0.113524 -0.250644 -0.437630 -0.487239 -0.184981 -0.241153 -0.264626   \n",
      "4  -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "5  -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "6  -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "7  -0.113524 -0.250644 -0.437630  2.052381 -0.184981 -0.241153 -0.264626   \n",
      "8  -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "9  -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "10 -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "11 -0.113524 -0.250644 -0.437630  2.052381 -0.184981 -0.241153 -0.264626   \n",
      "12 -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "13 -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "14 -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "15 -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "16 -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "17 -0.113524 -0.250644  2.285036 -0.487239 -0.184981 -0.241153 -0.264626   \n",
      "18 -0.113524 -0.250644 -0.437630 -0.487239  5.405951 -0.241153 -0.264626   \n",
      "19 -0.113524 -0.250644 -0.437630  2.052381 -0.184981 -0.241153 -0.264626   \n",
      "\n",
      "          63        64        65       66        67        68        69  \\\n",
      "0  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "1  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "2  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "3  -0.184068  2.104756 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "4  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "5  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "6  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "7  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "8  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "9  -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "10 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "11 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "12 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "13 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "14 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "15 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "16 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "17 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "18 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "19 -0.184068 -0.475115 -0.206185 -0.44352 -0.371014 -0.789004  0.793616   \n",
      "\n",
      "          70        71        72        73        74        75        76  \\\n",
      "0  -0.052607 -0.413735 -0.685882 -0.138346  2.266158  0.256571 -0.843735   \n",
      "1  -0.052607  0.818296 -0.685882  0.641898 -0.056646  0.032424 -1.460288   \n",
      "2  -0.052607 -0.413735 -0.685882 -0.138346 -0.377615  0.032424 -0.227182   \n",
      "3  -0.052607 -0.413735 -0.685882  1.032020 -0.490799  0.032424  1.005924   \n",
      "4  -0.052607  2.358335  1.457977 -0.138346 -0.330314  0.480717 -0.227182   \n",
      "5  -0.052607 -0.413735  1.457977  0.641898  0.250809  1.153157 -0.843735   \n",
      "6  -0.052607  1.742319 -0.685882  0.251776 -0.281324  0.256571 -0.843735   \n",
      "7  -0.052607  0.818296 -0.685882  0.251776 -0.497556  1.825598 -0.843735   \n",
      "8  -0.052607  0.202280  1.457977 -0.918591 -0.509381 -1.088310  1.005924   \n",
      "9  -0.052607 -1.645767 -0.685882  0.641898 -0.124218  1.153157  1.005924   \n",
      "10 -0.052607 -1.029751 -0.685882  0.251776  4.431855 -1.088310 -0.227182   \n",
      "11 -0.052607 -0.105728 -0.685882 -2.869202 -0.203616 -0.640016 -0.227182   \n",
      "12 -0.052607  0.510288  1.457977 -0.528468 -0.304975  1.377304 -0.227182   \n",
      "13 -0.052607  0.202280  1.457977 -2.088957 -0.585401 -1.088310 -0.227182   \n",
      "14 -0.052607 -0.105728  1.457977 -2.479079 -0.517828 -0.415870 -2.076841   \n",
      "15 -0.052607  0.818296  1.457977 -0.528468 -0.573575 -1.088310 -0.227182   \n",
      "16 -0.052607  0.510288  1.457977 -1.308713 -0.397887  0.704864 -1.460288   \n",
      "17 -0.052607  0.202280 -0.685882  0.641898 -0.526275 -0.640016 -0.843735   \n",
      "18 -0.052607  0.202280  1.457977 -0.528468 -0.406333 -1.088310 -0.227182   \n",
      "19 -0.052607 -0.413735 -0.685882 -2.088957 -0.531343 -0.640016 -0.227182   \n",
      "\n",
      "          77        78        79        80        81        82        83  \\\n",
      "0  -1.701010  0.763486 -1.736451  0.101292 -1.086340  0.227290 -0.183151   \n",
      "1  -1.189353 -0.521153 -1.276990 -1.515574 -0.446991 -0.064402 -0.183151   \n",
      "2  -1.701010  0.763486 -1.736451  0.365270 -1.132007 -0.793633 -0.183151   \n",
      "3  -0.677696  2.048125 -0.682394 -1.529162 -0.127317  0.956521  5.459962   \n",
      "4  -0.677696 -0.521153 -0.709421 -0.030697 -0.995004 -1.085325 -0.183151   \n",
      "5  -0.677696  0.763486 -0.751892 -0.591651 -0.949336 -0.939479 -0.183151   \n",
      "6  -0.166039 -0.521153 -0.276987 -0.761037 -0.492659 -1.741633 -0.183151   \n",
      "7  -0.166039  0.763486 -0.199767  0.840431  0.192357  0.373136 -0.183151   \n",
      "8  -1.189353 -0.521153 -1.242241 -1.130606 -0.538327 -0.574864 -0.183151   \n",
      "9  -1.189353 -0.521153 -1.099384 -0.014198  0.923041  1.029444 -0.183151   \n",
      "10 -1.189353  3.332764 -1.118689  1.579570 -0.127317  0.081444 -0.183151   \n",
      "11  0.345618 -0.521153  0.271277 -0.690643 -0.995004 -0.793633 -0.183151   \n",
      "12 -1.189353 -0.521153 -1.168882  1.238429  0.055354 -1.595787 -0.183151   \n",
      "13  0.857275  3.332764  1.031897 -1.335923  1.014376 -2.106248 -0.183151   \n",
      "14  0.345618 -0.521153  0.352359 -1.284594 -0.081649  0.373136 -0.183151   \n",
      "15  0.345618 -0.521153  0.278999 -0.751561 -0.583994 -1.741633 -0.183151   \n",
      "16  0.345618  0.763486  0.475911  0.017299  1.014376 -0.210248 -0.183151   \n",
      "17 -0.166039 -0.521153 -0.234516 -0.245179  0.009686 -1.158248 -0.183151   \n",
      "18  0.345618 -0.521153  0.325331  0.243434 -0.309988 -2.106248 -0.183151   \n",
      "19  0.345618 -0.521153  0.255833 -0.309341 -0.949336 -1.595787 -0.183151   \n",
      "\n",
      "          84        85        86        87        88        89       90  \\\n",
      "0  -0.030405 -1.305431  1.594038 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "1  -0.030405 -1.305431  1.594038 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "2  -0.030405 -1.305431  1.594038 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "3  -0.030405 -1.305431 -0.627338 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "4  -0.030405  0.766031 -0.627338 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "5  -0.030405 -1.305431 -0.627338  4.208741 -0.024004 -0.122104 -0.25466   \n",
      "6  -0.030405 -1.305431  1.594038 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "7  -0.030405 -1.305431  1.594038 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "8  -0.030405  0.766031 -0.627338 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "9  -0.030405  0.766031 -0.627338 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "10 -0.030405 -1.305431 -0.627338  4.208741 -0.024004 -0.122104 -0.25466   \n",
      "11 -0.030405  0.766031 -0.627338 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "12 -0.030405  0.766031 -0.627338 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "13 -0.030405 -1.305431 -0.627338  4.208741 -0.024004 -0.122104 -0.25466   \n",
      "14 -0.030405 -1.305431  1.594038 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "15 -0.030405 -1.305431  1.594038 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "16 -0.030405 -1.305431 -0.627338  4.208741 -0.024004 -0.122104 -0.25466   \n",
      "17 -0.030405  0.766031 -0.627338 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "18 -0.030405  0.766031 -0.627338 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "19 -0.030405 -1.305431  1.594038 -0.237601 -0.024004 -0.122104 -0.25466   \n",
      "\n",
      "          91        92        93        94        95        96        97  \\\n",
      "0  -0.424337 -0.485053 -0.192544 -0.250032  3.699370 -0.188904 -0.469419   \n",
      "1   2.356620 -0.485053 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "2  -0.424337 -0.485053 -0.192544 -0.250032  3.699370 -0.188904 -0.469419   \n",
      "3  -0.424337 -0.485053 -0.192544 -0.250032 -0.270316 -0.188904  2.130294   \n",
      "4  -0.424337  2.061632 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "5  -0.424337 -0.485053 -0.192544 -0.250032 -0.270316 -0.188904  2.130294   \n",
      "6  -0.424337 -0.485053 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "7  -0.424337  2.061632 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "8  -0.424337 -0.485053  5.193607 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "9   2.356620 -0.485053 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "10 -0.424337  2.061632 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "11 -0.424337  2.061632 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "12 -0.424337 -0.485053  5.193607 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "13 -0.424337  2.061632 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "14 -0.424337 -0.485053  5.193607 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "15  2.356620 -0.485053 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "16 -0.424337 -0.485053  5.193607 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "17 -0.424337 -0.485053 -0.192544 -0.250032 -0.270316 -0.188904  2.130294   \n",
      "18 -0.424337 -0.485053  5.193607 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "19 -0.424337 -0.485053 -0.192544 -0.250032 -0.270316 -0.188904 -0.469419   \n",
      "\n",
      "          98        99       100       101       102       103       104  \\\n",
      "0  -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  1.124895   \n",
      "1  -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  0.198312   \n",
      "2  -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803 -1.345994   \n",
      "3  -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803 -0.419411   \n",
      "4  -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803 -0.110549   \n",
      "5  -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  0.816034   \n",
      "6  -0.210135  2.279713 -0.362571 -0.792162  0.797549 -0.056803 -1.037133   \n",
      "7  -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803 -1.037133   \n",
      "8  -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803 -0.419411   \n",
      "9  -0.210135 -0.438652 -0.362571  1.262369 -1.253842 -0.056803  1.124895   \n",
      "10 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803 -0.419411   \n",
      "11 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  1.742617   \n",
      "12 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803 -1.654855   \n",
      "13 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  0.198312   \n",
      "14 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  2.360340   \n",
      "15 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  1.742617   \n",
      "16 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  0.507173   \n",
      "17 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803  0.198312   \n",
      "18 -0.210135 -0.438652 -0.362571 -0.792162  0.797549 -0.056803 -0.728272   \n",
      "19 -0.210135  2.279713 -0.362571 -0.792162  0.797549 -0.056803  0.507173   \n",
      "\n",
      "         105       106       107       108       109       110       111  \\\n",
      "0  -0.566421 -0.130578 -0.258403  0.776344  2.893107  3.394031  0.765135   \n",
      "1  -0.566421  1.420362 -0.107048  2.614119 -0.835530 -1.197872 -0.520352   \n",
      "2  -0.566421  0.644892 -0.206150  1.235787 -0.214090 -1.197872 -0.520352   \n",
      "3  -0.566421  0.644892 -0.388135 -0.831709  1.028789 -1.197872 -0.520352   \n",
      "4  -0.566421  1.032627 -0.143085  0.316900 -0.214090 -0.679013  2.050622   \n",
      "5  -0.566421  0.644892 -0.357504 -0.142544 -0.835530 -1.197872 -0.520352   \n",
      "6  -0.566421  1.032627 -0.348495  2.154675 -0.214090 -0.679013  2.050622   \n",
      "7  -0.566421  0.644892 -0.534085 -1.061431 -0.214090 -0.679013  0.765135   \n",
      "8   1.765471 -0.518313  0.105569 -0.831709  1.028789 -1.197872 -0.520352   \n",
      "9  -0.566421 -0.906048 -0.292638  0.316900  1.028789 -1.197872 -0.520352   \n",
      "10 -0.566421  0.644892  5.100272  1.695231 -0.214090 -0.679013 -0.520352   \n",
      "11 -0.566421  1.032627 -0.357504  0.776344 -0.214090  0.358705 -0.520352   \n",
      "12 -0.566421  0.644892  2.320031  1.924953 -2.699848 -0.160154  0.765135   \n",
      "13  1.765471  0.257157 -0.429578 -0.831709 -0.214090  0.877565  2.050622   \n",
      "14  1.765471 -1.293783 -0.415163  0.087178 -0.835530  0.358705 -0.520352   \n",
      "15  1.765471  0.644892 -0.526877  2.384397 -0.835530  0.877565 -0.520352   \n",
      "16  1.765471 -0.518313 -0.532283 -1.061431 -0.214090  0.358705 -0.520352   \n",
      "17 -0.566421  1.420362 -0.247592 -0.831709 -0.835530 -0.679013  0.765135   \n",
      "18  1.765471 -0.130578 -0.508859  0.546622 -0.214090  0.877565 -0.520352   \n",
      "19 -0.566421 -1.293783 -0.391739 -0.831709 -0.214090  0.358705 -0.520352   \n",
      "\n",
      "         112       113        114       115       116       117       118  \\\n",
      "0   4.122613 -0.802297 -10.160172 -0.038195 -0.182993 -0.029125 -1.306527   \n",
      "1  -1.223508 -1.422108  -0.226458 -0.110973 -0.182993 -0.029125  0.765388   \n",
      "2  -1.250924  0.359849  -0.688491 -0.474865 -0.182993 -0.029125  0.765388   \n",
      "3  -1.297923 -1.499584  -0.596085  1.053481 -0.182993 -0.029125  0.765388   \n",
      "4  -0.686937  0.127420  -0.503678 -1.057091  5.464691 -0.029125 -1.306527   \n",
      "5  -1.266590 -0.221224  -0.780898 -1.129870 -0.182993 -0.029125  0.765388   \n",
      "6  -0.655605  1.439961  -0.318865 -1.493761  5.464691 -0.029125 -1.306527   \n",
      "7  -0.698687 -0.918511   0.281778  0.616811 -0.182993 -0.029125 -1.306527   \n",
      "8  -1.262673 -0.873813  -0.642288 -0.474865 -0.182993 -0.029125  0.765388   \n",
      "9  -1.278340  0.437325  -0.596085  0.762367 -0.182993 -0.029125  0.765388   \n",
      "10 -0.522441  0.476064   0.974828  0.180140 -0.182993 -0.029125 -1.306527   \n",
      "11  0.253040  1.002447  -0.411271 -1.202648 -0.182993 -0.029125  0.765388   \n",
      "12 -0.166033 -1.499584   1.113438 -1.202648 -0.182993 -0.029125 -1.306527   \n",
      "13  0.832692 -1.298145  -0.134052 -2.148766 -0.182993 -0.029125 -1.306527   \n",
      "14  0.323538 -1.393934  -0.688491 -0.183751 -0.182993 -0.029125  0.765388   \n",
      "15  0.836609  0.650385   1.159641 -2.148766 -0.182993 -0.029125  0.765388   \n",
      "16  0.311788 -0.730780  -0.134052 -0.329308 -0.182993 -0.029125  0.765388   \n",
      "17 -0.702604 -1.112202  -0.411271 -1.202648 -0.182993 -0.029125  0.765388   \n",
      "18  0.820943 -0.536663  -0.134052 -1.930431 -0.182993 -0.029125  0.765388   \n",
      "19  0.162959  1.310695  -1.011915 -1.712096 -0.182993 -0.029125 -1.306527   \n",
      "\n",
      "         119       120      121       122      123       124       125  \\\n",
      "0   1.595663 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "1  -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864  2.064688   \n",
      "2  -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "3  -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "4  -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "5  -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "6  -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "7   1.595663 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "8  -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "9  -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "10  1.595663 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "11 -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "12  1.595663 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864  2.064688   \n",
      "13 -0.626699  4.202876 -0.02423 -0.126786 -0.25707 -0.412864  2.064688   \n",
      "14 -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864  2.064688   \n",
      "15 -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "16 -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "17 -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "18 -0.626699 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864 -0.484335   \n",
      "19  1.595663 -0.237932 -0.02423 -0.126786 -0.25707 -0.412864  2.064688   \n",
      "\n",
      "         126       127       128       129       130       131       132  \\\n",
      "0  -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518  2.281729   \n",
      "1  -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "2  -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518  2.281729   \n",
      "3  -0.196634  3.960014 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "4  -0.196634 -0.252524 -0.274734 -0.191876  2.147662 -0.215518 -0.438264   \n",
      "5  -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518  2.281729   \n",
      "6  -0.196634 -0.252524 -0.274734 -0.191876  2.147662 -0.215518 -0.438264   \n",
      "7  -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518  2.281729   \n",
      "8   5.085578 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "9  -0.196634  3.960014 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "10 -0.196634 -0.252524 -0.274734 -0.191876  2.147662 -0.215518 -0.438264   \n",
      "11 -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518  2.281729   \n",
      "12 -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "13 -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "14 -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "15 -0.196634  3.960014 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "16  5.085578 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "17 -0.196634 -0.252524 -0.274734 -0.191876  2.147662 -0.215518 -0.438264   \n",
      "18 -0.196634 -0.252524  3.639890 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "19 -0.196634 -0.252524 -0.274734 -0.191876 -0.465623 -0.215518 -0.438264   \n",
      "\n",
      "         133       134       135        136       137       138       139  \\\n",
      "0  -0.350214 -0.793361 -1.252524  18.233291  1.449144 -0.513521 -0.685882   \n",
      "1  -0.350214 -0.793361  0.798388  -0.054845 -0.111482 -0.513521 -0.685882   \n",
      "2  -0.350214 -0.793361  0.798388  -0.054845 -0.111482 -0.513521 -0.685882   \n",
      "3  -0.350214 -0.793361  0.798388  -0.054845 -0.423607 -0.513521 -0.685882   \n",
      "4  -0.350214 -0.793361  0.798388  -0.054845 -1.672108 -0.513521  1.457977   \n",
      "5  -0.350214 -0.793361  0.798388  -0.054845 -1.047857 -0.513521  1.457977   \n",
      "6  -0.350214 -0.793361  0.798388  -0.054845 -0.423607 -0.513521 -0.685882   \n",
      "7  -0.350214 -0.793361  0.798388  -0.054845  0.512768 -0.513521 -0.685882   \n",
      "8  -0.350214 -0.793361  0.798388  -0.054845  2.073394  1.947341  1.457977   \n",
      "9  -0.350214 -0.793361  0.798388  -0.054845  0.512768 -0.513521 -0.685882   \n",
      "10 -0.350214  1.260460 -1.252524  -0.054845  3.009769 -0.513521 -0.685882   \n",
      "11 -0.350214 -0.793361  0.798388  -0.054845 -0.735732 -0.513521 -0.685882   \n",
      "12 -0.350214 -0.793361  0.798388  -0.054845  0.824893 -0.513521  1.457977   \n",
      "13 -0.350214 -0.793361  0.798388  -0.054845 -0.111482  1.947341  1.457977   \n",
      "14 -0.350214 -0.793361  0.798388  -0.054845 -0.423607 -0.513521  1.457977   \n",
      "15 -0.350214 -0.793361  0.798388  -0.054845 -0.735732 -0.513521  1.457977   \n",
      "16 -0.350214 -0.793361  0.798388  -0.054845 -0.111482  1.947341  1.457977   \n",
      "17 -0.350214 -0.793361  0.798388  -0.054845  1.449144 -0.513521 -0.685882   \n",
      "18 -0.350214 -0.793361  0.798388  -0.054845  0.824893 -0.513521  1.457977   \n",
      "19 -0.350214 -0.793361  0.798388  -0.054845 -0.111482 -0.513521 -0.685882   \n",
      "\n",
      "         race_id order  \n",
      "0   201101010111     5  \n",
      "1   201101010111     2  \n",
      "2   201101010111    11  \n",
      "3   201101010111     7  \n",
      "4   201101010111     1  \n",
      "5   201101010111     6  \n",
      "6   201101010111     3  \n",
      "7   201101010111     8  \n",
      "8   201101010111     4  \n",
      "9   201101010111    12  \n",
      "10  201101010111    14  \n",
      "11  201101010112     8  \n",
      "12  201101010112     4  \n",
      "13  201101010112     6  \n",
      "14  201101010112     9  \n",
      "15  201101010112     3  \n",
      "16  201101010112     5  \n",
      "17  201101010112     2  \n",
      "18  201101010112    10  \n",
      "19  201101010112     7  \n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data.dtypes)\n",
    "print(data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_series_data(raw_data):\n",
    "    number_of_race = raw_data.race_id.nunique()\n",
    "    time_series_data = np.full((number_of_race, 18, 140), 0.0)#-float('inf')\n",
    "    label = np.full((number_of_race, 18), 19)\n",
    "    race_number = 0\n",
    "    horse_number = 0\n",
    "    for i in range(len(raw_data)):\n",
    "        if i == 0:\n",
    "#             print(race_number)\n",
    "#             print(horse_number)\n",
    "#             print(raw_data.iloc[i].order)\n",
    "            label[race_number][horse_number] = float(raw_data.iloc[i].order)\n",
    "            time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "            horse_number += 1\n",
    "            continue\n",
    "        if data.iloc[i].race_id != data.iloc[i-1].race_id:\n",
    "            race_number += 1\n",
    "            horse_number = 0\n",
    "            label[race_number][horse_number] = float(raw_data.iloc[i].order)\n",
    "            time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "            horse_number += 1\n",
    "        else:\n",
    "            label[race_number][horse_number] = float(raw_data.iloc[i].order)\n",
    "            time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "            horse_number += 1\n",
    "    return time_series_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8489, 18, 140)\n",
      "(8489, 18)\n"
     ]
    }
   ],
   "source": [
    "X, y_order = create_time_series_data(data)\n",
    "print(X.shape)\n",
    "print(y_order.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "X = X.astype('float32')\n",
    "print(X.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[ 5  2 11  7  1  6  3  8  4 12 14 19 19 19 19 19 19 19]\n"
     ]
    }
   ],
   "source": [
    "print(X[0][11])\n",
    "print(y_order[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,)\n",
      "[ 0.          1.22620251  1.21236789  1.23692263  1.24838235  1.27347735\n",
      "  1.27233213  1.2771175   1.33202573  1.35824     1.42768248  1.52624955\n",
      "  1.69170984  1.90080609  2.16445691  2.54619076  3.30697312 14.51111111\n",
      " 19.42562929  0.13969294]\n"
     ]
    }
   ],
   "source": [
    "alpha = len(y_order) / pd.DataFrame(y_order.flatten()).value_counts()\n",
    "alpha = alpha.sort_index()\n",
    "alpha = np.array(alpha)\n",
    "alpha = np.append(0,alpha)\n",
    "print(alpha.shape)\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "(8489, 18, 20)\n",
      "(8489, 18, 140)\n"
     ]
    }
   ],
   "source": [
    "# creating X,y (parameters and target)\n",
    "y = np_utils.to_categorical(y_order, dtype='float32')\n",
    "print(y[0])\n",
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_label(label, factor=0.03):\n",
    "    # smooth label\n",
    "    label *= (1 - factor)\n",
    "    label[:,:,1:4] += (factor / 3)\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.01 0.01 0.01 0.   0.97 0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.01 0.98 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.01 0.01 0.01 0.   0.   0.   0.   0.   0.   0.   0.97 0.   0.\n",
      "  0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.01 0.01 0.01 0.   0.   0.   0.97 0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.98 0.01 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.01 0.01 0.01 0.   0.   0.97 0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.01 0.01 0.98 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.01 0.01 0.01 0.   0.   0.   0.   0.97 0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.01 0.01 0.01 0.97 0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.01 0.01 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.97 0.\n",
      "  0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.01 0.01 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.97 0.   0.   0.   0.   0.  ]\n",
      " [0.   0.01 0.01 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.01 0.01 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.01 0.01 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.01 0.01 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.01 0.01 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.01 0.01 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.01 0.01 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.97]]\n",
      "(8489, 18, 20)\n",
      "(8489, 18, 140)\n"
     ]
    }
   ],
   "source": [
    "y = smooth_label(y) \n",
    "print(y[0])\n",
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(data.race_id.value_counts().plot.hist(bins=19,range=(1,19)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(pd.DataFrame(y_order.flatten()).plot.hist(bins=19))## ,ylim=(0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUFFER_SIZE = 1000\n",
    "# BATCH_SIZE = 128\n",
    "# def make_batches(ds):\n",
    "#     return (\n",
    "#       ds\n",
    "# #       .cache()\n",
    "# #       .shuffle(BUFFER_SIZE)\n",
    "#       .batch(BATCH_SIZE)\n",
    "# #       .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#       .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "# X = tf.convert_to_tensor(X)\n",
    "# y = tf.convert_to_tensor(y)\n",
    "# X = make_batches(X)\n",
    "# y = make_batches(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.3145431  -0.50991476  0.5901306  -0.63937795 -0.25580055 -0.46515507\n",
      " -0.38537532  0.8550409  -0.7798238   0.7798238   0.6798608  -0.66338956\n",
      " -0.18337315 -0.02931084 -1.3091208   1.597206   -0.23655261 -0.02236226\n",
      " -0.24513794  2.2255282  -0.4990219  -0.17550196 -0.2203592  -0.25591546\n",
      " -0.17979759 -0.49679595 -0.19606745 -0.4557965  -0.22428839 -0.71253043\n",
      "  0.7906001   0.4072698  -0.4643385   0.34258357  2.5039315  -0.1354737\n",
      "  2.453141   -0.56739587 -0.4724972  -0.76210487 -1.3390157  -0.6119031\n",
      " -1.1224415   0.37262917  1.0992585  -0.5208136   1.2936039  -1.4287373\n",
      "  0.6136168   2.5408416  -0.18337315 -0.03058303  0.76699543 -0.627405\n",
      " -0.23864526 -0.02422995 -0.1135239  -0.25064442 -0.4376299  -0.487239\n",
      " -0.18498135 -0.24115273 -0.26462567 -0.18406838  2.1047556  -0.2061855\n",
      " -0.44352028 -0.37101352  1.2674206  -1.2600559  -0.0526072  -0.10572761\n",
      " -0.685882    0.64189816 -0.5685074  -0.864163    0.38937113  0.3456182\n",
      " -0.52115315  0.51452124 -1.5155745   1.2427148   2.560829   -0.18315147\n",
      " -0.03040453  0.76603085 -0.6273377  -0.23760074 -0.02400442 -0.12210409\n",
      " -0.25465992  2.3566198  -0.48505253 -0.1925444  -0.25003174 -0.27031633\n",
      " -0.18890353 -0.46941882 -0.21013454 -0.4386517  -0.36257088  1.2623687\n",
      " -1.2538418  -0.0568033   2.0514784  -0.5664212  -2.4569879  -0.5647159\n",
      " -0.83170927 -0.21409014  1.136994   -0.52035177  1.2791817  -0.8022966\n",
      "  0.79001486  2.0723774  -0.18299298 -0.02912458  0.7653881  -0.6266987\n",
      " -0.23793232 -0.02422995 -0.12678613 -0.25707033 -0.41286385 -0.4843346\n",
      " -0.19663447 -0.25252435 -0.27473357 -0.19187577  2.1476622  -0.21551774\n",
      " -0.43826422 -0.35021392  1.2604601  -1.2525241  -0.05484473 -0.11148198\n",
      " -0.5135207  -0.685882  ]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6791, 18, 140)\n",
      "(1698, 18, 140)\n",
      "(6791, 18, 20)\n",
      "(1698, 18, 20)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_focal_loss(alpha, gamma):\n",
    "    \"\"\"\n",
    "    Softmax version of focal loss.\n",
    "    When there is a skew between different categories/labels in your data set, you can try to apply this function as a\n",
    "    loss.\n",
    "           m\n",
    "      FL = ∑  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n",
    "          c=1\n",
    "      where m = number of classes, c = class and o = observation\n",
    "    Parameters:\n",
    "      alpha -- the same as weighing factor in balanced cross entropy. Alpha is used to specify the weight of different\n",
    "      categories/labels, the size of the array needs to be consistent with the number of classes.\n",
    "      gamma -- focusing parameter for modulating factor (1-p)\n",
    "    Default value:\n",
    "      gamma -- 2.0 as mentioned in the paper\n",
    "      alpha -- 0.25 as mentioned in the paper\n",
    "    References:\n",
    "        Official paper: https://arxiv.org/pdf/1708.02002.pdf\n",
    "        https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy\n",
    "    Usage:\n",
    "     model.compile(loss=[categorical_focal_loss(alpha=[[.25, .25, .25]], gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "\n",
    "    alpha = np.array(alpha, dtype=np.float32)\n",
    "\n",
    "    def categorical_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred: A tensor resulting from a softmax\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        # Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "        # Calculate Cross Entropy\n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "\n",
    "        # Calculate Focal Loss\n",
    "        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
    "\n",
    "        # Compute mean loss in mini_batch\n",
    "        return K.mean(K.sum(loss, axis=-1))\n",
    "\n",
    "    return categorical_focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataset with batch size\n",
    "batch_size = 1024 # hyperparameter\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=6791).batch(batch_size)\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "valid_dataset = valid_dataset.shuffle(buffer_size=1698).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters\n",
    "num_layers = 5 # hyperparameter\n",
    "d_model = 140 # 4*35\n",
    "num_heads = 20 # hyperparameter *must be a factor of d_model*\n",
    "d_ffn = 8 # hyperparameter\n",
    "pe_input = 18\n",
    "target_size = 20\n",
    "dropout_rate = 0.1 # hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model\n",
    "trans_race = transformer.TransRace(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ffn=d_ffn,\n",
    "    pe_input=pe_input,\n",
    "    target_size=target_size,\n",
    "    rate=dropout_rate,\n",
    ")\n",
    "\n",
    "# trans_race.build(input_shape=(None,18,139))\n",
    "# opt = optimizers.Adam(lr = 0.0001, clipvalue = 2.)\n",
    "# loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "#     label_smoothing=0.1,\n",
    "# )\n",
    "loss = categorical_focal_loss(alpha=[alpha], gamma=0.5)\n",
    "trans_race.compile(\n",
    "    optimizer='Adam',\n",
    "    loss=loss,#'categorical_crossentropy', # 'sigmoid_focal_crossentropy'\n",
    "    metrics=['accuracy'], #['categorical_accuracy']\n",
    ")\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "#     patience=10,\n",
    "#     min_delta=0.001,\n",
    "#     restore_best_weights=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7 steps, validate on 2 steps\n",
      "Epoch 1/2\n",
      "7/7 [==============================] - 4s 508ms/step - loss: 2.4874 - acc: 0.4482 - val_loss: 2.4796 - val_acc: 0.4480\n",
      "Epoch 2/2\n",
      "7/7 [==============================] - 2s 258ms/step - loss: 2.4818 - acc: 0.4485 - val_loss: 2.4708 - val_acc: 0.4518\n",
      "Model: \"trans_race\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Encoder)            multiple                  409540    \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             multiple                  2820      \n",
      "=================================================================\n",
      "Total params: 412,360\n",
      "Trainable params: 412,360\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-07b401021ef8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                         )\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans_race\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrans_race\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../models/results/transformer5.h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# the problem is gradient exploding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures)\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \"\"\"\n\u001b[1;32m   1170\u001b[0m     saving.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1171\u001b[0;31m                       signatures)\n\u001b[0m\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures)\u001b[0m\n\u001b[1;32m    100\u001b[0m         not isinstance(model, sequential.Sequential)):\n\u001b[1;32m    101\u001b[0m       raise NotImplementedError(\n\u001b[0;32m--> 102\u001b[0;31m           \u001b[0;34m'Saving the model to HDF5 format requires the model to be a '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m           \u001b[0;34m'Functional model or a Sequential model. It does not work for '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m           \u001b[0;34m'subclassed models, because such models are defined via the body of '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`."
     ]
    }
   ],
   "source": [
    "history = trans_race.fit(\n",
    "                        train_dataset,\n",
    "                        validation_data=valid_dataset,\n",
    "                        epochs=20,\n",
    "#                         callbacks=[early_stopping],\n",
    "                        verbose=True, # hide the output because we have so many epochs\n",
    "                        )\n",
    "print(trans_race.summary())\n",
    "trans_race.save_weights(\"../models/results/transformer5.h5\")\n",
    "\n",
    "# the problem is gradient exploding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \n",
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "# for loss\n",
    "axL.plot(history.history['loss'],label=\"loss for training\")\n",
    "axL.plot(history.history['val_loss'],label=\"loss for validation\")\n",
    "axL.set_title('model loss')\n",
    "axL.set_xlabel('epoch')\n",
    "axL.set_ylabel('loss')\n",
    "axL.legend(loc='upper right')\n",
    "axR.plot(history.history['acc'],label=\"acc for training\")\n",
    "axR.plot(history.history['val_acc'],label=\"acc for validation\")\n",
    "axR.set_title('model accuracy')\n",
    "axR.set_xlabel('epoch')\n",
    "axR.set_ylabel('accuracy')\n",
    "axR.legend(loc='lower right')\n",
    "# figureの保存\n",
    "# plt.savefig(\"../models/results/trans_race_result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1 = trans_race.layers[0]\n",
    "# print(l1.get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "pred = trans_race.predict(X_valid)\n",
    "y_pred = np.argmax(pred, axis = 2)\n",
    "y_ans = np.argmax(y_valid, axis = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1698, 18, 20)\n",
      "(1698, 18)\n",
      "(1698, 18)\n"
     ]
    }
   ],
   "source": [
    "print(pred.shape)\n",
    "print(y_pred.shape)\n",
    "print(y_ans.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8 10  9  3  4  7  5  6  1  2 19 19 19 19 19 19 19 19]\n",
      "[ 2 16  2  1 14  1  1 15 14  1  2  2 16  4 19 19 19 19]\n"
     ]
    }
   ],
   "source": [
    "print(y_ans[0])\n",
    "print(y_pred[1])\n",
    "# print(np.mean(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  7.953474676089517\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(y_pred)):\n",
    "    for j in range(len(y_pred[0])):\n",
    "        if (y_pred[i][j] == y_ans[i][j]):\n",
    "            correct += 1\n",
    "accuracy = correct / len(y_pred)\n",
    "print(\"accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 何位に予想した？　何位が含まれていた？\n",
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "label = ['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19']\n",
    "\n",
    "axL.hist(y_pred.flatten(), bins = 19, label = \"pred_order\")##, range = (1,21)\n",
    "axL.set_xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19])\n",
    "axL.set_xticklabels(label)\n",
    "axL.set_title('pred order distribution')\n",
    "axL.legend()\n",
    "axR.hist(y_ans.flatten(), bins = 19, label = \"ans_order\")##, range = (1,21)\n",
    "axR.set_xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19])\n",
    "axR.set_xticklabels(label)\n",
    "axR.set_title('ans order distribution')\n",
    "axR.legend()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.unique(pred_order))\n",
    "# print(np.unique(Y_ans))\n",
    "# u, c = np.unique(pred_order, return_counts = True)\n",
    "# print(u)\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-d3b8ff27fe96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mincrease\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX_test_inv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandard_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mX_test_inv_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_inv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0modds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_inv_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "# accuracy of the first horse\n",
    "i = 0\n",
    "correct_first = 0\n",
    "all_f = 0\n",
    "increase = 0\n",
    "X_test_inv = standard_scale.inverse_transform(X_test)\n",
    "X_test_inv_df = pd.DataFrame(X_test_inv)\n",
    "odds = X_test_inv_df[4].values\n",
    "hit_odds = []\n",
    "select = []\n",
    "\n",
    "for i in range(len(Y_ans)):\n",
    "    if (pred_order[i] == 1):  # いちい予想した総数  & (pred[i][1]*odds[i] > 1.0)) | (pred[i][1]> 0.25)\n",
    "        all_f = all_f + 1\n",
    "        if (Y_ans[i] == 1):\n",
    "            correct_first = correct_first + 1   #　一致した総数\n",
    "            increase += odds[i]\n",
    "            hit_odds.append(odds[i])\n",
    "            \n",
    "            \n",
    "            \n",
    "print(\"all_f: \", all_f)\n",
    "print(\"correct_first: \", correct_first)\n",
    "print(\"hit odds average: \", np.array(hit_odds).mean())\n",
    "print(\"spent money:\", all_f * 100)\n",
    "revenue = (increase - all_f) * 100\n",
    "retrive = increase / all_f\n",
    " \n",
    "print(\"retrive rate: \", retrive) \n",
    "print(\"revenue: \", revenue)\n",
    "accurate_rate = correct_first / all_f\n",
    "print(\"accuracy: \",accurate_rate)\n",
    "#print(\"\\n\".join(map(str,hit_odds)))\n",
    "print(\"min: \", min(hit_odds))\n",
    "print(\"mid: \", np.median(np.array(hit_odds)))\n",
    "print(\"max: \", max(hit_odds))\n",
    "\n",
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "axL.hist(hit_odds, bins = 50, label = \"hit_odds\")\n",
    "axL.set_title('hit odds distribution')\n",
    "axL.legend()\n",
    "axR.hist(odds, bins = 50, label = \"odds\", range = (0,40))\n",
    "axR.set_title('all odds distribution')\n",
    "axR.legend()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_ans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-698be0e6cf82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mall_f_odds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_ans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mY_ans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 一位の総数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mall_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_f\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y_ans' is not defined"
     ]
    }
   ],
   "source": [
    "# 一位だった時一位予想していた確率\n",
    "i = 0\n",
    "correct_first = 0\n",
    "all_f = 0\n",
    "odds_f = []\n",
    "p_rate_f = []\n",
    "\n",
    "all_f_odds = []\n",
    "\n",
    "for i in range(len(Y_ans)):\n",
    "    if (Y_ans[i] == 1):  # 一位の総数\n",
    "        all_f = all_f + 1\n",
    "        all_f_odds.append(odds[i])\n",
    "        if (pred_order[i] == 1):\n",
    "            correct_first = correct_first + 1   #　一致した総数\n",
    "            odds_f.append(odds[i])\n",
    "            p_rate_f.append(pred[i][1])\n",
    "\n",
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "axL.scatter(p_rate_f, odds_f)  \n",
    "axL.set_title('correlation odss and prediction')\n",
    "#axL.xlabel('prediction rate first')\n",
    "#axL.ylabel('odds')\n",
    "axR.hist(odds_f, bins = 50, label = \"odds\")\n",
    "axR.set_title('all first odds distribution')\n",
    "axR.legend()\n",
    "\n",
    "fig.show()\n",
    "accurate_rate = correct_first / all_f\n",
    "print(\"accuracy: \",accurate_rate)\n",
    "print(\"all_f_odds average: \", np.array(all_f_odds).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ensamble log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = resnet.resrace(X_test.shape[1], 19)\n",
    "model1.load_weights(\"model/win5_resrace_model_best1.h5\")\n",
    "pred1 = model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = resnet.resrace(X_test.shape[1], 19)\n",
    "model2.load_weights(\"model/win5_resrace_model_best2.h5\")\n",
    "pred2 = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = resnet.resrace(X_test.shape[1], 19)\n",
    "model3.load_weights(\"model/win5_resrace_model_best3.h5\")\n",
    "pred3 = model3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = resnet.resrace(X_test.shape[1], 19)\n",
    "model4.load_weights(\"model/win5_resrace_model_best4.h5\")\n",
    "pred4 = model4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = resnet.resrace(X_test.shape[1], 19)\n",
    "model5.load_weights(\"model/win5_resrace_model_best5.h5\")\n",
    "pred5 = model5.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = resnet.resrace(X_test.shape[1], 19)\n",
    "model6.load_weights(\"model/win5_resrace_model_best6.h5\")\n",
    "pred6 = model6.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = resnet.resrace(X_test.shape[1], 19)\n",
    "model7.load_weights(\"model/win5_resrace_model_best7.h5\")\n",
    "pred7 = model7.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = resnet.resrace(X_test.shape[1], 19)\n",
    "model8.load_weights(\"model/win5_resrace_model_best8.h5\")\n",
    "pred8 = model8.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model9 = resnet.resrace(X_test.shape[1], 19)\n",
    "model9.load_weights(\"model/win5_resrace_model_best9.h5\")\n",
    "pred9 = model9.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10 = resnet.resrace(X_test.shape[1], 19)\n",
    "model10.load_weights(\"model/win5_resrace_model_best10.h5\")\n",
    "pred10 = model10.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pred1 = np.log(pred1)\n",
    "log_pred2 = np.log(pred2)\n",
    "log_pred3 = np.log(pred3)\n",
    "log_pred4 = np.log(pred4)\n",
    "log_pred5 = np.log(pred5)\n",
    "# log_pred6 = np.log(pred6)\n",
    "# log_pred7 = np.log(pred7)\n",
    "# log_pred8 = np.log(pred8)\n",
    "# log_pred9 = np.log(pred9)\n",
    "# log_pred10 = np.log(pred10)\n",
    "\n",
    "sum_pred = log_pred1 + log_pred2 + log_pred3 + log_pred4 + log_pred5 #+ log_pred6 + log_pred7 + log_pred8 + log_pred9 + log_pred10\n",
    "pred_order = np.argmax(sum_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sum_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pred_order[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
