{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pprint\n",
    "# import sys\n",
    "\n",
    "# pprint.pprint(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fundamental libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/mnt/c/Users/44yos/RacePrediction/resnet_win5')\n",
    "from os import path\n",
    "import time\n",
    "\n",
    "# preporcessing libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt \n",
    "from pickle import dump\n",
    "\n",
    "# tesndorflow libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from tensorflow.python.keras import optimizers, callbacks\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "# from models import resnet\n",
    "from models import transformer\n",
    "\n",
    "# from utils import functions\n",
    "from utils import create_time_series_data, smooth_label, categorical_focal_loss, order_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race_id                        int64\n",
      "race_round                     int64\n",
      "ground_condition               int64\n",
      "total_horse_number             int64\n",
      "order                          int64\n",
      "frame_number                   int64\n",
      "horse_number                   int64\n",
      "age                            int64\n",
      "burden_weight                float64\n",
      "goal_time                    float64\n",
      "half_order                   float64\n",
      "last_time                    float64\n",
      "odds                         float64\n",
      "horse_weight                 float64\n",
      "pop                          float64\n",
      "race_rank                      int64\n",
      "distance                       int64\n",
      "ground_type_ダ                  int64\n",
      "ground_type_芝                  int64\n",
      "circle_右                       int64\n",
      "circle_左                       int64\n",
      "weather_circumstance_小雨        int64\n",
      "weather_circumstance_小雪        int64\n",
      "weather_circumstance_晴         int64\n",
      "weather_circumstance_曇         int64\n",
      "weather_circumstance_雨         int64\n",
      "weather_circumstance_雪         int64\n",
      "place_中京                       int64\n",
      "place_中山                       int64\n",
      "place_京都                       int64\n",
      "place_函館                       int64\n",
      "place_小倉                       int64\n",
      "place_新潟                       int64\n",
      "place_札幌                       int64\n",
      "place_東京                       int64\n",
      "place_福島                       int64\n",
      "place_阪神                       int64\n",
      "sex_セ                          int64\n",
      "sex_牝                          int64\n",
      "sex_牡                          int64\n",
      "horse_weight_dif             float64\n",
      "f_grass_win_rate             float64\n",
      "f_dart_win_rate              float64\n",
      "f_win_rate                   float64\n",
      "g_f_grass_win_rate           float64\n",
      "g_f_dart_win_rate            float64\n",
      "g_f_win_rate                 float64\n",
      "m_grass_win_rate             float64\n",
      "m_dart_win_rate              float64\n",
      "m_win_rate                   float64\n",
      "whole_horse_number_1         float64\n",
      "odds_1                       float64\n",
      "order_1                        int64\n",
      "burden_weight_1              float64\n",
      "race_distance_1              float64\n",
      "ground_condition_1           float64\n",
      "goal_time_1                  float64\n",
      "half_order_1                 float64\n",
      "last_time_1                  float64\n",
      "horse_weight_1               float64\n",
      "weather_circumstance_小雨_1    float64\n",
      "weather_circumstance_小雪_1    float64\n",
      "weather_circumstance_晴_1     float64\n",
      "weather_circumstance_曇_1     float64\n",
      "weather_circumstance_雨_1     float64\n",
      "weather_circumstance_雪_1     float64\n",
      "main_place_その他_1             float64\n",
      "main_place_中京_1              float64\n",
      "main_place_中山_1              float64\n",
      "main_place_京都_1              float64\n",
      "main_place_函館_1              float64\n",
      "main_place_小倉_1              float64\n",
      "main_place_新潟_1              float64\n",
      "main_place_札幌_1              float64\n",
      "main_place_東京_1              float64\n",
      "main_place_福島_1              float64\n",
      "main_place_阪神_1              float64\n",
      "race_rank_1                  float64\n",
      "ground_type_ダ_1              float64\n",
      "ground_type_芝_1              float64\n",
      "ground_type_障_1              float64\n",
      "horse_weight_dif_1           float64\n",
      "same_jockey_1                float64\n",
      "whole_horse_number_2         float64\n",
      "odds_2                       float64\n",
      "order_2                        int64\n",
      "burden_weight_2              float64\n",
      "race_distance_2              float64\n",
      "ground_condition_2           float64\n",
      "goal_time_2                  float64\n",
      "half_order_2                 float64\n",
      "last_time_2                  float64\n",
      "horse_weight_2               float64\n",
      "weather_circumstance_小雨_2    float64\n",
      "weather_circumstance_小雪_2    float64\n",
      "weather_circumstance_晴_2     float64\n",
      "weather_circumstance_曇_2     float64\n",
      "weather_circumstance_雨_2     float64\n",
      "weather_circumstance_雪_2     float64\n",
      "main_place_その他_2             float64\n",
      "main_place_中京_2              float64\n",
      "main_place_中山_2              float64\n",
      "main_place_京都_2              float64\n",
      "main_place_函館_2              float64\n",
      "main_place_小倉_2              float64\n",
      "main_place_新潟_2              float64\n",
      "main_place_札幌_2              float64\n",
      "main_place_東京_2              float64\n",
      "main_place_福島_2              float64\n",
      "main_place_阪神_2              float64\n",
      "race_rank_2                  float64\n",
      "ground_type_ダ_2              float64\n",
      "ground_type_芝_2              float64\n",
      "ground_type_障_2              float64\n",
      "horse_weight_dif_2           float64\n",
      "same_jockey_2                float64\n",
      "whole_horse_number_3         float64\n",
      "odds_3                       float64\n",
      "order_3                        int64\n",
      "burden_weight_3              float64\n",
      "race_distance_3              float64\n",
      "ground_condition_3           float64\n",
      "goal_time_3                  float64\n",
      "half_order_3                 float64\n",
      "last_time_3                  float64\n",
      "horse_weight_3               float64\n",
      "weather_circumstance_小雨_3    float64\n",
      "weather_circumstance_小雪_3    float64\n",
      "weather_circumstance_晴_3     float64\n",
      "weather_circumstance_曇_3     float64\n",
      "weather_circumstance_雨_3     float64\n",
      "weather_circumstance_雪_3     float64\n",
      "main_place_その他_3             float64\n",
      "main_place_中京_3              float64\n",
      "main_place_中山_3              float64\n",
      "main_place_京都_3              float64\n",
      "main_place_函館_3              float64\n",
      "main_place_小倉_3              float64\n",
      "main_place_新潟_3              float64\n",
      "main_place_札幌_3              float64\n",
      "main_place_東京_3              float64\n",
      "main_place_福島_3              float64\n",
      "main_place_阪神_3              float64\n",
      "race_rank_3                  float64\n",
      "ground_type_ダ_3              float64\n",
      "ground_type_芝_3              float64\n",
      "ground_type_障_3              float64\n",
      "horse_weight_dif_3           float64\n",
      "same_jockey_3                float64\n",
      "same_jockey                  float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# # load data\n",
    "# data = pd.read_csv(\"../data/csv/data.csv\", sep = \",\")\n",
    "# print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217032\n"
     ]
    }
   ],
   "source": [
    "# data.dropna(inplace=True)\n",
    "# print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215967\n"
     ]
    }
   ],
   "source": [
    "# data.drop_duplicates(inplace=True)\n",
    "# print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adjust columns type\n",
    "# data['race_id'] = data['race_id'].astype(str)\n",
    "# data['order'] = data['order'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # delete race day information\n",
    "# data.drop([\"horse_number\", \"half_order\", \"goal_time\" ,\"last_time\", \"horse_weight\", \"horse_weight_dif\", \"frame_number\",\"pop\"], axis = 1, inplace=True)\n",
    "# # \"race_round\",\n",
    "# data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race_id                       object\n",
      "race_round                     int64\n",
      "ground_condition               int64\n",
      "total_horse_number             int64\n",
      "order                         object\n",
      "age                            int64\n",
      "burden_weight                float64\n",
      "odds                         float64\n",
      "race_rank                      int64\n",
      "distance                       int64\n",
      "ground_type_ダ                  int64\n",
      "ground_type_芝                  int64\n",
      "circle_右                       int64\n",
      "circle_左                       int64\n",
      "weather_circumstance_小雨        int64\n",
      "weather_circumstance_小雪        int64\n",
      "weather_circumstance_晴         int64\n",
      "weather_circumstance_曇         int64\n",
      "weather_circumstance_雨         int64\n",
      "weather_circumstance_雪         int64\n",
      "place_中京                       int64\n",
      "place_中山                       int64\n",
      "place_京都                       int64\n",
      "place_函館                       int64\n",
      "place_小倉                       int64\n",
      "place_新潟                       int64\n",
      "place_札幌                       int64\n",
      "place_東京                       int64\n",
      "place_福島                       int64\n",
      "place_阪神                       int64\n",
      "sex_セ                          int64\n",
      "sex_牝                          int64\n",
      "sex_牡                          int64\n",
      "f_grass_win_rate             float64\n",
      "f_dart_win_rate              float64\n",
      "f_win_rate                   float64\n",
      "g_f_grass_win_rate           float64\n",
      "g_f_dart_win_rate            float64\n",
      "g_f_win_rate                 float64\n",
      "m_grass_win_rate             float64\n",
      "m_dart_win_rate              float64\n",
      "m_win_rate                   float64\n",
      "whole_horse_number_1         float64\n",
      "odds_1                       float64\n",
      "order_1                        int64\n",
      "burden_weight_1              float64\n",
      "race_distance_1              float64\n",
      "ground_condition_1           float64\n",
      "goal_time_1                  float64\n",
      "half_order_1                 float64\n",
      "last_time_1                  float64\n",
      "horse_weight_1               float64\n",
      "weather_circumstance_小雨_1    float64\n",
      "weather_circumstance_小雪_1    float64\n",
      "weather_circumstance_晴_1     float64\n",
      "weather_circumstance_曇_1     float64\n",
      "weather_circumstance_雨_1     float64\n",
      "weather_circumstance_雪_1     float64\n",
      "main_place_その他_1             float64\n",
      "main_place_中京_1              float64\n",
      "main_place_中山_1              float64\n",
      "main_place_京都_1              float64\n",
      "main_place_函館_1              float64\n",
      "main_place_小倉_1              float64\n",
      "main_place_新潟_1              float64\n",
      "main_place_札幌_1              float64\n",
      "main_place_東京_1              float64\n",
      "main_place_福島_1              float64\n",
      "main_place_阪神_1              float64\n",
      "race_rank_1                  float64\n",
      "ground_type_ダ_1              float64\n",
      "ground_type_芝_1              float64\n",
      "ground_type_障_1              float64\n",
      "horse_weight_dif_1           float64\n",
      "same_jockey_1                float64\n",
      "whole_horse_number_2         float64\n",
      "odds_2                       float64\n",
      "order_2                        int64\n",
      "burden_weight_2              float64\n",
      "race_distance_2              float64\n",
      "ground_condition_2           float64\n",
      "goal_time_2                  float64\n",
      "half_order_2                 float64\n",
      "last_time_2                  float64\n",
      "horse_weight_2               float64\n",
      "weather_circumstance_小雨_2    float64\n",
      "weather_circumstance_小雪_2    float64\n",
      "weather_circumstance_晴_2     float64\n",
      "weather_circumstance_曇_2     float64\n",
      "weather_circumstance_雨_2     float64\n",
      "weather_circumstance_雪_2     float64\n",
      "main_place_その他_2             float64\n",
      "main_place_中京_2              float64\n",
      "main_place_中山_2              float64\n",
      "main_place_京都_2              float64\n",
      "main_place_函館_2              float64\n",
      "main_place_小倉_2              float64\n",
      "main_place_新潟_2              float64\n",
      "main_place_札幌_2              float64\n",
      "main_place_東京_2              float64\n",
      "main_place_福島_2              float64\n",
      "main_place_阪神_2              float64\n",
      "race_rank_2                  float64\n",
      "ground_type_ダ_2              float64\n",
      "ground_type_芝_2              float64\n",
      "ground_type_障_2              float64\n",
      "horse_weight_dif_2           float64\n",
      "same_jockey_2                float64\n",
      "whole_horse_number_3         float64\n",
      "odds_3                       float64\n",
      "order_3                        int64\n",
      "burden_weight_3              float64\n",
      "race_distance_3              float64\n",
      "ground_condition_3           float64\n",
      "goal_time_3                  float64\n",
      "half_order_3                 float64\n",
      "last_time_3                  float64\n",
      "horse_weight_3               float64\n",
      "weather_circumstance_小雨_3    float64\n",
      "weather_circumstance_小雪_3    float64\n",
      "weather_circumstance_晴_3     float64\n",
      "weather_circumstance_曇_3     float64\n",
      "weather_circumstance_雨_3     float64\n",
      "weather_circumstance_雪_3     float64\n",
      "main_place_その他_3             float64\n",
      "main_place_中京_3              float64\n",
      "main_place_中山_3              float64\n",
      "main_place_京都_3              float64\n",
      "main_place_函館_3              float64\n",
      "main_place_小倉_3              float64\n",
      "main_place_新潟_3              float64\n",
      "main_place_札幌_3              float64\n",
      "main_place_東京_3              float64\n",
      "main_place_福島_3              float64\n",
      "main_place_阪神_3              float64\n",
      "race_rank_3                  float64\n",
      "ground_type_ダ_3              float64\n",
      "ground_type_芝_3              float64\n",
      "ground_type_障_3              float64\n",
      "horse_weight_dif_3           float64\n",
      "same_jockey_3                float64\n",
      "same_jockey                  float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # standarlization \n",
    "# no_scale_data = data[['race_id','order']]\n",
    "# scale_columns = data.drop(['race_id','order'], axis=1).columns.values\n",
    "# standard_scale = StandardScaler()\n",
    "# data = pd.DataFrame(standard_scale.fit_transform(data[scale_columns]))\n",
    "\n",
    "# # PCA\n",
    "# pca = PCA()\n",
    "# data = pd.DataFrame(pca.fit_transform(data))\n",
    "# contrb_rate = pd.DataFrame(pca.explained_variance_ratio_, columns = ['rate'])\n",
    "# sum_rate = 0\n",
    "\n",
    "# #  # to get the colum of the specific contribution rate\n",
    "# # for i in range(len(contrb_rate)):\n",
    "# #     sum_rate += contrb_rate.rate[i]\n",
    "# #     if sum_rate >= 0.9:\n",
    "# #         max_col = i + 1\n",
    "# #         break\n",
    "\n",
    "max_col = 84\n",
    "# # print(max_col)\n",
    "# data = data.loc[:, :max_col-1]\n",
    "# print(data.shape[1])\n",
    "# # print(data.head(5))\n",
    "# # print(len(data), len(no_scale_data))\n",
    "# # print(no_scale_data[no_scale_data['race_id'].isnull()])\n",
    "# data = pd.concat([data, no_scale_data], axis=1)\n",
    "# dump(standard_scale, open(\"standard_scale.pkl\", \"wb\"))\n",
    "# dump(pca, open(\"pca.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sorted(no_scale_data['order'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.shape)\n",
    "# print(data.dtypes)\n",
    "# print(data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_time_series_data(raw_data):\n",
    "#     number_of_race = raw_data.race_id.nunique()\n",
    "#     time_series_data = np.full((number_of_race, 24, max_col), 0.0)#-float('inf')\n",
    "#     label = np.full((number_of_race, 24), 25)\n",
    "#     race_number = 0\n",
    "#     horse_number = 0\n",
    "#     for i in range(len(raw_data)):\n",
    "#         if i == 0:\n",
    "#             label[race_number][horse_number] = float(raw_data.iloc[i].order)\n",
    "#             time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "#             horse_number += 1\n",
    "#             continue\n",
    "#         # add new race\n",
    "#         if data.iloc[i].race_id != data.iloc[i-1].race_id:\n",
    "#             race_number += 1\n",
    "#             horse_number = 0\n",
    "#             label[race_number][horse_number] = float(raw_data.iloc[i].order)\n",
    "#             time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "#             horse_number += 1\n",
    "#         # add new horse to the same race\n",
    "#         else:\n",
    "# #             print(data.iloc[i].race_id ,race_number, horse_number)\n",
    "#             label[race_number][horse_number] = float(raw_data.iloc[i].order)\n",
    "#             time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "#             horse_number += 1\n",
    "#     del raw_data\n",
    "#     return time_series_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20132, 24, 84)\n",
      "(20132, 24)\n"
     ]
    }
   ],
   "source": [
    "# X, y_order = create_time_series_data(data)\n",
    "# np.save('X', X)\n",
    "# np.save('y_order', y_order)\n",
    "X = np.load('X.npy')\n",
    "y_order = np.load('y_order.npy')\n",
    "# del data\n",
    "print(X.shape)\n",
    "print(y_order.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.50071551 -0.00976296  5.28838983 ...  0.29231694  0.46891232\n",
      "   0.09022868]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "X = X.astype('float32')\n",
    "print(X.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[ 7 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25]\n"
     ]
    }
   ],
   "source": [
    "print(X[0][11])\n",
    "print(y_order[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26,)\n",
      "[0.00000000e+00 1.21687621e+00 1.21555368e+00 1.23063757e+00\n",
      " 1.24041898e+00 1.25990362e+00 1.26100846e+00 1.28155834e+00\n",
      " 1.32325490e+00 1.37157651e+00 1.45736210e+00 1.56413643e+00\n",
      " 1.73521807e+00 1.98052140e+00 2.27608819e+00 2.71833648e+00\n",
      " 3.58156912e+00 1.49347181e+01 2.00918164e+01 1.25825000e+03\n",
      " 1.43800000e+03 2.51650000e+03 3.35533333e+03 4.02640000e+03\n",
      " 5.03300000e+03 7.53440294e-02]\n"
     ]
    }
   ],
   "source": [
    "alpha = len(y_order) / pd.DataFrame(y_order.flatten()).value_counts()\n",
    "alpha = alpha.sort_index()\n",
    "alpha = np.array(alpha)\n",
    "alpha = np.append(0,alpha)\n",
    "print(alpha.shape)\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]]\n",
      "(20132, 24, 26)\n",
      "(20132, 24, 84)\n"
     ]
    }
   ],
   "source": [
    "# creating X,y (parameters and target)\n",
    "y = np_utils.to_categorical(y_order, dtype='float32')\n",
    "print(y[5])\n",
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def smooth_label(label, factor=0.03):\n",
    "#     # smooth label\n",
    "#     label *= (1 - factor)\n",
    "# #     label[:,:,1:4] += (factor / 3)\n",
    "\n",
    "#     for i in range(label.shape[0]):\n",
    "#         for j in range(label.shape[1]):\n",
    "#             t = np.where(label[i][j] == 1 - factor)\n",
    "#             label[i,j,max(0,t[0][0]-1):min(26,t[0][0]+2)] += (factor / 3)\n",
    "#     return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.   0.   0.   0.01 0.98 0.01 0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.01 0.98 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]]\n",
      "(20132, 24, 26)\n",
      "(20132, 24, 84)\n"
     ]
    }
   ],
   "source": [
    "y = smooth_label(y) \n",
    "print(y[4])\n",
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.race_id.value_counts().plot.hist(bins=25,range=(1,25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(pd.DataFrame(y_order.flatten()).plot.hist(bins=25))## ,ylim=(0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.01, random_state = 0)\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.0841513e+00  7.0771635e-02 -5.8418870e-01  2.4646287e-01\n",
      "  1.8285137e+00 -1.1262701e+00  6.5735537e-01  1.5329328e+00\n",
      "  6.2066919e-01 -6.7069030e-01  1.0873965e+00  1.7795300e-01\n",
      " -2.2035263e+00 -5.5455178e-01  1.5312845e+00  1.9849390e+00\n",
      " -1.9881687e+00 -4.5285341e-01 -1.1557992e+00  3.0845302e-01\n",
      "  6.5695131e-01 -1.6801572e+00  1.2509111e-01 -6.3480353e-01\n",
      "  2.6424465e-01 -1.8350914e+00  1.3959821e+00 -1.1001785e+00\n",
      " -2.4194989e+00  2.9623857e+00  1.4317343e+00  1.9310854e-02\n",
      "  3.4215423e-01  7.4966168e-01 -1.0088371e+00  8.0969352e-01\n",
      " -2.5986239e-01 -8.4669787e-01  1.0779321e+00  6.1363038e-02\n",
      " -1.5148355e+00 -1.8475902e-03 -8.8226789e-01 -6.8742210e-01\n",
      " -2.6793274e-01  1.6574528e+00 -2.0822718e+00 -1.7538213e+00\n",
      "  4.2922177e+00  1.6022534e+00 -4.4454589e-01 -6.7625672e-01\n",
      " -8.8914499e-02 -1.1668312e-01 -3.3394665e-01 -7.5860035e-01\n",
      "  3.7061727e-01 -6.4284933e-01  3.2509527e-01  4.8748016e-01\n",
      "  1.5954223e+00  1.4406800e-01 -8.7014019e-01  6.6753399e-01\n",
      " -1.0578674e+00  4.2397591e-01  6.7731267e-01  3.3324012e-01\n",
      " -1.4028546e+00 -1.2829390e+00 -9.2539579e-01  3.5360447e-01\n",
      " -5.5841304e-02  2.7825090e-01  1.1097189e-01  1.2788044e+00\n",
      "  4.5809287e-01  1.7082896e+00 -6.1947507e-01  1.6214646e+00\n",
      " -8.2937258e-01  6.4784966e-02  9.0706927e-01 -1.0770866e+00]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19930, 24, 84)\n",
      "(202, 24, 84)\n",
      "(19930, 24, 26)\n",
      "(202, 24, 26)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def categorical_focal_loss(alpha, gamma):\n",
    "#     \"\"\"\n",
    "#     Softmax version of focal loss.\n",
    "#     When there is a skew between different categories/labels in your data set, you can try to apply this function as a\n",
    "#     loss.\n",
    "#            m\n",
    "#       FL = ∑  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n",
    "#           c=1\n",
    "#       where m = number of classes, c = class and o = observation\n",
    "#     Parameters:\n",
    "#       alpha -- the same as weighing factor in balanced cross entropy. Alpha is used to specify the weight of different\n",
    "#       categories/labels, the size of the array needs to be consistent with the number of classes.\n",
    "#       gamma -- focusing parameter for modulating factor (1-p)\n",
    "#     Default value:\n",
    "#       gamma -- 2.0 as mentioned in the paper\n",
    "#       alpha -- 0.25 as mentioned in the paper\n",
    "#     References:\n",
    "#         Official paper: https://arxiv.org/pdf/1708.02002.pdf\n",
    "#         https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy\n",
    "#     Usage:\n",
    "#      model.compile(loss=[categorical_focal_loss(alpha=[[.25, .25, .25]], gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "#     \"\"\"\n",
    "\n",
    "#     alpha = np.array(alpha, dtype=np.float32)\n",
    "\n",
    "#     def categorical_focal_loss_fixed(y_true, y_pred):\n",
    "#         \"\"\"\n",
    "#         :param y_true: A tensor of the same shape as `y_pred`\n",
    "#         :param y_pred: A tensor resulting from a softmax\n",
    "#         :return: Output tensor.\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Clip the prediction value to prevent NaN's and Inf's\n",
    "#         epsilon = K.epsilon()\n",
    "#         y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "#         # Calculate Cross Entropy\n",
    "#         cross_entropy = -y_true * K.log(y_pred)\n",
    "\n",
    "#         # Calculate Focal Loss\n",
    "#         loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
    "\n",
    "#         # Compute mean loss in mini_batch\n",
    "#         return K.mean(K.sum(loss, axis=-1))\n",
    "\n",
    "#     return categorical_focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataset with batch size\n",
    "batch_size = 2048 # hyperparameter\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=19930).batch(batch_size)\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "valid_dataset = valid_dataset.shuffle(buffer_size=202).batch(batch_size)\n",
    "\n",
    "\n",
    "del X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters\n",
    "num_layers = 4 # hyperparameter\n",
    "d_model = max_col # 4*35 84=4*3*7\n",
    "num_heads = 28 # hyperparameter *must be a factor of d_model*\n",
    "d_ffn = 256 # hyperparameter\n",
    "pe_input = 24\n",
    "target_size = 26\n",
    "dropout_rate = 0.1 # hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# set up model\n",
    "trans_race = transformer.TransRace(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ffn=d_ffn,\n",
    "    pe_input=pe_input,\n",
    "    target_size=target_size,\n",
    "    rate=dropout_rate,\n",
    ")\n",
    "opt = optimizers.Adam(decay=0.01)\n",
    "loss = categorical_focal_loss(alpha=[alpha], gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method TransRace.call of <models.transformer.TransRace object at 0x7fe6c7228518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TransRace.call of <models.transformer.TransRace object at 0x7fe6c7228518>>, which Python reported as:\n",
      "    def call(self, inputs, training=True):\n",
      "        inp = inputs\n",
      "\n",
      "        enc_padding_mask = self.create_masks(inp) #, tar\n",
      "        # , look_ahead_mask, dec_padding_mask\n",
      "\n",
      "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
      "\n",
      "        final_output = self.final_layer(enc_output)\n",
      "#         print(\"final output shape\", final_output.shape)\n",
      "\n",
      "        return final_output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method TransRace.call of <models.transformer.TransRace object at 0x7fe6c7228518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TransRace.call of <models.transformer.TransRace object at 0x7fe6c7228518>>, which Python reported as:\n",
      "    def call(self, inputs, training=True):\n",
      "        inp = inputs\n",
      "\n",
      "        enc_padding_mask = self.create_masks(inp) #, tar\n",
      "        # , look_ahead_mask, dec_padding_mask\n",
      "\n",
      "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
      "\n",
      "        final_output = self.final_layer(enc_output)\n",
      "#         print(\"final output shape\", final_output.shape)\n",
      "\n",
      "        return final_output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fe6c7228a20>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fe6c7228a20>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fe6c7228a20>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fe6c7228a20>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c7228978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c7228978>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c7228978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c7228978>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c7228978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c7228978>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c7228978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c7228978>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c7228978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c7228978>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c7228978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c7228978>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fe6c724aa58>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fe6c724aa58>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fe6c724aa58>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fe6c724aa58>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c724aac8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c724aac8>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c724aac8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c724aac8>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c724aac8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c724aac8>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c724aac8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c724aac8>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c724aac8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c724aac8>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c724aac8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c724aac8>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fe6c71dd7f0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fe6c71dd7f0>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fe6c71dd7f0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fe6c71dd7f0>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c71dd860>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c71dd860>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c71dd860>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c71dd860>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c71dd860>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c71dd860>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c71dd860>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c71dd860>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c71dd860>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c71dd860>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c71dd860>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c71dd860>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fe6c71ef4a8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fe6c71ef4a8>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fe6c71ef4a8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fe6c71ef4a8>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c71ef518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c71ef518>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c71ef518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c71ef518>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c71ef518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c71ef518>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c71ef518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c71ef518>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c71ef518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c71ef518>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c71ef518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fe6c71ef518>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:From /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 10 steps, validate on 1 steps\n",
      "Epoch 1/200\n",
      "10/10 [==============================] - 14s 1s/step - loss: 3.5327 - acc: 0.0627 - val_loss: 3.2767 - val_acc: 0.0623\n",
      "Epoch 2/200\n",
      "10/10 [==============================] - 4s 441ms/step - loss: 2.9308 - acc: 0.0622 - val_loss: 2.8226 - val_acc: 0.0994\n",
      "Epoch 3/200\n",
      "10/10 [==============================] - 4s 439ms/step - loss: 2.7716 - acc: 0.0939 - val_loss: 2.8920 - val_acc: 0.1691\n",
      "Epoch 4/200\n",
      "10/10 [==============================] - 4s 446ms/step - loss: 2.7370 - acc: 0.0967 - val_loss: 2.4635 - val_acc: 0.2240\n",
      "Epoch 5/200\n",
      "10/10 [==============================] - 4s 438ms/step - loss: 2.6648 - acc: 0.1247 - val_loss: 2.6422 - val_acc: 0.1634\n",
      "Epoch 6/200\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 2.6431 - acc: 0.1155 - val_loss: 2.5998 - val_acc: 0.1869\n",
      "Epoch 7/200\n",
      "10/10 [==============================] - 4s 425ms/step - loss: 2.5897 - acc: 0.1395 - val_loss: 2.4290 - val_acc: 0.1949\n",
      "Epoch 8/200\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 2.5922 - acc: 0.1257 - val_loss: 2.5891 - val_acc: 0.2261\n",
      "Epoch 9/200\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 2.5388 - acc: 0.1462 - val_loss: 2.4215 - val_acc: 0.1875\n",
      "Epoch 10/200\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 2.5430 - acc: 0.1479 - val_loss: 2.3629 - val_acc: 0.2133\n",
      "Epoch 11/200\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 2.5435 - acc: 0.1648 - val_loss: 2.3608 - val_acc: 0.2339\n",
      "Epoch 12/200\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 2.5418 - acc: 0.1652 - val_loss: 2.4759 - val_acc: 0.2486\n",
      "Epoch 13/200\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 2.4910 - acc: 0.1726 - val_loss: 2.3484 - val_acc: 0.2436\n",
      "Epoch 14/200\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 2.4839 - acc: 0.1963 - val_loss: 2.7191 - val_acc: 0.2679\n",
      "Epoch 15/200\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 2.5052 - acc: 0.1970 - val_loss: 2.4182 - val_acc: 0.2776\n",
      "Epoch 16/200\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 2.4490 - acc: 0.2275 - val_loss: 2.2798 - val_acc: 0.2964\n",
      "Epoch 17/200\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 2.4261 - acc: 0.2528 - val_loss: 2.3445 - val_acc: 0.3121\n",
      "Epoch 18/200\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 2.4573 - acc: 0.2577 - val_loss: 2.3461 - val_acc: 0.3325\n",
      "Epoch 19/200\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 2.4303 - acc: 0.2831 - val_loss: 2.2686 - val_acc: 0.3342\n",
      "Epoch 20/200\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 2.3965 - acc: 0.3054 - val_loss: 2.2982 - val_acc: 0.3581\n",
      "Epoch 21/200\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 2.3497 - acc: 0.3347 - val_loss: 2.3508 - val_acc: 0.3787\n",
      "Epoch 22/200\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 2.3355 - acc: 0.3527 - val_loss: 2.3256 - val_acc: 0.4035\n",
      "Epoch 23/200\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 2.3396 - acc: 0.3755 - val_loss: 2.2692 - val_acc: 0.4138\n",
      "Epoch 24/200\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 2.3215 - acc: 0.3942 - val_loss: 2.2479 - val_acc: 0.4363\n",
      "Epoch 25/200\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 2.3495 - acc: 0.4092 - val_loss: 2.3892 - val_acc: 0.4441\n",
      "Epoch 26/200\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 2.3086 - acc: 0.4189 - val_loss: 2.3653 - val_acc: 0.4585\n",
      "Epoch 27/200\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 2.2692 - acc: 0.4336 - val_loss: 2.2740 - val_acc: 0.4610\n",
      "Epoch 28/200\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 2.2430 - acc: 0.4416 - val_loss: 2.2506 - val_acc: 0.4678\n",
      "Epoch 29/200\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 2.2137 - acc: 0.4493 - val_loss: 2.2749 - val_acc: 0.4682\n",
      "Epoch 30/200\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 2.2134 - acc: 0.4587 - val_loss: 2.2384 - val_acc: 0.4755\n",
      "Epoch 31/200\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 2.1881 - acc: 0.4627 - val_loss: 2.2124 - val_acc: 0.4808\n",
      "Epoch 32/200\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 2.2046 - acc: 0.4699 - val_loss: 2.2047 - val_acc: 0.4868\n",
      "Epoch 33/200\n",
      "10/10 [==============================] - 4s 443ms/step - loss: 2.1658 - acc: 0.4751 - val_loss: 2.2160 - val_acc: 0.4880\n",
      "Epoch 34/200\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 2.1766 - acc: 0.4803 - val_loss: 2.2015 - val_acc: 0.4926\n",
      "Epoch 35/200\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 2.1860 - acc: 0.4843 - val_loss: 2.1926 - val_acc: 0.4950\n",
      "Epoch 36/200\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 2.1620 - acc: 0.4894 - val_loss: 2.1829 - val_acc: 0.4983\n",
      "Epoch 37/200\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 2.1557 - acc: 0.4948 - val_loss: 2.1626 - val_acc: 0.5021\n",
      "Epoch 38/200\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 2.1932 - acc: 0.4972 - val_loss: 2.1673 - val_acc: 0.5058\n",
      "Epoch 39/200\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 2.1832 - acc: 0.5014 - val_loss: 2.2001 - val_acc: 0.5087\n",
      "Epoch 40/200\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 2.1419 - acc: 0.5032 - val_loss: 2.1660 - val_acc: 0.5118\n",
      "Epoch 41/200\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 2.1314 - acc: 0.5085 - val_loss: 2.1514 - val_acc: 0.5165\n",
      "Epoch 42/200\n",
      "10/10 [==============================] - 4s 426ms/step - loss: 2.1209 - acc: 0.5102 - val_loss: 2.1484 - val_acc: 0.5159\n",
      "Epoch 43/200\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 2.1581 - acc: 0.5105 - val_loss: 2.1429 - val_acc: 0.5206\n",
      "Epoch 44/200\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 2.1392 - acc: 0.5137 - val_loss: 2.1512 - val_acc: 0.5231\n",
      "Epoch 45/200\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 2.1297 - acc: 0.5161 - val_loss: 2.1441 - val_acc: 0.5248\n",
      "Epoch 46/200\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 2.1330 - acc: 0.5187 - val_loss: 2.1473 - val_acc: 0.5274\n",
      "Epoch 47/200\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 2.1279 - acc: 0.5197 - val_loss: 2.1370 - val_acc: 0.5314\n",
      "Epoch 48/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 2.1379 - acc: 0.5215 - val_loss: 2.1393 - val_acc: 0.5299\n",
      "Epoch 49/200\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 2.1226 - acc: 0.5226 - val_loss: 2.1394 - val_acc: 0.5342\n",
      "Epoch 50/200\n",
      "10/10 [==============================] - 4s 432ms/step - loss: 2.1130 - acc: 0.5259 - val_loss: 2.1332 - val_acc: 0.5338\n",
      "Epoch 51/200\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 2.1023 - acc: 0.5258 - val_loss: 2.1321 - val_acc: 0.5353\n",
      "Epoch 52/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 4s 427ms/step - loss: 2.1050 - acc: 0.5298 - val_loss: 2.1548 - val_acc: 0.5377\n",
      "Epoch 53/200\n",
      "10/10 [==============================] - 4s 425ms/step - loss: 2.0764 - acc: 0.5302 - val_loss: 2.1733 - val_acc: 0.5367\n",
      "Epoch 54/200\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 2.0770 - acc: 0.5307 - val_loss: 2.1745 - val_acc: 0.5394\n",
      "Epoch 55/200\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 2.0872 - acc: 0.5326 - val_loss: 2.1485 - val_acc: 0.5394\n",
      "Epoch 56/200\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 2.0794 - acc: 0.5342 - val_loss: 2.1326 - val_acc: 0.5402\n",
      "Epoch 57/200\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 2.0653 - acc: 0.5349 - val_loss: 2.1217 - val_acc: 0.5413\n",
      "Epoch 58/200\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 2.0752 - acc: 0.5360 - val_loss: 2.1144 - val_acc: 0.5423\n",
      "Epoch 59/200\n",
      "10/10 [==============================] - 4s 422ms/step - loss: 2.0640 - acc: 0.5364 - val_loss: 2.1111 - val_acc: 0.5423\n",
      "Epoch 60/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 2.1056 - acc: 0.5384 - val_loss: 2.1086 - val_acc: 0.5413\n",
      "Epoch 61/200\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 2.0676 - acc: 0.5391 - val_loss: 2.1229 - val_acc: 0.5458\n",
      "Epoch 62/200\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 2.0721 - acc: 0.5412 - val_loss: 2.1141 - val_acc: 0.5474\n",
      "Epoch 63/200\n",
      "10/10 [==============================] - 4s 425ms/step - loss: 2.0646 - acc: 0.5414 - val_loss: 2.1040 - val_acc: 0.5452\n",
      "Epoch 64/200\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 2.0894 - acc: 0.5435 - val_loss: 2.0974 - val_acc: 0.5493\n",
      "Epoch 65/200\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 2.0700 - acc: 0.5436 - val_loss: 2.0954 - val_acc: 0.5499\n",
      "Epoch 66/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 2.0626 - acc: 0.5438 - val_loss: 2.1007 - val_acc: 0.5499\n",
      "Epoch 67/200\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 2.0915 - acc: 0.5465 - val_loss: 2.0975 - val_acc: 0.5528\n",
      "Epoch 68/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 2.0506 - acc: 0.5469 - val_loss: 2.0921 - val_acc: 0.5491\n",
      "Epoch 69/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 2.0404 - acc: 0.5466 - val_loss: 2.0929 - val_acc: 0.5497\n",
      "Epoch 70/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 2.0437 - acc: 0.5480 - val_loss: 2.1085 - val_acc: 0.5512\n",
      "Epoch 71/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 2.0526 - acc: 0.5495 - val_loss: 2.1182 - val_acc: 0.5518\n",
      "Epoch 72/200\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 2.0352 - acc: 0.5496 - val_loss: 2.1149 - val_acc: 0.5545\n",
      "Epoch 73/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 2.0585 - acc: 0.5493 - val_loss: 2.1036 - val_acc: 0.5522\n",
      "Epoch 74/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 2.0442 - acc: 0.5503 - val_loss: 2.0921 - val_acc: 0.5526\n",
      "Epoch 75/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 2.0512 - acc: 0.5509 - val_loss: 2.0908 - val_acc: 0.5542\n",
      "Epoch 76/200\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 2.0559 - acc: 0.5521 - val_loss: 2.0936 - val_acc: 0.5547\n",
      "Epoch 77/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 2.0678 - acc: 0.5526 - val_loss: 2.1077 - val_acc: 0.5538\n",
      "Epoch 78/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 2.0491 - acc: 0.5530 - val_loss: 2.1064 - val_acc: 0.5536\n",
      "Epoch 79/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 2.0258 - acc: 0.5540 - val_loss: 2.1047 - val_acc: 0.5538\n",
      "Epoch 80/200\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 2.0380 - acc: 0.5545 - val_loss: 2.0968 - val_acc: 0.5565\n",
      "Epoch 81/200\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 2.0195 - acc: 0.5545 - val_loss: 2.0851 - val_acc: 0.5567\n",
      "Epoch 82/200\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 2.0307 - acc: 0.5547 - val_loss: 2.0829 - val_acc: 0.5575\n",
      "Epoch 83/200\n",
      "10/10 [==============================] - 4s 437ms/step - loss: 2.0288 - acc: 0.5571 - val_loss: 2.0862 - val_acc: 0.5580\n",
      "Epoch 84/200\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 2.0383 - acc: 0.5559 - val_loss: 2.0900 - val_acc: 0.5582\n",
      "Epoch 85/200\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 2.0302 - acc: 0.5573 - val_loss: 2.0956 - val_acc: 0.5590\n",
      "Epoch 86/200\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 2.0439 - acc: 0.5573 - val_loss: 2.0954 - val_acc: 0.5623\n",
      "Epoch 87/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 2.0308 - acc: 0.5580 - val_loss: 2.0961 - val_acc: 0.5621\n",
      "Epoch 88/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 2.0276 - acc: 0.5580 - val_loss: 2.0961 - val_acc: 0.5623\n",
      "Epoch 89/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 2.0386 - acc: 0.5583 - val_loss: 2.0891 - val_acc: 0.5613\n",
      "Epoch 90/200\n",
      "10/10 [==============================] - 4s 425ms/step - loss: 2.0205 - acc: 0.5590 - val_loss: 2.0855 - val_acc: 0.5619\n",
      "Epoch 91/200\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 2.0395 - acc: 0.5595 - val_loss: 2.0918 - val_acc: 0.5625\n",
      "Epoch 92/200\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 2.0187 - acc: 0.5608 - val_loss: 2.0924 - val_acc: 0.5637\n",
      "Epoch 93/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 2.0433 - acc: 0.5607 - val_loss: 2.0834 - val_acc: 0.5623\n",
      "Epoch 94/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 2.0265 - acc: 0.5603 - val_loss: 2.0786 - val_acc: 0.5621\n",
      "Epoch 95/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 2.0231 - acc: 0.5608 - val_loss: 2.0756 - val_acc: 0.5617\n",
      "Epoch 96/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 2.0335 - acc: 0.5623 - val_loss: 2.0722 - val_acc: 0.5617\n",
      "Epoch 97/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 2.0343 - acc: 0.5618 - val_loss: 2.0830 - val_acc: 0.5611\n",
      "Epoch 98/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 2.0376 - acc: 0.5627 - val_loss: 2.0924 - val_acc: 0.5619\n",
      "Epoch 99/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 2.0575 - acc: 0.5624 - val_loss: 2.0787 - val_acc: 0.5617\n",
      "Epoch 100/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 2.0196 - acc: 0.5631 - val_loss: 2.0677 - val_acc: 0.5613\n",
      "Epoch 101/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 2.0218 - acc: 0.5625 - val_loss: 2.0607 - val_acc: 0.5608\n",
      "Epoch 102/200\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 2.0266 - acc: 0.5637 - val_loss: 2.0663 - val_acc: 0.5631\n",
      "Epoch 103/200\n",
      "10/10 [==============================] - 5s 463ms/step - loss: 2.0106 - acc: 0.5635 - val_loss: 2.0730 - val_acc: 0.5623\n",
      "Epoch 104/200\n",
      "10/10 [==============================] - 5s 455ms/step - loss: 2.0088 - acc: 0.5646 - val_loss: 2.0773 - val_acc: 0.5631\n",
      "Epoch 105/200\n",
      "10/10 [==============================] - 4s 436ms/step - loss: 2.0210 - acc: 0.5642 - val_loss: 2.0696 - val_acc: 0.5652\n",
      "Epoch 106/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 2.0118 - acc: 0.5648 - val_loss: 2.0697 - val_acc: 0.5623\n",
      "Epoch 107/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 2.0184 - acc: 0.5654 - val_loss: 2.0749 - val_acc: 0.5646\n",
      "Epoch 108/200\n",
      "10/10 [==============================] - 4s 425ms/step - loss: 2.0193 - acc: 0.5654 - val_loss: 2.0783 - val_acc: 0.5639\n",
      "Epoch 109/200\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 2.0107 - acc: 0.5659 - val_loss: 2.0837 - val_acc: 0.5654\n",
      "Epoch 110/200\n",
      "10/10 [==============================] - 4s 426ms/step - loss: 2.0114 - acc: 0.5664 - val_loss: 2.0876 - val_acc: 0.5652\n",
      "Epoch 111/200\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 2.0186 - acc: 0.5662 - val_loss: 2.0916 - val_acc: 0.5656\n",
      "Epoch 112/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 2.0201 - acc: 0.5666 - val_loss: 2.0845 - val_acc: 0.5646\n",
      "Epoch 113/200\n",
      "10/10 [==============================] - 4s 422ms/step - loss: 2.0102 - acc: 0.5664 - val_loss: 2.0754 - val_acc: 0.5642\n",
      "Epoch 114/200\n",
      "10/10 [==============================] - 4s 422ms/step - loss: 2.0298 - acc: 0.5667 - val_loss: 2.0705 - val_acc: 0.5648\n",
      "Epoch 115/200\n",
      "10/10 [==============================] - 4s 425ms/step - loss: 1.9977 - acc: 0.5675 - val_loss: 2.0680 - val_acc: 0.5652\n",
      "Epoch 116/200\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 2.0393 - acc: 0.5674 - val_loss: 2.0714 - val_acc: 0.5668\n",
      "Epoch 117/200\n",
      "10/10 [==============================] - 4s 425ms/step - loss: 2.0122 - acc: 0.5675 - val_loss: 2.0753 - val_acc: 0.5650\n",
      "Epoch 118/200\n",
      "10/10 [==============================] - 4s 422ms/step - loss: 2.0030 - acc: 0.5679 - val_loss: 2.0801 - val_acc: 0.5664\n",
      "Epoch 119/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 2.0163 - acc: 0.5680 - val_loss: 2.0774 - val_acc: 0.5668\n",
      "Epoch 120/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 1.9952 - acc: 0.5682 - val_loss: 2.0754 - val_acc: 0.5668\n",
      "Epoch 121/200\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 2.0075 - acc: 0.5688 - val_loss: 2.0802 - val_acc: 0.5672\n",
      "Epoch 122/200\n",
      "10/10 [==============================] - 4s 422ms/step - loss: 2.0218 - acc: 0.5689 - val_loss: 2.0750 - val_acc: 0.5666\n",
      "Epoch 123/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 2.0009 - acc: 0.5701 - val_loss: 2.0753 - val_acc: 0.5668\n",
      "Epoch 124/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 2.0210 - acc: 0.5685 - val_loss: 2.0783 - val_acc: 0.5672\n",
      "Epoch 125/200\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 1.9967 - acc: 0.5693 - val_loss: 2.0756 - val_acc: 0.5687\n",
      "Epoch 126/200\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 2.0054 - acc: 0.5694 - val_loss: 2.0729 - val_acc: 0.5693\n",
      "Epoch 127/200\n",
      "10/10 [==============================] - 4s 422ms/step - loss: 1.9957 - acc: 0.5697 - val_loss: 2.0748 - val_acc: 0.5691\n",
      "Epoch 128/200\n",
      "10/10 [==============================] - 4s 422ms/step - loss: 2.0001 - acc: 0.5698 - val_loss: 2.0774 - val_acc: 0.5689\n",
      "Epoch 129/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 1.9942 - acc: 0.5694 - val_loss: 2.0791 - val_acc: 0.5689\n",
      "Epoch 130/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 2.0305 - acc: 0.5699 - val_loss: 2.0699 - val_acc: 0.5693\n",
      "Epoch 131/200\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 2.0114 - acc: 0.5712 - val_loss: 2.0507 - val_acc: 0.5695\n",
      "Epoch 132/200\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 1.9925 - acc: 0.5709 - val_loss: 2.0536 - val_acc: 0.5708\n",
      "Epoch 133/200\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 2.0098 - acc: 0.5710 - val_loss: 2.0561 - val_acc: 0.5712\n",
      "Epoch 134/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 2.0067 - acc: 0.5715 - val_loss: 2.0593 - val_acc: 0.5710\n",
      "Epoch 135/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 2.0086 - acc: 0.5719 - val_loss: 2.0617 - val_acc: 0.5703\n",
      "Epoch 136/200\n",
      "10/10 [==============================] - 4s 425ms/step - loss: 1.9984 - acc: 0.5716 - val_loss: 2.0674 - val_acc: 0.5708\n",
      "Epoch 137/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 1.9995 - acc: 0.5707 - val_loss: 2.0737 - val_acc: 0.5705\n",
      "Epoch 138/200\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 2.0105 - acc: 0.5717 - val_loss: 2.0800 - val_acc: 0.5716\n",
      "Epoch 139/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 2.0098 - acc: 0.5721 - val_loss: 2.0766 - val_acc: 0.5714\n",
      "Epoch 140/200\n",
      "10/10 [==============================] - 4s 425ms/step - loss: 2.0011 - acc: 0.5723 - val_loss: 2.0766 - val_acc: 0.5708\n",
      "Epoch 141/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 2.0065 - acc: 0.5722 - val_loss: 2.0745 - val_acc: 0.5705\n",
      "Epoch 142/200\n",
      "10/10 [==============================] - 4s 422ms/step - loss: 1.9942 - acc: 0.5717 - val_loss: 2.0726 - val_acc: 0.5716\n",
      "Epoch 143/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 2.0091 - acc: 0.5727 - val_loss: 2.0679 - val_acc: 0.5712\n",
      "Epoch 144/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 2.0052 - acc: 0.5723 - val_loss: 2.0696 - val_acc: 0.5714\n",
      "Epoch 145/200\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 2.0049 - acc: 0.5724 - val_loss: 2.0733 - val_acc: 0.5722\n",
      "Epoch 146/200\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 1.9923 - acc: 0.5729 - val_loss: 2.0760 - val_acc: 0.5726\n",
      "Epoch 147/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 1.9874 - acc: 0.5732 - val_loss: 2.0836 - val_acc: 0.5726\n",
      "Epoch 148/200\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 2.0097 - acc: 0.5733 - val_loss: 2.0749 - val_acc: 0.5728\n",
      "Epoch 149/200\n",
      "10/10 [==============================] - 4s 433ms/step - loss: 2.0250 - acc: 0.5729 - val_loss: 2.0712 - val_acc: 0.5722\n",
      "Epoch 150/200\n",
      "10/10 [==============================] - 4s 426ms/step - loss: 1.9955 - acc: 0.5738 - val_loss: 2.0798 - val_acc: 0.5720\n",
      "Epoch 151/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 1.9912 - acc: 0.5732 - val_loss: 2.0891 - val_acc: 0.5720\n",
      "Epoch 152/200\n",
      "10/10 [==============================] - 4s 425ms/step - loss: 1.9899 - acc: 0.5740 - val_loss: 2.0928 - val_acc: 0.5724\n",
      "Epoch 153/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 1.9925 - acc: 0.5739 - val_loss: 2.0934 - val_acc: 0.5726\n",
      "Epoch 154/200\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 1.9827 - acc: 0.5736 - val_loss: 2.0878 - val_acc: 0.5730\n",
      "Epoch 155/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 1.9889 - acc: 0.5746 - val_loss: 2.0739 - val_acc: 0.5728\n",
      "Epoch 156/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 1.9839 - acc: 0.5743 - val_loss: 2.0709 - val_acc: 0.5724\n",
      "Epoch 157/200\n",
      "10/10 [==============================] - 4s 425ms/step - loss: 1.9922 - acc: 0.5744 - val_loss: 2.0720 - val_acc: 0.5720\n",
      "Epoch 158/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 1.9945 - acc: 0.5746 - val_loss: 2.0672 - val_acc: 0.5726\n",
      "Epoch 159/200\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 1.9962 - acc: 0.5744 - val_loss: 2.0641 - val_acc: 0.5736\n",
      "Epoch 160/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 1.9911 - acc: 0.5743 - val_loss: 2.0642 - val_acc: 0.5732\n",
      "Epoch 161/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 1.9911 - acc: 0.5742 - val_loss: 2.0655 - val_acc: 0.5730\n",
      "Epoch 162/200\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 1.9887 - acc: 0.5753 - val_loss: 2.0575 - val_acc: 0.5741\n",
      "Epoch 163/200\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 1.9920 - acc: 0.5747 - val_loss: 2.0623 - val_acc: 0.5738\n",
      "Epoch 164/200\n",
      "10/10 [==============================] - 4s 430ms/step - loss: 1.9988 - acc: 0.5749 - val_loss: 2.0630 - val_acc: 0.5734\n",
      "Epoch 165/200\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 1.9825 - acc: 0.5750 - val_loss: 2.0584 - val_acc: 0.5743\n",
      "Epoch 166/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 1.9894 - acc: 0.5755 - val_loss: 2.0601 - val_acc: 0.5738\n",
      "Epoch 167/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 1.9879 - acc: 0.5750 - val_loss: 2.0642 - val_acc: 0.5736\n",
      "Epoch 168/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 1.9852 - acc: 0.5752 - val_loss: 2.0673 - val_acc: 0.5736\n",
      "Epoch 169/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 1.9897 - acc: 0.5755 - val_loss: 2.0684 - val_acc: 0.5741\n",
      "Epoch 170/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 1.9836 - acc: 0.5751 - val_loss: 2.0680 - val_acc: 0.5734\n",
      "Epoch 171/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 2.0053 - acc: 0.5755 - val_loss: 2.0565 - val_acc: 0.5736\n",
      "Epoch 172/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 1.9867 - acc: 0.5755 - val_loss: 2.0450 - val_acc: 0.5741\n",
      "Epoch 173/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 1.9957 - acc: 0.5756 - val_loss: 2.0426 - val_acc: 0.5738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 1.9912 - acc: 0.5754 - val_loss: 2.0417 - val_acc: 0.5743\n",
      "Epoch 175/200\n",
      "10/10 [==============================] - 4s 422ms/step - loss: 1.9867 - acc: 0.5766 - val_loss: 2.0428 - val_acc: 0.5738\n",
      "Epoch 176/200\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 1.9834 - acc: 0.5772 - val_loss: 2.0465 - val_acc: 0.5745\n",
      "Epoch 177/200\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 1.9799 - acc: 0.5763 - val_loss: 2.0510 - val_acc: 0.5751\n",
      "Epoch 178/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 1.9839 - acc: 0.5761 - val_loss: 2.0596 - val_acc: 0.5745\n",
      "Epoch 179/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 1.9782 - acc: 0.5774 - val_loss: 2.0584 - val_acc: 0.5747\n",
      "Epoch 180/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 1.9787 - acc: 0.5769 - val_loss: 2.0584 - val_acc: 0.5745\n",
      "Epoch 181/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 1.9918 - acc: 0.5775 - val_loss: 2.0587 - val_acc: 0.5749\n",
      "Epoch 182/200\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 1.9773 - acc: 0.5765 - val_loss: 2.0589 - val_acc: 0.5753\n",
      "Epoch 183/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 1.9925 - acc: 0.5772 - val_loss: 2.0594 - val_acc: 0.5749\n",
      "Epoch 184/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 1.9773 - acc: 0.5774 - val_loss: 2.0578 - val_acc: 0.5749\n",
      "Epoch 185/200\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 1.9799 - acc: 0.5774 - val_loss: 2.0576 - val_acc: 0.5757\n",
      "Epoch 186/200\n",
      "10/10 [==============================] - 4s 422ms/step - loss: 1.9737 - acc: 0.5770 - val_loss: 2.0593 - val_acc: 0.5755\n",
      "Epoch 187/200\n",
      "10/10 [==============================] - 4s 427ms/step - loss: 1.9757 - acc: 0.5777 - val_loss: 2.0640 - val_acc: 0.5763\n",
      "Epoch 188/200\n",
      "10/10 [==============================] - 4s 437ms/step - loss: 1.9808 - acc: 0.5770 - val_loss: 2.0654 - val_acc: 0.5751\n",
      "Epoch 189/200\n",
      "10/10 [==============================] - 4s 437ms/step - loss: 1.9743 - acc: 0.5778 - val_loss: 2.0595 - val_acc: 0.5755\n",
      "Epoch 190/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 2.0070 - acc: 0.5780 - val_loss: 2.0447 - val_acc: 0.5755\n",
      "Epoch 191/200\n",
      "10/10 [==============================] - 4s 425ms/step - loss: 1.9785 - acc: 0.5782 - val_loss: 2.0438 - val_acc: 0.5749\n",
      "Epoch 192/200\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 1.9881 - acc: 0.5780 - val_loss: 2.0461 - val_acc: 0.5763\n",
      "Epoch 193/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 1.9823 - acc: 0.5783 - val_loss: 2.0472 - val_acc: 0.5763\n",
      "Epoch 194/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 1.9796 - acc: 0.5782 - val_loss: 2.0482 - val_acc: 0.5757\n",
      "Epoch 195/200\n",
      "10/10 [==============================] - 4s 422ms/step - loss: 1.9776 - acc: 0.5782 - val_loss: 2.0473 - val_acc: 0.5751\n",
      "Epoch 196/200\n",
      "10/10 [==============================] - 4s 422ms/step - loss: 1.9774 - acc: 0.5782 - val_loss: 2.0490 - val_acc: 0.5747\n",
      "Epoch 197/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 1.9712 - acc: 0.5780 - val_loss: 2.0493 - val_acc: 0.5757\n",
      "Epoch 198/200\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 1.9860 - acc: 0.5784 - val_loss: 2.0500 - val_acc: 0.5763\n",
      "Epoch 199/200\n",
      "10/10 [==============================] - 4s 422ms/step - loss: 1.9769 - acc: 0.5786 - val_loss: 2.0516 - val_acc: 0.5757\n",
      "Epoch 200/200\n",
      "10/10 [==============================] - 4s 422ms/step - loss: 1.9723 - acc: 0.5779 - val_loss: 2.0561 - val_acc: 0.5753\n",
      "Model: \"trans_race\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Encoder)            multiple                  288976    \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             multiple                  2210      \n",
      "=================================================================\n",
      "Total params: 291,186\n",
      "Trainable params: 291,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "trans_race.compile(\n",
    "    optimizer=opt,\n",
    "    loss=loss,#'categorical_crossentropy', # 'sigmoid_focal_crossentropy'\n",
    "    metrics=['accuracy'], #['categorical_accuracy']\n",
    ")\n",
    "\n",
    "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath='../models/results/transformer.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "history = trans_race.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=200,\n",
    "    verbose=True, # hide the output because we have so many epochs\n",
    "    callbacks=[model_checkpoint_callback]\n",
    ")\n",
    "print(trans_race.summary())\n",
    "# trans_race.save_weights(\"../models/results/transformer.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe5bbf3b208>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot \n",
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "# for loss\n",
    "axL.plot(history.history['loss'],label=\"loss for training\")\n",
    "axL.plot(history.history['val_loss'],label=\"loss for validation\")\n",
    "axL.set_title('model loss')\n",
    "axL.set_xlabel('epoch')\n",
    "axL.set_ylabel('loss')\n",
    "axL.legend(loc='upper right')\n",
    "axR.plot(history.history['acc'],label=\"acc for training\")\n",
    "axR.plot(history.history['val_acc'],label=\"acc for validation\")\n",
    "axR.set_title('model accuracy')\n",
    "axR.set_xlabel('epoch')\n",
    "axR.set_ylabel('accuracy')\n",
    "axR.legend(loc='lower right')\n",
    "# figureの保存\n",
    "# plt.savefig(\"../models/results/trans_race_result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "preds = trans_race.predict(X_valid)\n",
    "y_pred = np.argmax(preds, axis = 2)\n",
    "y_ans = np.argmax(y_valid, axis = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def order_algorithm(preds):\n",
    "#     num_race = preds.shape[0]\n",
    "#     y_preds = np.full((num_race, 24), 25)\n",
    "#     for i in range(num_race):\n",
    "#         one_race = preds[i,:,:]\n",
    "#         init_preds = np.argmax(one_race, axis = -1)\n",
    "#         exist_horse = np.delete(one_race, np.where(init_preds == 25)[0], 0)\n",
    "#         for j in range(1,exist_horse.shape[0]+1):\n",
    "#             one_order = np.argmax(exist_horse[:,j])\n",
    "#             for k in range(one_race.shape[0]):\n",
    "#                 if np.array_equal(one_race[k], exist_horse[one_order]):\n",
    "#                     y_preds[i][k] = j\n",
    "#                     exist_horse = np.delete(exist_horse, one_order, 0)\n",
    "#                     break\n",
    "#     return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202, 24)\n",
      "[ 3  1  6  9  4  2 10  7  5  8 25 25 25 25 25 25 25 25 25 25 25 25 25 25]\n"
     ]
    }
   ],
   "source": [
    "y_preds = order_algorithm(preds)\n",
    "print(y_preds.shape)\n",
    "print(y_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202, 24, 26)\n",
      "(202, 24)\n",
      "(202, 24)\n"
     ]
    }
   ],
   "source": [
    "print(preds.shape)\n",
    "print(y_preds.shape)\n",
    "print(y_ans.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11  4  5 12  8  1  2  9  6 10 25 25 25 25 25 25 25 25 25 25 25 25 25 25]\n",
      "[ 3  1  6  9  4  2 10  7  5  8 25 25 25 25 25 25 25 25 25 25 25 25 25 25]\n",
      "[ 1  1  7 10  1  1  7 10  7 10 25 25 25 25 25 25 25 25 25 25 25 25 25 25]\n"
     ]
    }
   ],
   "source": [
    "print(y_ans[0])\n",
    "print(y_preds[0])\n",
    "print(y_pred[0])\n",
    "# print(np.mean(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  13.841584158415841\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(y_preds.shape[0]):\n",
    "    for j in range(y_preds.shape[1]):\n",
    "        if (y_preds[i][j] == y_ans[i][j]):\n",
    "            correct += 1\n",
    "accuracy = correct / len(y_pred)\n",
    "print(\"accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the distribution of prediction\n",
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "label = ['0','1','2','3','4','5','6','7','8','9',' 10',' 11',' 12',' 13',' 14',' 15',' 16',' 17',' 18',' 19',' 20',' 21',' 22',' 23',' 24']\n",
    "\n",
    "axL.hist(y_preds.flatten(), bins = 25, label = \"pred_order\")##, range = (1,21)\n",
    "axL.set_xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24])\n",
    "axL.set_xticklabels(label)\n",
    "axL.set_title('pred order distribution')\n",
    "axL.legend()\n",
    "axR.hist(y_ans.flatten(), bins = 25, label = \"ans_order\")##, range = (1,21)\n",
    "axR.set_xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24])\n",
    "axR.set_xticklabels(label)\n",
    "axR.set_title('ans order distribution')\n",
    "axR.legend()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (202,24,84) (140,) (202,24,84) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-5f0aedd9dcac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mincrease\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_valid_inv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandard_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mX_valid_inv_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid_inv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0modds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_inv_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'odds'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    935\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_mean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (202,24,84) (140,) (202,24,84) "
     ]
    }
   ],
   "source": [
    "# precision = TP / (TP + FP)\n",
    "# the accuracy of predected True\n",
    "i = 0\n",
    "correct_first = 0\n",
    "all_f = 0\n",
    "increase = 0\n",
    "X_valid_inv = standard_scale.inverse_transform(X_valid)\n",
    "X_valid_inv_df = pd.DataFrame(X_valid_inv)\n",
    "odds = X_test_inv_df['odds'].values\n",
    "hit_odds = []\n",
    "select = []\n",
    "\n",
    "for i in range(len(Y_ans)):\n",
    "    if (pred_order[i] == 1):  # total nubber of predicted 1st (TP + FP)  & (pred[i][1]*odds[i] > 1.0)) | (pred[i][1]> 0.25)\n",
    "        all_f = all_f + 1\n",
    "        if (Y_ans[i] == 1):\n",
    "            correct_first = correct_first + 1   #　True Positive\n",
    "            increase += odds[i]\n",
    "            hit_odds.append(odds[i])\n",
    "            \n",
    "            \n",
    "            \n",
    "print(\"all_f: \", all_f)\n",
    "print(\"correct_first: \", correct_first)\n",
    "print(\"hit odds average: \", np.array(hit_odds).mean())\n",
    "print(\"spent money:\", all_f * 100)\n",
    "revenue = (increase - all_f) * 100\n",
    "retrive = increase / all_f\n",
    " \n",
    "print(\"retrive rate: \", retrive) \n",
    "print(\"revenue: \", revenue)\n",
    "accurate_rate = correct_first / all_f\n",
    "print(\"accuracy: \",accurate_rate)\n",
    "#print(\"\\n\".join(map(str,hit_odds)))\n",
    "print(\"min: \", min(hit_odds))\n",
    "print(\"mid: \", np.median(np.array(hit_odds)))\n",
    "print(\"max: \", max(hit_odds))\n",
    "\n",
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "axL.hist(hit_odds, bins = 50, label = \"hit_odds\")\n",
    "axL.set_title('hit odds distribution')\n",
    "axL.legend()\n",
    "axR.hist(odds, bins = 50, label = \"odds\", range = (0,40))\n",
    "axR.set_title('all odds distribution')\n",
    "axR.legend()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall = TP / (TP + FN)\n",
    "# the accuracy of label True\n",
    "i = 0\n",
    "correct_first = 0\n",
    "all_f = 0\n",
    "odds_f = []\n",
    "p_rate_f = []\n",
    "\n",
    "all_f_odds = []\n",
    "\n",
    "for i in range(len(Y_ans)):\n",
    "    if (Y_ans[i] == 1):  # TP + FN\n",
    "        all_f = all_f + 1\n",
    "        all_f_odds.append(odds[i])\n",
    "        if (pred_order[i] == 1):\n",
    "            correct_first = correct_first + 1   #　TP\n",
    "            odds_f.append(odds[i])\n",
    "            p_rate_f.append(pred[i][1])\n",
    "\n",
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "axL.scatter(p_rate_f, odds_f)  \n",
    "axL.set_title('correlation odss and prediction')\n",
    "#axL.xlabel('prediction rate first')\n",
    "#axL.ylabel('odds')\n",
    "axR.hist(odds_f, bins = 50, label = \"odds\")\n",
    "axR.set_title('all first odds distribution')\n",
    "axR.legend()\n",
    "\n",
    "fig.show()\n",
    "accurate_rate = correct_first / all_f\n",
    "print(\"accuracy: \",accurate_rate)\n",
    "print(\"all_f_odds average: \", np.array(all_f_odds).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
