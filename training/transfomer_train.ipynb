{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pprint\n",
    "# import sys\n",
    "\n",
    "# pprint.pprint(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fundamental libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/mnt/c/Users/44yos/RacePrediction/resnet_win5')\n",
    "from os import path\n",
    "import time\n",
    "\n",
    "# preporcessing libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt \n",
    "from pickle import dump\n",
    "\n",
    "# tesndorflow libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from tensorflow.python.keras import optimizers, callbacks\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "# from models import resnet\n",
    "from models import transformer\n",
    "\n",
    "# from utils import functions\n",
    "from utils import create_time_series_data, smooth_label, categorical_focal_loss, order_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race_id                        int64\n",
      "race_round                     int64\n",
      "ground_condition               int64\n",
      "total_horse_number             int64\n",
      "order                          int64\n",
      "frame_number                   int64\n",
      "horse_number                   int64\n",
      "age                            int64\n",
      "burden_weight                float64\n",
      "goal_time                    float64\n",
      "half_order                   float64\n",
      "last_time                    float64\n",
      "odds                         float64\n",
      "horse_weight                 float64\n",
      "pop                          float64\n",
      "race_rank                      int64\n",
      "distance                       int64\n",
      "ground_type_ダ                  int64\n",
      "ground_type_芝                  int64\n",
      "circle_右                       int64\n",
      "circle_左                       int64\n",
      "weather_circumstance_小雨        int64\n",
      "weather_circumstance_小雪        int64\n",
      "weather_circumstance_晴         int64\n",
      "weather_circumstance_曇         int64\n",
      "weather_circumstance_雨         int64\n",
      "weather_circumstance_雪         int64\n",
      "place_中京                       int64\n",
      "place_中山                       int64\n",
      "place_京都                       int64\n",
      "place_函館                       int64\n",
      "place_小倉                       int64\n",
      "place_新潟                       int64\n",
      "place_札幌                       int64\n",
      "place_東京                       int64\n",
      "place_福島                       int64\n",
      "place_阪神                       int64\n",
      "sex_セ                          int64\n",
      "sex_牝                          int64\n",
      "sex_牡                          int64\n",
      "horse_weight_dif             float64\n",
      "f_grass_win_rate             float64\n",
      "f_dart_win_rate              float64\n",
      "f_win_rate                   float64\n",
      "g_f_grass_win_rate           float64\n",
      "g_f_dart_win_rate            float64\n",
      "g_f_win_rate                 float64\n",
      "m_grass_win_rate             float64\n",
      "m_dart_win_rate              float64\n",
      "m_win_rate                   float64\n",
      "whole_horse_number_1         float64\n",
      "odds_1                       float64\n",
      "order_1                        int64\n",
      "burden_weight_1              float64\n",
      "race_distance_1              float64\n",
      "ground_condition_1           float64\n",
      "goal_time_1                  float64\n",
      "half_order_1                 float64\n",
      "last_time_1                  float64\n",
      "horse_weight_1               float64\n",
      "weather_circumstance_小雨_1    float64\n",
      "weather_circumstance_小雪_1    float64\n",
      "weather_circumstance_晴_1     float64\n",
      "weather_circumstance_曇_1     float64\n",
      "weather_circumstance_雨_1     float64\n",
      "weather_circumstance_雪_1     float64\n",
      "main_place_その他_1             float64\n",
      "main_place_中京_1              float64\n",
      "main_place_中山_1              float64\n",
      "main_place_京都_1              float64\n",
      "main_place_函館_1              float64\n",
      "main_place_小倉_1              float64\n",
      "main_place_新潟_1              float64\n",
      "main_place_札幌_1              float64\n",
      "main_place_東京_1              float64\n",
      "main_place_福島_1              float64\n",
      "main_place_阪神_1              float64\n",
      "race_rank_1                  float64\n",
      "ground_type_ダ_1              float64\n",
      "ground_type_芝_1              float64\n",
      "ground_type_障_1              float64\n",
      "horse_weight_dif_1           float64\n",
      "same_jockey_1                float64\n",
      "whole_horse_number_2         float64\n",
      "odds_2                       float64\n",
      "order_2                        int64\n",
      "burden_weight_2              float64\n",
      "race_distance_2              float64\n",
      "ground_condition_2           float64\n",
      "goal_time_2                  float64\n",
      "half_order_2                 float64\n",
      "last_time_2                  float64\n",
      "horse_weight_2               float64\n",
      "weather_circumstance_小雨_2    float64\n",
      "weather_circumstance_小雪_2    float64\n",
      "weather_circumstance_晴_2     float64\n",
      "weather_circumstance_曇_2     float64\n",
      "weather_circumstance_雨_2     float64\n",
      "weather_circumstance_雪_2     float64\n",
      "main_place_その他_2             float64\n",
      "main_place_中京_2              float64\n",
      "main_place_中山_2              float64\n",
      "main_place_京都_2              float64\n",
      "main_place_函館_2              float64\n",
      "main_place_小倉_2              float64\n",
      "main_place_新潟_2              float64\n",
      "main_place_札幌_2              float64\n",
      "main_place_東京_2              float64\n",
      "main_place_福島_2              float64\n",
      "main_place_阪神_2              float64\n",
      "race_rank_2                  float64\n",
      "ground_type_ダ_2              float64\n",
      "ground_type_芝_2              float64\n",
      "ground_type_障_2              float64\n",
      "horse_weight_dif_2           float64\n",
      "same_jockey_2                float64\n",
      "whole_horse_number_3         float64\n",
      "odds_3                       float64\n",
      "order_3                        int64\n",
      "burden_weight_3              float64\n",
      "race_distance_3              float64\n",
      "ground_condition_3           float64\n",
      "goal_time_3                  float64\n",
      "half_order_3                 float64\n",
      "last_time_3                  float64\n",
      "horse_weight_3               float64\n",
      "weather_circumstance_小雨_3    float64\n",
      "weather_circumstance_小雪_3    float64\n",
      "weather_circumstance_晴_3     float64\n",
      "weather_circumstance_曇_3     float64\n",
      "weather_circumstance_雨_3     float64\n",
      "weather_circumstance_雪_3     float64\n",
      "main_place_その他_3             float64\n",
      "main_place_中京_3              float64\n",
      "main_place_中山_3              float64\n",
      "main_place_京都_3              float64\n",
      "main_place_函館_3              float64\n",
      "main_place_小倉_3              float64\n",
      "main_place_新潟_3              float64\n",
      "main_place_札幌_3              float64\n",
      "main_place_東京_3              float64\n",
      "main_place_福島_3              float64\n",
      "main_place_阪神_3              float64\n",
      "race_rank_3                  float64\n",
      "ground_type_ダ_3              float64\n",
      "ground_type_芝_3              float64\n",
      "ground_type_障_3              float64\n",
      "horse_weight_dif_3           float64\n",
      "same_jockey_3                float64\n",
      "same_jockey                  float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# # load data\n",
    "data = pd.read_csv(\"../data/csv/data.csv\", sep = \",\")\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217032\n"
     ]
    }
   ],
   "source": [
    "data.dropna(inplace=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215967\n"
     ]
    }
   ],
   "source": [
    "data.drop_duplicates(inplace=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adjust columns type\n",
    "data['race_id'] = data['race_id'].astype(str)\n",
    "data['order'] = data['order'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete race day information\n",
    "data.drop([\"horse_number\", \"half_order\", \"goal_time\" ,\"last_time\", \"horse_weight\", \"horse_weight_dif\", \"frame_number\",\"pop\"], axis = 1, inplace=True)\n",
    "# \"race_round\",\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # standarlization \n",
    "# no_scale_data = data[['race_id','order']]\n",
    "# scale_columns = data.drop(['race_id','order'], axis=1).columns.values\n",
    "# standard_scale = StandardScaler()\n",
    "# data = pd.DataFrame(standard_scale.fit_transform(data[scale_columns]))\n",
    "\n",
    "# # PCA\n",
    "# pca = PCA()\n",
    "# data = pd.DataFrame(pca.fit_transform(data))\n",
    "# contrb_rate = pd.DataFrame(pca.explained_variance_ratio_, columns = ['rate'])\n",
    "# sum_rate = 0\n",
    "\n",
    "# #  # to get the colum of the specific contribution rate\n",
    "# # for i in range(len(contrb_rate)):\n",
    "# #     sum_rate += contrb_rate.rate[i]\n",
    "# #     if sum_rate >= 0.9:\n",
    "# #         max_col = i + 1\n",
    "# #         break\n",
    "\n",
    "max_col = 84\n",
    "# # print(max_col)\n",
    "# data = data.loc[:, :max_col-1]\n",
    "# print(data.shape[1])\n",
    "# # print(data.head(5))\n",
    "# # print(len(data), len(no_scale_data))\n",
    "# # print(no_scale_data[no_scale_data['race_id'].isnull()])\n",
    "# data = pd.concat([data, no_scale_data], axis=1)\n",
    "# dump(standard_scale, open(\"standard_scale.pkl\", \"wb\"))\n",
    "# dump(pca, open(\"pca.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sorted(no_scale_data['order'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.shape)\n",
    "# print(data.dtypes)\n",
    "# print(data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_time_series_data(raw_data):\n",
    "#     number_of_race = raw_data.race_id.nunique()\n",
    "#     time_series_data = np.full((number_of_race, 24, max_col), 0.0)#-float('inf')\n",
    "#     label = np.full((number_of_race, 24), 25)\n",
    "#     race_number = 0\n",
    "#     horse_number = 0\n",
    "#     for i in range(len(raw_data)):\n",
    "#         if i == 0:\n",
    "#             label[race_number][horse_number] = float(raw_data.iloc[i].order)\n",
    "#             time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "#             horse_number += 1\n",
    "#             continue\n",
    "#         # add new race\n",
    "#         if data.iloc[i].race_id != data.iloc[i-1].race_id:\n",
    "#             race_number += 1\n",
    "#             horse_number = 0\n",
    "#             label[race_number][horse_number] = float(raw_data.iloc[i].order)\n",
    "#             time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "#             horse_number += 1\n",
    "#         # add new horse to the same race\n",
    "#         else:\n",
    "# #             print(data.iloc[i].race_id ,race_number, horse_number)\n",
    "#             label[race_number][horse_number] = float(raw_data.iloc[i].order)\n",
    "#             time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "#             horse_number += 1\n",
    "#     del raw_data\n",
    "#     return time_series_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20132, 24, 84)\n",
      "(20132, 24)\n"
     ]
    }
   ],
   "source": [
    "# X, y_order = create_time_series_data(data)\n",
    "# np.save('X', X)\n",
    "# np.save('y_order', y_order)\n",
    "X = np.load('X.npy')\n",
    "y_order = np.load('y_order.npy')\n",
    "# del data\n",
    "print(X.shape)\n",
    "print(y_order.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "X = X.astype('float32')\n",
    "print(X.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[ 7 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25]\n"
     ]
    }
   ],
   "source": [
    "print(X[0][11])\n",
    "print(y_order[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26,)\n",
      "[0.00000000e+00 1.21687621e+00 1.21555368e+00 1.23063757e+00\n",
      " 1.24041898e+00 1.25990362e+00 1.26100846e+00 1.28155834e+00\n",
      " 1.32325490e+00 1.37157651e+00 1.45736210e+00 1.56413643e+00\n",
      " 1.73521807e+00 1.98052140e+00 2.27608819e+00 2.71833648e+00\n",
      " 3.58156912e+00 1.49347181e+01 2.00918164e+01 1.25825000e+03\n",
      " 1.43800000e+03 2.51650000e+03 3.35533333e+03 4.02640000e+03\n",
      " 5.03300000e+03 7.53440294e-02]\n"
     ]
    }
   ],
   "source": [
    "alpha = len(y_order) / pd.DataFrame(y_order.flatten()).value_counts()\n",
    "alpha = alpha.sort_index()\n",
    "alpha = np.array(alpha)\n",
    "alpha = np.append(0,alpha)\n",
    "print(alpha.shape)\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]]\n",
      "(20132, 24, 26)\n",
      "(20132, 24, 84)\n"
     ]
    }
   ],
   "source": [
    "# creating X,y (parameters and target)\n",
    "y = np_utils.to_categorical(y_order, dtype='float32')\n",
    "print(y[5])\n",
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def smooth_label(label, factor=0.03):\n",
    "#     # smooth label\n",
    "#     label *= (1 - factor)\n",
    "# #     label[:,:,1:4] += (factor / 3)\n",
    "\n",
    "#     for i in range(label.shape[0]):\n",
    "#         for j in range(label.shape[1]):\n",
    "#             t = np.where(label[i][j] == 1 - factor)\n",
    "#             label[i,j,max(0,t[0][0]-1):min(26,t[0][0]+2)] += (factor / 3)\n",
    "#     return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.   0.   0.   0.01 0.98 0.01 0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.01 0.98 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]]\n",
      "(20132, 24, 26)\n",
      "(20132, 24, 84)\n"
     ]
    }
   ],
   "source": [
    "y = smooth_label(y) \n",
    "print(y[4])\n",
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.race_id.value_counts().plot.hist(bins=25,range=(1,25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(pd.DataFrame(y_order.flatten()).plot.hist(bins=25))## ,ylim=(0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.01, random_state = 0)\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.0841513e+00  7.0771635e-02 -5.8418870e-01  2.4646287e-01\n",
      "  1.8285137e+00 -1.1262701e+00  6.5735537e-01  1.5329328e+00\n",
      "  6.2066919e-01 -6.7069030e-01  1.0873965e+00  1.7795300e-01\n",
      " -2.2035263e+00 -5.5455178e-01  1.5312845e+00  1.9849390e+00\n",
      " -1.9881687e+00 -4.5285341e-01 -1.1557992e+00  3.0845302e-01\n",
      "  6.5695131e-01 -1.6801572e+00  1.2509111e-01 -6.3480353e-01\n",
      "  2.6424465e-01 -1.8350914e+00  1.3959821e+00 -1.1001785e+00\n",
      " -2.4194989e+00  2.9623857e+00  1.4317343e+00  1.9310854e-02\n",
      "  3.4215423e-01  7.4966168e-01 -1.0088371e+00  8.0969352e-01\n",
      " -2.5986239e-01 -8.4669787e-01  1.0779321e+00  6.1363038e-02\n",
      " -1.5148355e+00 -1.8475902e-03 -8.8226789e-01 -6.8742210e-01\n",
      " -2.6793274e-01  1.6574528e+00 -2.0822718e+00 -1.7538213e+00\n",
      "  4.2922177e+00  1.6022534e+00 -4.4454589e-01 -6.7625672e-01\n",
      " -8.8914499e-02 -1.1668312e-01 -3.3394665e-01 -7.5860035e-01\n",
      "  3.7061727e-01 -6.4284933e-01  3.2509527e-01  4.8748016e-01\n",
      "  1.5954223e+00  1.4406800e-01 -8.7014019e-01  6.6753399e-01\n",
      " -1.0578674e+00  4.2397591e-01  6.7731267e-01  3.3324012e-01\n",
      " -1.4028546e+00 -1.2829390e+00 -9.2539579e-01  3.5360447e-01\n",
      " -5.5841304e-02  2.7825090e-01  1.1097189e-01  1.2788044e+00\n",
      "  4.5809287e-01  1.7082896e+00 -6.1947507e-01  1.6214646e+00\n",
      " -8.2937258e-01  6.4784966e-02  9.0706927e-01 -1.0770866e+00]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19930, 24, 84)\n",
      "(202, 24, 84)\n",
      "(19930, 24, 26)\n",
      "(202, 24, 26)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def categorical_focal_loss(alpha, gamma):\n",
    "#     \"\"\"\n",
    "#     Softmax version of focal loss.\n",
    "#     When there is a skew between different categories/labels in your data set, you can try to apply this function as a\n",
    "#     loss.\n",
    "#            m\n",
    "#       FL = ∑  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n",
    "#           c=1\n",
    "#       where m = number of classes, c = class and o = observation\n",
    "#     Parameters:\n",
    "#       alpha -- the same as weighing factor in balanced cross entropy. Alpha is used to specify the weight of different\n",
    "#       categories/labels, the size of the array needs to be consistent with the number of classes.\n",
    "#       gamma -- focusing parameter for modulating factor (1-p)\n",
    "#     Default value:\n",
    "#       gamma -- 2.0 as mentioned in the paper\n",
    "#       alpha -- 0.25 as mentioned in the paper\n",
    "#     References:\n",
    "#         Official paper: https://arxiv.org/pdf/1708.02002.pdf\n",
    "#         https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy\n",
    "#     Usage:\n",
    "#      model.compile(loss=[categorical_focal_loss(alpha=[[.25, .25, .25]], gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "#     \"\"\"\n",
    "\n",
    "#     alpha = np.array(alpha, dtype=np.float32)\n",
    "\n",
    "#     def categorical_focal_loss_fixed(y_true, y_pred):\n",
    "#         \"\"\"\n",
    "#         :param y_true: A tensor of the same shape as `y_pred`\n",
    "#         :param y_pred: A tensor resulting from a softmax\n",
    "#         :return: Output tensor.\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Clip the prediction value to prevent NaN's and Inf's\n",
    "#         epsilon = K.epsilon()\n",
    "#         y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "#         # Calculate Cross Entropy\n",
    "#         cross_entropy = -y_true * K.log(y_pred)\n",
    "\n",
    "#         # Calculate Focal Loss\n",
    "#         loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
    "\n",
    "#         # Compute mean loss in mini_batch\n",
    "#         return K.mean(K.sum(loss, axis=-1))\n",
    "\n",
    "#     return categorical_focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataset with batch size\n",
    "batch_size = 2048 # hyperparameter\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=19930).batch(batch_size)\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "valid_dataset = valid_dataset.shuffle(buffer_size=202).batch(batch_size)\n",
    "\n",
    "\n",
    "del X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters\n",
    "num_layers = 2 # hyperparameter\n",
    "d_model = max_col # 4*35 84=4*3*7\n",
    "num_heads = 28 # hyperparameter *must be a factor of d_model*\n",
    "d_ffn = 256 # hyperparameter\n",
    "pe_input = 24\n",
    "target_size = 26\n",
    "dropout_rate = 0.1 # hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# set up model\n",
    "trans_race = transformer.TransRace(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ffn=d_ffn,\n",
    "    pe_input=pe_input,\n",
    "    target_size=target_size,\n",
    "    rate=dropout_rate,\n",
    ")\n",
    "opt = optimizers.Adam(decay=0.01)\n",
    "loss = categorical_focal_loss(alpha=[alpha], gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method TransRace.call of <models.transformer.TransRace object at 0x7f4c9e12d438>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TransRace.call of <models.transformer.TransRace object at 0x7f4c9e12d438>>, which Python reported as:\n",
      "    def call(self, inputs, training):\n",
      "        inp = inputs\n",
      "\n",
      "        enc_padding_mask = self.create_masks(inp) #, tar\n",
      "        # , look_ahead_mask, dec_padding_mask\n",
      "\n",
      "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
      "#         print(\"enc output shape\", enc_output.shape)\n",
      "\n",
      "        final_output = self.final_layer(enc_output)\n",
      "#         print(\"final output shape\", final_output.shape)\n",
      "        \n",
      "        return final_output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method TransRace.call of <models.transformer.TransRace object at 0x7f4c9e12d438>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TransRace.call of <models.transformer.TransRace object at 0x7f4c9e12d438>>, which Python reported as:\n",
      "    def call(self, inputs, training):\n",
      "        inp = inputs\n",
      "\n",
      "        enc_padding_mask = self.create_masks(inp) #, tar\n",
      "        # , look_ahead_mask, dec_padding_mask\n",
      "\n",
      "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
      "#         print(\"enc output shape\", enc_output.shape)\n",
      "\n",
      "        final_output = self.final_layer(enc_output)\n",
      "#         print(\"final output shape\", final_output.shape)\n",
      "        \n",
      "        return final_output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7f4ca7c8def0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7f4ca7c8def0>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7f4ca7c8def0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7f4ca7c8def0>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7f4ca7c8df28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7f4ca7c8df28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7f4ca7c8df28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7f4ca7c8df28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7f4ca7c8df28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7f4ca7c8df28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7f4ca7c8df28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7f4ca7c8df28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7f4ca7c8df28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7f4ca7c8df28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7f4ca7c8df28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7f4ca7c8df28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7f4ca7ca9f60>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7f4ca7ca9f60>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7f4ca7ca9f60>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7f4ca7ca9f60>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7f4ca7ca9fd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7f4ca7ca9fd0>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7f4ca7ca9fd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7f4ca7ca9fd0>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7f4ca7ca9fd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7f4ca7ca9fd0>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7f4ca7ca9fd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7f4ca7ca9fd0>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7f4ca7ca9fd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7f4ca7ca9fd0>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7f4ca7ca9fd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7f4ca7ca9fd0>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:From /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 10 steps, validate on 1 steps\n",
      "Epoch 1/1000\n",
      "10/10 [==============================] - 10s 980ms/step - loss: 3.6949 - acc: 0.0507 - val_loss: 3.2445 - val_acc: 0.0850\n",
      "Epoch 2/1000\n",
      "10/10 [==============================] - 1s 142ms/step - loss: 3.1018 - acc: 0.1015 - val_loss: 3.2059 - val_acc: 0.1514\n",
      "Epoch 3/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.8509 - acc: 0.1429 - val_loss: 2.7495 - val_acc: 0.2382\n",
      "Epoch 4/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.7937 - acc: 0.1868 - val_loss: 2.6098 - val_acc: 0.2919\n",
      "Epoch 5/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.6919 - acc: 0.2086 - val_loss: 2.6455 - val_acc: 0.3284\n",
      "Epoch 6/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 2.6573 - acc: 0.2459 - val_loss: 2.4722 - val_acc: 0.3430\n",
      "Epoch 7/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.5844 - acc: 0.2538 - val_loss: 2.4136 - val_acc: 0.3581\n",
      "Epoch 8/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.5622 - acc: 0.2842 - val_loss: 2.4023 - val_acc: 0.3876\n",
      "Epoch 9/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.4892 - acc: 0.3051 - val_loss: 2.3783 - val_acc: 0.4049\n",
      "Epoch 10/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.5050 - acc: 0.3325 - val_loss: 2.3939 - val_acc: 0.4243\n",
      "Epoch 11/1000\n",
      "10/10 [==============================] - 1s 140ms/step - loss: 2.4604 - acc: 0.3568 - val_loss: 2.3379 - val_acc: 0.4406\n",
      "Epoch 12/1000\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 2.4761 - acc: 0.3748 - val_loss: 2.3364 - val_acc: 0.4474\n",
      "Epoch 13/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.4155 - acc: 0.3901 - val_loss: 2.3358 - val_acc: 0.4653\n",
      "Epoch 14/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.3995 - acc: 0.4068 - val_loss: 2.3130 - val_acc: 0.4759\n",
      "Epoch 15/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.4097 - acc: 0.4287 - val_loss: 2.3237 - val_acc: 0.4798\n",
      "Epoch 16/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.3801 - acc: 0.4319 - val_loss: 2.2988 - val_acc: 0.4827\n",
      "Epoch 17/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.3151 - acc: 0.4492 - val_loss: 2.2687 - val_acc: 0.4915\n",
      "Epoch 18/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.3259 - acc: 0.4522 - val_loss: 2.2566 - val_acc: 0.5058\n",
      "Epoch 19/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.2970 - acc: 0.4671 - val_loss: 2.2782 - val_acc: 0.5078\n",
      "Epoch 20/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.2942 - acc: 0.4745 - val_loss: 2.2520 - val_acc: 0.5179\n",
      "Epoch 21/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.2729 - acc: 0.4777 - val_loss: 2.2466 - val_acc: 0.5192\n",
      "Epoch 22/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.2652 - acc: 0.4855 - val_loss: 2.2439 - val_acc: 0.5221\n",
      "Epoch 23/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 2.2411 - acc: 0.4907 - val_loss: 2.2289 - val_acc: 0.5252\n",
      "Epoch 24/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.2229 - acc: 0.4950 - val_loss: 2.2200 - val_acc: 0.5227\n",
      "Epoch 25/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.2373 - acc: 0.5008 - val_loss: 2.2222 - val_acc: 0.5270\n",
      "Epoch 26/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.2367 - acc: 0.5064 - val_loss: 2.2170 - val_acc: 0.5281\n",
      "Epoch 27/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.2188 - acc: 0.5097 - val_loss: 2.2065 - val_acc: 0.5324\n",
      "Epoch 28/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.2251 - acc: 0.5121 - val_loss: 2.2050 - val_acc: 0.5330\n",
      "Epoch 29/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.1872 - acc: 0.5146 - val_loss: 2.2119 - val_acc: 0.5361\n",
      "Epoch 30/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.1925 - acc: 0.5173 - val_loss: 2.2092 - val_acc: 0.5344\n",
      "Epoch 31/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.1953 - acc: 0.5215 - val_loss: 2.2039 - val_acc: 0.5384\n",
      "Epoch 32/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.1732 - acc: 0.5225 - val_loss: 2.1982 - val_acc: 0.5386\n",
      "Epoch 33/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.1916 - acc: 0.5261 - val_loss: 2.2081 - val_acc: 0.5398\n",
      "Epoch 34/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.1665 - acc: 0.5274 - val_loss: 2.1993 - val_acc: 0.5413\n",
      "Epoch 35/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.1823 - acc: 0.5287 - val_loss: 2.1866 - val_acc: 0.5400\n",
      "Epoch 36/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.1654 - acc: 0.5312 - val_loss: 2.1850 - val_acc: 0.5468\n",
      "Epoch 37/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.1378 - acc: 0.5343 - val_loss: 2.1939 - val_acc: 0.5462\n",
      "Epoch 38/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.1632 - acc: 0.5360 - val_loss: 2.1827 - val_acc: 0.5458\n",
      "Epoch 39/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.1331 - acc: 0.5355 - val_loss: 2.1931 - val_acc: 0.5458\n",
      "Epoch 40/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.1347 - acc: 0.5373 - val_loss: 2.1948 - val_acc: 0.5476\n",
      "Epoch 41/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.1375 - acc: 0.5388 - val_loss: 2.1842 - val_acc: 0.5468\n",
      "Epoch 42/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.1152 - acc: 0.5405 - val_loss: 2.1780 - val_acc: 0.5497\n",
      "Epoch 43/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.1583 - acc: 0.5431 - val_loss: 2.1839 - val_acc: 0.5497\n",
      "Epoch 44/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.1333 - acc: 0.5426 - val_loss: 2.1883 - val_acc: 0.5493\n",
      "Epoch 45/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.1300 - acc: 0.5436 - val_loss: 2.1764 - val_acc: 0.5534\n",
      "Epoch 46/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 2.1321 - acc: 0.5460 - val_loss: 2.1870 - val_acc: 0.5547\n",
      "Epoch 47/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.1092 - acc: 0.5468 - val_loss: 2.1851 - val_acc: 0.5540\n",
      "Epoch 48/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.1114 - acc: 0.5472 - val_loss: 2.1815 - val_acc: 0.5536\n",
      "Epoch 49/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0984 - acc: 0.5485 - val_loss: 2.1783 - val_acc: 0.5534\n",
      "Epoch 50/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.1071 - acc: 0.5493 - val_loss: 2.1757 - val_acc: 0.5545\n",
      "Epoch 51/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 124ms/step - loss: 2.1144 - acc: 0.5482 - val_loss: 2.1753 - val_acc: 0.5563\n",
      "Epoch 52/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.1316 - acc: 0.5508 - val_loss: 2.1721 - val_acc: 0.5578\n",
      "Epoch 53/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0890 - acc: 0.5505 - val_loss: 2.1666 - val_acc: 0.5571\n",
      "Epoch 54/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 2.1070 - acc: 0.5522 - val_loss: 2.1674 - val_acc: 0.5594\n",
      "Epoch 55/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.1044 - acc: 0.5516 - val_loss: 2.1711 - val_acc: 0.5600\n",
      "Epoch 56/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0951 - acc: 0.5534 - val_loss: 2.1620 - val_acc: 0.5596\n",
      "Epoch 57/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0842 - acc: 0.5539 - val_loss: 2.1545 - val_acc: 0.5582\n",
      "Epoch 58/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0845 - acc: 0.5534 - val_loss: 2.1534 - val_acc: 0.5584\n",
      "Epoch 59/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0941 - acc: 0.5548 - val_loss: 2.1534 - val_acc: 0.5600\n",
      "Epoch 60/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.1034 - acc: 0.5562 - val_loss: 2.1558 - val_acc: 0.5590\n",
      "Epoch 61/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0931 - acc: 0.5559 - val_loss: 2.1528 - val_acc: 0.5590\n",
      "Epoch 62/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0727 - acc: 0.5559 - val_loss: 2.1457 - val_acc: 0.5604\n",
      "Epoch 63/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.0899 - acc: 0.5563 - val_loss: 2.1446 - val_acc: 0.5611\n",
      "Epoch 64/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0890 - acc: 0.5578 - val_loss: 2.1444 - val_acc: 0.5602\n",
      "Epoch 65/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0964 - acc: 0.5584 - val_loss: 2.1453 - val_acc: 0.5606\n",
      "Epoch 66/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0744 - acc: 0.5582 - val_loss: 2.1406 - val_acc: 0.5611\n",
      "Epoch 67/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.0821 - acc: 0.5587 - val_loss: 2.1399 - val_acc: 0.5623\n",
      "Epoch 68/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.0600 - acc: 0.5591 - val_loss: 2.1394 - val_acc: 0.5633\n",
      "Epoch 69/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 2.0913 - acc: 0.5609 - val_loss: 2.1418 - val_acc: 0.5644\n",
      "Epoch 70/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0902 - acc: 0.5601 - val_loss: 2.1408 - val_acc: 0.5637\n",
      "Epoch 71/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0756 - acc: 0.5612 - val_loss: 2.1375 - val_acc: 0.5644\n",
      "Epoch 72/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0769 - acc: 0.5621 - val_loss: 2.1353 - val_acc: 0.5639\n",
      "Epoch 73/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0745 - acc: 0.5612 - val_loss: 2.1299 - val_acc: 0.5642\n",
      "Epoch 74/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0792 - acc: 0.5616 - val_loss: 2.1283 - val_acc: 0.5639\n",
      "Epoch 75/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.1026 - acc: 0.5623 - val_loss: 2.1299 - val_acc: 0.5648\n",
      "Epoch 76/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0558 - acc: 0.5624 - val_loss: 2.1352 - val_acc: 0.5658\n",
      "Epoch 77/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0700 - acc: 0.5632 - val_loss: 2.1371 - val_acc: 0.5658\n",
      "Epoch 78/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0655 - acc: 0.5638 - val_loss: 2.1365 - val_acc: 0.5668\n",
      "Epoch 79/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0481 - acc: 0.5638 - val_loss: 2.1371 - val_acc: 0.5660\n",
      "Epoch 80/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0571 - acc: 0.5632 - val_loss: 2.1368 - val_acc: 0.5668\n",
      "Epoch 81/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.0663 - acc: 0.5651 - val_loss: 2.1359 - val_acc: 0.5672\n",
      "Epoch 82/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0449 - acc: 0.5644 - val_loss: 2.1380 - val_acc: 0.5666\n",
      "Epoch 83/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0601 - acc: 0.5652 - val_loss: 2.1355 - val_acc: 0.5666\n",
      "Epoch 84/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0464 - acc: 0.5652 - val_loss: 2.1346 - val_acc: 0.5670\n",
      "Epoch 85/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0672 - acc: 0.5653 - val_loss: 2.1268 - val_acc: 0.5660\n",
      "Epoch 86/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0477 - acc: 0.5664 - val_loss: 2.1245 - val_acc: 0.5662\n",
      "Epoch 87/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0565 - acc: 0.5658 - val_loss: 2.1239 - val_acc: 0.5662\n",
      "Epoch 88/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.0545 - acc: 0.5666 - val_loss: 2.1216 - val_acc: 0.5677\n",
      "Epoch 89/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0500 - acc: 0.5673 - val_loss: 2.1205 - val_acc: 0.5672\n",
      "Epoch 90/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0514 - acc: 0.5667 - val_loss: 2.1214 - val_acc: 0.5670\n",
      "Epoch 91/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0503 - acc: 0.5668 - val_loss: 2.1213 - val_acc: 0.5679\n",
      "Epoch 92/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.0500 - acc: 0.5673 - val_loss: 2.1223 - val_acc: 0.5681\n",
      "Epoch 93/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 2.0540 - acc: 0.5683 - val_loss: 2.1284 - val_acc: 0.5685\n",
      "Epoch 94/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0450 - acc: 0.5679 - val_loss: 2.1328 - val_acc: 0.5681\n",
      "Epoch 95/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0461 - acc: 0.5675 - val_loss: 2.1323 - val_acc: 0.5687\n",
      "Epoch 96/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0404 - acc: 0.5686 - val_loss: 2.1302 - val_acc: 0.5693\n",
      "Epoch 97/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0579 - acc: 0.5689 - val_loss: 2.1323 - val_acc: 0.5687\n",
      "Epoch 98/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0449 - acc: 0.5683 - val_loss: 2.1299 - val_acc: 0.5697\n",
      "Epoch 99/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0503 - acc: 0.5692 - val_loss: 2.1235 - val_acc: 0.5699\n",
      "Epoch 100/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0464 - acc: 0.5695 - val_loss: 2.1206 - val_acc: 0.5683\n",
      "Epoch 101/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0554 - acc: 0.5692 - val_loss: 2.1241 - val_acc: 0.5695\n",
      "Epoch 102/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0446 - acc: 0.5691 - val_loss: 2.1276 - val_acc: 0.5677\n",
      "Epoch 103/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0474 - acc: 0.5691 - val_loss: 2.1254 - val_acc: 0.5687\n",
      "Epoch 104/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0408 - acc: 0.5702 - val_loss: 2.1267 - val_acc: 0.5683\n",
      "Epoch 105/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0442 - acc: 0.5707 - val_loss: 2.1231 - val_acc: 0.5677\n",
      "Epoch 106/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0377 - acc: 0.5707 - val_loss: 2.1233 - val_acc: 0.5691\n",
      "Epoch 107/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0290 - acc: 0.5707 - val_loss: 2.1223 - val_acc: 0.5685\n",
      "Epoch 108/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0293 - acc: 0.5712 - val_loss: 2.1280 - val_acc: 0.5687\n",
      "Epoch 109/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0249 - acc: 0.5708 - val_loss: 2.1321 - val_acc: 0.5681\n",
      "Epoch 110/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0361 - acc: 0.5703 - val_loss: 2.1392 - val_acc: 0.5693\n",
      "Epoch 111/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0301 - acc: 0.5714 - val_loss: 2.1418 - val_acc: 0.5683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0374 - acc: 0.5713 - val_loss: 2.1428 - val_acc: 0.5691\n",
      "Epoch 113/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0261 - acc: 0.5718 - val_loss: 2.1441 - val_acc: 0.5687\n",
      "Epoch 114/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0232 - acc: 0.5714 - val_loss: 2.1464 - val_acc: 0.5687\n",
      "Epoch 115/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0432 - acc: 0.5720 - val_loss: 2.1493 - val_acc: 0.5683\n",
      "Epoch 116/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0400 - acc: 0.5711 - val_loss: 2.1474 - val_acc: 0.5683\n",
      "Epoch 117/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0318 - acc: 0.5719 - val_loss: 2.1402 - val_acc: 0.5689\n",
      "Epoch 118/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0291 - acc: 0.5728 - val_loss: 2.1337 - val_acc: 0.5699\n",
      "Epoch 119/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0162 - acc: 0.5719 - val_loss: 2.1316 - val_acc: 0.5710\n",
      "Epoch 120/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0357 - acc: 0.5719 - val_loss: 2.1277 - val_acc: 0.5710\n",
      "Epoch 121/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0304 - acc: 0.5719 - val_loss: 2.1296 - val_acc: 0.5714\n",
      "Epoch 122/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0248 - acc: 0.5726 - val_loss: 2.1267 - val_acc: 0.5714\n",
      "Epoch 123/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0299 - acc: 0.5736 - val_loss: 2.1237 - val_acc: 0.5720\n",
      "Epoch 124/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0078 - acc: 0.5735 - val_loss: 2.1270 - val_acc: 0.5718\n",
      "Epoch 125/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0175 - acc: 0.5726 - val_loss: 2.1296 - val_acc: 0.5712\n",
      "Epoch 126/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0178 - acc: 0.5735 - val_loss: 2.1287 - val_acc: 0.5716\n",
      "Epoch 127/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0355 - acc: 0.5734 - val_loss: 2.1271 - val_acc: 0.5712\n",
      "Epoch 128/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0363 - acc: 0.5738 - val_loss: 2.1273 - val_acc: 0.5716\n",
      "Epoch 129/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0143 - acc: 0.5743 - val_loss: 2.1279 - val_acc: 0.5718\n",
      "Epoch 130/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0165 - acc: 0.5739 - val_loss: 2.1273 - val_acc: 0.5714\n",
      "Epoch 131/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0316 - acc: 0.5739 - val_loss: 2.1234 - val_acc: 0.5714\n",
      "Epoch 132/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0229 - acc: 0.5744 - val_loss: 2.1235 - val_acc: 0.5716\n",
      "Epoch 133/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.0167 - acc: 0.5740 - val_loss: 2.1212 - val_acc: 0.5724\n",
      "Epoch 134/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.0161 - acc: 0.5746 - val_loss: 2.1176 - val_acc: 0.5728\n",
      "Epoch 135/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0132 - acc: 0.5746 - val_loss: 2.1141 - val_acc: 0.5716\n",
      "Epoch 136/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0091 - acc: 0.5745 - val_loss: 2.1102 - val_acc: 0.5724\n",
      "Epoch 137/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0170 - acc: 0.5748 - val_loss: 2.1124 - val_acc: 0.5716\n",
      "Epoch 138/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0216 - acc: 0.5750 - val_loss: 2.1163 - val_acc: 0.5730\n",
      "Epoch 139/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0181 - acc: 0.5761 - val_loss: 2.1138 - val_acc: 0.5732\n",
      "Epoch 140/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0168 - acc: 0.5750 - val_loss: 2.1132 - val_acc: 0.5730\n",
      "Epoch 141/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0146 - acc: 0.5751 - val_loss: 2.1169 - val_acc: 0.5732\n",
      "Epoch 142/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0128 - acc: 0.5761 - val_loss: 2.1160 - val_acc: 0.5722\n",
      "Epoch 143/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0190 - acc: 0.5757 - val_loss: 2.1148 - val_acc: 0.5724\n",
      "Epoch 144/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0190 - acc: 0.5767 - val_loss: 2.1142 - val_acc: 0.5720\n",
      "Epoch 145/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0244 - acc: 0.5757 - val_loss: 2.1127 - val_acc: 0.5720\n",
      "Epoch 146/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0245 - acc: 0.5748 - val_loss: 2.1074 - val_acc: 0.5732\n",
      "Epoch 147/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0147 - acc: 0.5769 - val_loss: 2.1050 - val_acc: 0.5730\n",
      "Epoch 148/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0008 - acc: 0.5760 - val_loss: 2.1057 - val_acc: 0.5732\n",
      "Epoch 149/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0052 - acc: 0.5768 - val_loss: 2.1073 - val_acc: 0.5734\n",
      "Epoch 150/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.0363 - acc: 0.5760 - val_loss: 2.1026 - val_acc: 0.5736\n",
      "Epoch 151/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0095 - acc: 0.5760 - val_loss: 2.1064 - val_acc: 0.5732\n",
      "Epoch 152/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0033 - acc: 0.5758 - val_loss: 2.1075 - val_acc: 0.5732\n",
      "Epoch 153/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.0285 - acc: 0.5762 - val_loss: 2.1099 - val_acc: 0.5741\n",
      "Epoch 154/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0183 - acc: 0.5766 - val_loss: 2.1068 - val_acc: 0.5724\n",
      "Epoch 155/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9989 - acc: 0.5763 - val_loss: 2.1023 - val_acc: 0.5726\n",
      "Epoch 156/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0093 - acc: 0.5767 - val_loss: 2.1018 - val_acc: 0.5724\n",
      "Epoch 157/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0083 - acc: 0.5764 - val_loss: 2.1015 - val_acc: 0.5722\n",
      "Epoch 158/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0099 - acc: 0.5768 - val_loss: 2.1017 - val_acc: 0.5726\n",
      "Epoch 159/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0242 - acc: 0.5771 - val_loss: 2.1001 - val_acc: 0.5724\n",
      "Epoch 160/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0072 - acc: 0.5767 - val_loss: 2.0994 - val_acc: 0.5730\n",
      "Epoch 161/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0168 - acc: 0.5771 - val_loss: 2.0979 - val_acc: 0.5724\n",
      "Epoch 162/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0003 - acc: 0.5771 - val_loss: 2.0953 - val_acc: 0.5722\n",
      "Epoch 163/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0047 - acc: 0.5772 - val_loss: 2.0902 - val_acc: 0.5728\n",
      "Epoch 164/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0154 - acc: 0.5778 - val_loss: 2.0877 - val_acc: 0.5722\n",
      "Epoch 165/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0068 - acc: 0.5768 - val_loss: 2.0882 - val_acc: 0.5728\n",
      "Epoch 166/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0099 - acc: 0.5774 - val_loss: 2.0896 - val_acc: 0.5726\n",
      "Epoch 167/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9942 - acc: 0.5770 - val_loss: 2.0913 - val_acc: 0.5741\n",
      "Epoch 168/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0211 - acc: 0.5778 - val_loss: 2.0902 - val_acc: 0.5741\n",
      "Epoch 169/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0087 - acc: 0.5776 - val_loss: 2.0888 - val_acc: 0.5738\n",
      "Epoch 170/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9999 - acc: 0.5773 - val_loss: 2.0884 - val_acc: 0.5738\n",
      "Epoch 171/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9958 - acc: 0.5785 - val_loss: 2.0892 - val_acc: 0.5741\n",
      "Epoch 172/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9962 - acc: 0.5780 - val_loss: 2.0896 - val_acc: 0.5745\n",
      "Epoch 173/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0208 - acc: 0.5783 - val_loss: 2.0887 - val_acc: 0.5730\n",
      "Epoch 174/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0157 - acc: 0.5781 - val_loss: 2.0891 - val_acc: 0.5741\n",
      "Epoch 175/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0062 - acc: 0.5785 - val_loss: 2.0883 - val_acc: 0.5736\n",
      "Epoch 176/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0124 - acc: 0.5776 - val_loss: 2.0895 - val_acc: 0.5732\n",
      "Epoch 177/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0027 - acc: 0.5782 - val_loss: 2.0889 - val_acc: 0.5741\n",
      "Epoch 178/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0088 - acc: 0.5785 - val_loss: 2.0886 - val_acc: 0.5745\n",
      "Epoch 179/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9944 - acc: 0.5785 - val_loss: 2.0861 - val_acc: 0.5745\n",
      "Epoch 180/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0024 - acc: 0.5791 - val_loss: 2.0865 - val_acc: 0.5743\n",
      "Epoch 181/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9961 - acc: 0.5785 - val_loss: 2.0845 - val_acc: 0.5749\n",
      "Epoch 182/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0050 - acc: 0.5789 - val_loss: 2.0861 - val_acc: 0.5745\n",
      "Epoch 183/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9930 - acc: 0.5790 - val_loss: 2.0867 - val_acc: 0.5745\n",
      "Epoch 184/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9913 - acc: 0.5794 - val_loss: 2.0872 - val_acc: 0.5745\n",
      "Epoch 185/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9995 - acc: 0.5792 - val_loss: 2.0865 - val_acc: 0.5747\n",
      "Epoch 186/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0012 - acc: 0.5796 - val_loss: 2.0825 - val_acc: 0.5749\n",
      "Epoch 187/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 1.9886 - acc: 0.5791 - val_loss: 2.0792 - val_acc: 0.5751\n",
      "Epoch 188/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0091 - acc: 0.5796 - val_loss: 2.0772 - val_acc: 0.5749\n",
      "Epoch 189/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9957 - acc: 0.5792 - val_loss: 2.0767 - val_acc: 0.5747\n",
      "Epoch 190/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9911 - acc: 0.5795 - val_loss: 2.0772 - val_acc: 0.5751\n",
      "Epoch 191/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0008 - acc: 0.5796 - val_loss: 2.0761 - val_acc: 0.5759\n",
      "Epoch 192/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0019 - acc: 0.5799 - val_loss: 2.0767 - val_acc: 0.5759\n",
      "Epoch 193/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9972 - acc: 0.5787 - val_loss: 2.0799 - val_acc: 0.5757\n",
      "Epoch 194/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0202 - acc: 0.5797 - val_loss: 2.0812 - val_acc: 0.5757\n",
      "Epoch 195/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9950 - acc: 0.5799 - val_loss: 2.0829 - val_acc: 0.5753\n",
      "Epoch 196/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9879 - acc: 0.5795 - val_loss: 2.0822 - val_acc: 0.5743\n",
      "Epoch 197/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9954 - acc: 0.5798 - val_loss: 2.0816 - val_acc: 0.5751\n",
      "Epoch 198/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9882 - acc: 0.5793 - val_loss: 2.0817 - val_acc: 0.5753\n",
      "Epoch 199/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9971 - acc: 0.5800 - val_loss: 2.0812 - val_acc: 0.5753\n",
      "Epoch 200/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9937 - acc: 0.5795 - val_loss: 2.0840 - val_acc: 0.5753\n",
      "Epoch 201/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9933 - acc: 0.5800 - val_loss: 2.0851 - val_acc: 0.5753\n",
      "Epoch 202/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.0022 - acc: 0.5795 - val_loss: 2.0853 - val_acc: 0.5761\n",
      "Epoch 203/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9901 - acc: 0.5795 - val_loss: 2.0856 - val_acc: 0.5759\n",
      "Epoch 204/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0081 - acc: 0.5808 - val_loss: 2.0834 - val_acc: 0.5761\n",
      "Epoch 205/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9885 - acc: 0.5807 - val_loss: 2.0827 - val_acc: 0.5755\n",
      "Epoch 206/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9902 - acc: 0.5809 - val_loss: 2.0836 - val_acc: 0.5761\n",
      "Epoch 207/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9979 - acc: 0.5802 - val_loss: 2.0823 - val_acc: 0.5763\n",
      "Epoch 208/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9966 - acc: 0.5807 - val_loss: 2.0819 - val_acc: 0.5767\n",
      "Epoch 209/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9936 - acc: 0.5803 - val_loss: 2.0831 - val_acc: 0.5767\n",
      "Epoch 210/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0061 - acc: 0.5802 - val_loss: 2.0830 - val_acc: 0.5767\n",
      "Epoch 211/1000\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 1.9955 - acc: 0.5806 - val_loss: 2.0816 - val_acc: 0.5774\n",
      "Epoch 212/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9966 - acc: 0.5807 - val_loss: 2.0805 - val_acc: 0.5761\n",
      "Epoch 213/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9913 - acc: 0.5814 - val_loss: 2.0784 - val_acc: 0.5763\n",
      "Epoch 214/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9939 - acc: 0.5812 - val_loss: 2.0772 - val_acc: 0.5765\n",
      "Epoch 215/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9912 - acc: 0.5804 - val_loss: 2.0772 - val_acc: 0.5761\n",
      "Epoch 216/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0170 - acc: 0.5810 - val_loss: 2.0768 - val_acc: 0.5763\n",
      "Epoch 217/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0015 - acc: 0.5811 - val_loss: 2.0752 - val_acc: 0.5769\n",
      "Epoch 218/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9904 - acc: 0.5811 - val_loss: 2.0751 - val_acc: 0.5771\n",
      "Epoch 219/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9935 - acc: 0.5805 - val_loss: 2.0760 - val_acc: 0.5774\n",
      "Epoch 220/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9793 - acc: 0.5810 - val_loss: 2.0774 - val_acc: 0.5767\n",
      "Epoch 221/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9971 - acc: 0.5808 - val_loss: 2.0786 - val_acc: 0.5765\n",
      "Epoch 222/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9970 - acc: 0.5811 - val_loss: 2.0781 - val_acc: 0.5771\n",
      "Epoch 223/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9926 - acc: 0.5816 - val_loss: 2.0770 - val_acc: 0.5767\n",
      "Epoch 224/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0084 - acc: 0.5816 - val_loss: 2.0746 - val_acc: 0.5784\n",
      "Epoch 225/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9891 - acc: 0.5812 - val_loss: 2.0745 - val_acc: 0.5776\n",
      "Epoch 226/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9768 - acc: 0.5812 - val_loss: 2.0762 - val_acc: 0.5776\n",
      "Epoch 227/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9884 - acc: 0.5817 - val_loss: 2.0798 - val_acc: 0.5771\n",
      "Epoch 228/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0037 - acc: 0.5815 - val_loss: 2.0790 - val_acc: 0.5782\n",
      "Epoch 229/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9882 - acc: 0.5818 - val_loss: 2.0780 - val_acc: 0.5782\n",
      "Epoch 230/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9926 - acc: 0.5823 - val_loss: 2.0758 - val_acc: 0.5776\n",
      "Epoch 231/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9972 - acc: 0.5813 - val_loss: 2.0776 - val_acc: 0.5778\n",
      "Epoch 232/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0013 - acc: 0.5811 - val_loss: 2.0756 - val_acc: 0.5774\n",
      "Epoch 233/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9907 - acc: 0.5818 - val_loss: 2.0741 - val_acc: 0.5776\n",
      "Epoch 234/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9866 - acc: 0.5826 - val_loss: 2.0732 - val_acc: 0.5771\n",
      "Epoch 235/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9877 - acc: 0.5814 - val_loss: 2.0737 - val_acc: 0.5771\n",
      "Epoch 236/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9889 - acc: 0.5813 - val_loss: 2.0724 - val_acc: 0.5769\n",
      "Epoch 237/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9824 - acc: 0.5819 - val_loss: 2.0703 - val_acc: 0.5771\n",
      "Epoch 238/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9836 - acc: 0.5815 - val_loss: 2.0693 - val_acc: 0.5776\n",
      "Epoch 239/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0078 - acc: 0.5828 - val_loss: 2.0687 - val_acc: 0.5790\n",
      "Epoch 240/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9825 - acc: 0.5819 - val_loss: 2.0688 - val_acc: 0.5784\n",
      "Epoch 241/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9806 - acc: 0.5822 - val_loss: 2.0696 - val_acc: 0.5786\n",
      "Epoch 242/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9796 - acc: 0.5826 - val_loss: 2.0707 - val_acc: 0.5780\n",
      "Epoch 243/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9811 - acc: 0.5820 - val_loss: 2.0725 - val_acc: 0.5780\n",
      "Epoch 244/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9753 - acc: 0.5822 - val_loss: 2.0759 - val_acc: 0.5771\n",
      "Epoch 245/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9819 - acc: 0.5825 - val_loss: 2.0792 - val_acc: 0.5778\n",
      "Epoch 246/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9804 - acc: 0.5819 - val_loss: 2.0800 - val_acc: 0.5771\n",
      "Epoch 247/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9793 - acc: 0.5824 - val_loss: 2.0802 - val_acc: 0.5780\n",
      "Epoch 248/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9822 - acc: 0.5821 - val_loss: 2.0788 - val_acc: 0.5782\n",
      "Epoch 249/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9742 - acc: 0.5824 - val_loss: 2.0783 - val_acc: 0.5771\n",
      "Epoch 250/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9781 - acc: 0.5828 - val_loss: 2.0775 - val_acc: 0.5780\n",
      "Epoch 251/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9778 - acc: 0.5824 - val_loss: 2.0769 - val_acc: 0.5776\n",
      "Epoch 252/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9852 - acc: 0.5829 - val_loss: 2.0799 - val_acc: 0.5771\n",
      "Epoch 253/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9864 - acc: 0.5824 - val_loss: 2.0822 - val_acc: 0.5778\n",
      "Epoch 254/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9755 - acc: 0.5831 - val_loss: 2.0835 - val_acc: 0.5784\n",
      "Epoch 255/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9823 - acc: 0.5828 - val_loss: 2.0811 - val_acc: 0.5788\n",
      "Epoch 256/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9763 - acc: 0.5832 - val_loss: 2.0790 - val_acc: 0.5788\n",
      "Epoch 257/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9844 - acc: 0.5828 - val_loss: 2.0777 - val_acc: 0.5786\n",
      "Epoch 258/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9823 - acc: 0.5823 - val_loss: 2.0798 - val_acc: 0.5776\n",
      "Epoch 259/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9894 - acc: 0.5824 - val_loss: 2.0809 - val_acc: 0.5786\n",
      "Epoch 260/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9846 - acc: 0.5835 - val_loss: 2.0850 - val_acc: 0.5792\n",
      "Epoch 261/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9902 - acc: 0.5834 - val_loss: 2.0821 - val_acc: 0.5800\n",
      "Epoch 262/1000\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 1.9784 - acc: 0.5830 - val_loss: 2.0799 - val_acc: 0.5804\n",
      "Epoch 263/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9786 - acc: 0.5833 - val_loss: 2.0783 - val_acc: 0.5798\n",
      "Epoch 264/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9732 - acc: 0.5833 - val_loss: 2.0799 - val_acc: 0.5792\n",
      "Epoch 265/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9805 - acc: 0.5822 - val_loss: 2.0820 - val_acc: 0.5794\n",
      "Epoch 266/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9790 - acc: 0.5834 - val_loss: 2.0847 - val_acc: 0.5796\n",
      "Epoch 267/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9725 - acc: 0.5834 - val_loss: 2.0875 - val_acc: 0.5798\n",
      "Epoch 268/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9731 - acc: 0.5841 - val_loss: 2.0873 - val_acc: 0.5792\n",
      "Epoch 269/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9783 - acc: 0.5831 - val_loss: 2.0872 - val_acc: 0.5792\n",
      "Epoch 270/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9746 - acc: 0.5840 - val_loss: 2.0847 - val_acc: 0.5794\n",
      "Epoch 271/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9708 - acc: 0.5830 - val_loss: 2.0790 - val_acc: 0.5800\n",
      "Epoch 272/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9786 - acc: 0.5835 - val_loss: 2.0771 - val_acc: 0.5800\n",
      "Epoch 273/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9781 - acc: 0.5835 - val_loss: 2.0777 - val_acc: 0.5798\n",
      "Epoch 274/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9761 - acc: 0.5831 - val_loss: 2.0793 - val_acc: 0.5796\n",
      "Epoch 275/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9973 - acc: 0.5830 - val_loss: 2.0810 - val_acc: 0.5792\n",
      "Epoch 276/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9755 - acc: 0.5833 - val_loss: 2.0811 - val_acc: 0.5792\n",
      "Epoch 277/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9863 - acc: 0.5837 - val_loss: 2.0808 - val_acc: 0.5796\n",
      "Epoch 278/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9784 - acc: 0.5829 - val_loss: 2.0806 - val_acc: 0.5802\n",
      "Epoch 279/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9831 - acc: 0.5833 - val_loss: 2.0819 - val_acc: 0.5798\n",
      "Epoch 280/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9760 - acc: 0.5840 - val_loss: 2.0808 - val_acc: 0.5802\n",
      "Epoch 281/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9770 - acc: 0.5842 - val_loss: 2.0776 - val_acc: 0.5811\n",
      "Epoch 282/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9728 - acc: 0.5840 - val_loss: 2.0771 - val_acc: 0.5802\n",
      "Epoch 283/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9724 - acc: 0.5840 - val_loss: 2.0772 - val_acc: 0.5800\n",
      "Epoch 284/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9739 - acc: 0.5834 - val_loss: 2.0782 - val_acc: 0.5802\n",
      "Epoch 285/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9770 - acc: 0.5838 - val_loss: 2.0792 - val_acc: 0.5804\n",
      "Epoch 286/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9887 - acc: 0.5839 - val_loss: 2.0811 - val_acc: 0.5802\n",
      "Epoch 287/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9811 - acc: 0.5837 - val_loss: 2.0809 - val_acc: 0.5802\n",
      "Epoch 288/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9750 - acc: 0.5835 - val_loss: 2.0808 - val_acc: 0.5796\n",
      "Epoch 289/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9686 - acc: 0.5837 - val_loss: 2.0820 - val_acc: 0.5804\n",
      "Epoch 290/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9789 - acc: 0.5831 - val_loss: 2.0845 - val_acc: 0.5802\n",
      "Epoch 291/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9768 - acc: 0.5838 - val_loss: 2.0846 - val_acc: 0.5804\n",
      "Epoch 292/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9775 - acc: 0.5832 - val_loss: 2.0851 - val_acc: 0.5802\n",
      "Epoch 293/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9789 - acc: 0.5842 - val_loss: 2.0834 - val_acc: 0.5804\n",
      "Epoch 294/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9700 - acc: 0.5841 - val_loss: 2.0834 - val_acc: 0.5802\n",
      "Epoch 295/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9776 - acc: 0.5839 - val_loss: 2.0828 - val_acc: 0.5809\n",
      "Epoch 296/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 1.9672 - acc: 0.5840 - val_loss: 2.0799 - val_acc: 0.5817\n",
      "Epoch 297/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9709 - acc: 0.5837 - val_loss: 2.0797 - val_acc: 0.5817\n",
      "Epoch 298/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9870 - acc: 0.5844 - val_loss: 2.0787 - val_acc: 0.5807\n",
      "Epoch 299/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9694 - acc: 0.5836 - val_loss: 2.0787 - val_acc: 0.5804\n",
      "Epoch 300/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9672 - acc: 0.5846 - val_loss: 2.0799 - val_acc: 0.5804\n",
      "Epoch 301/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9722 - acc: 0.5842 - val_loss: 2.0785 - val_acc: 0.5811\n",
      "Epoch 302/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9810 - acc: 0.5844 - val_loss: 2.0757 - val_acc: 0.5811\n",
      "Epoch 303/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9690 - acc: 0.5838 - val_loss: 2.0746 - val_acc: 0.5809\n",
      "Epoch 304/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9727 - acc: 0.5847 - val_loss: 2.0743 - val_acc: 0.5807\n",
      "Epoch 305/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9819 - acc: 0.5843 - val_loss: 2.0753 - val_acc: 0.5813\n",
      "Epoch 306/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9689 - acc: 0.5841 - val_loss: 2.0770 - val_acc: 0.5817\n",
      "Epoch 307/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9799 - acc: 0.5843 - val_loss: 2.0778 - val_acc: 0.5817\n",
      "Epoch 308/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9710 - acc: 0.5849 - val_loss: 2.0795 - val_acc: 0.5813\n",
      "Epoch 309/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9897 - acc: 0.5838 - val_loss: 2.0808 - val_acc: 0.5821\n",
      "Epoch 310/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9699 - acc: 0.5841 - val_loss: 2.0825 - val_acc: 0.5823\n",
      "Epoch 311/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9659 - acc: 0.5842 - val_loss: 2.0839 - val_acc: 0.5819\n",
      "Epoch 312/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9843 - acc: 0.5848 - val_loss: 2.0833 - val_acc: 0.5821\n",
      "Epoch 313/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9792 - acc: 0.5845 - val_loss: 2.0848 - val_acc: 0.5823\n",
      "Epoch 314/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9716 - acc: 0.5843 - val_loss: 2.0845 - val_acc: 0.5821\n",
      "Epoch 315/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9827 - acc: 0.5843 - val_loss: 2.0844 - val_acc: 0.5823\n",
      "Epoch 316/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9808 - acc: 0.5846 - val_loss: 2.0834 - val_acc: 0.5821\n",
      "Epoch 317/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9710 - acc: 0.5849 - val_loss: 2.0817 - val_acc: 0.5823\n",
      "Epoch 318/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9719 - acc: 0.5854 - val_loss: 2.0793 - val_acc: 0.5821\n",
      "Epoch 319/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9704 - acc: 0.5846 - val_loss: 2.0788 - val_acc: 0.5817\n",
      "Epoch 320/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9640 - acc: 0.5846 - val_loss: 2.0780 - val_acc: 0.5821\n",
      "Epoch 321/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9693 - acc: 0.5848 - val_loss: 2.0782 - val_acc: 0.5817\n",
      "Epoch 322/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9868 - acc: 0.5852 - val_loss: 2.0818 - val_acc: 0.5819\n",
      "Epoch 323/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9696 - acc: 0.5852 - val_loss: 2.0842 - val_acc: 0.5821\n",
      "Epoch 324/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9674 - acc: 0.5853 - val_loss: 2.0844 - val_acc: 0.5821\n",
      "Epoch 325/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9771 - acc: 0.5853 - val_loss: 2.0842 - val_acc: 0.5823\n",
      "Epoch 326/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9672 - acc: 0.5852 - val_loss: 2.0826 - val_acc: 0.5823\n",
      "Epoch 327/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9677 - acc: 0.5853 - val_loss: 2.0807 - val_acc: 0.5827\n",
      "Epoch 328/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9649 - acc: 0.5849 - val_loss: 2.0790 - val_acc: 0.5815\n",
      "Epoch 329/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9721 - acc: 0.5852 - val_loss: 2.0776 - val_acc: 0.5827\n",
      "Epoch 330/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9723 - acc: 0.5851 - val_loss: 2.0754 - val_acc: 0.5813\n",
      "Epoch 331/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9688 - acc: 0.5856 - val_loss: 2.0730 - val_acc: 0.5817\n",
      "Epoch 332/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9640 - acc: 0.5848 - val_loss: 2.0728 - val_acc: 0.5815\n",
      "Epoch 333/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9701 - acc: 0.5849 - val_loss: 2.0711 - val_acc: 0.5817\n",
      "Epoch 334/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9740 - acc: 0.5851 - val_loss: 2.0707 - val_acc: 0.5813\n",
      "Epoch 335/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9662 - acc: 0.5852 - val_loss: 2.0718 - val_acc: 0.5821\n",
      "Epoch 336/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9609 - acc: 0.5860 - val_loss: 2.0726 - val_acc: 0.5825\n",
      "Epoch 337/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9672 - acc: 0.5859 - val_loss: 2.0734 - val_acc: 0.5831\n",
      "Epoch 338/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9687 - acc: 0.5850 - val_loss: 2.0747 - val_acc: 0.5831\n",
      "Epoch 339/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9686 - acc: 0.5862 - val_loss: 2.0753 - val_acc: 0.5835\n",
      "Epoch 340/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9640 - acc: 0.5853 - val_loss: 2.0757 - val_acc: 0.5825\n",
      "Epoch 341/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9914 - acc: 0.5850 - val_loss: 2.0754 - val_acc: 0.5815\n",
      "Epoch 342/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9715 - acc: 0.5855 - val_loss: 2.0766 - val_acc: 0.5825\n",
      "Epoch 343/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0088 - acc: 0.5853 - val_loss: 2.0768 - val_acc: 0.5821\n",
      "Epoch 344/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9758 - acc: 0.5855 - val_loss: 2.0763 - val_acc: 0.5829\n",
      "Epoch 345/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0010 - acc: 0.5859 - val_loss: 2.0778 - val_acc: 0.5829\n",
      "Epoch 346/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9638 - acc: 0.5860 - val_loss: 2.0783 - val_acc: 0.5827\n",
      "Epoch 347/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9767 - acc: 0.5854 - val_loss: 2.0782 - val_acc: 0.5823\n",
      "Epoch 348/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9734 - acc: 0.5862 - val_loss: 2.0777 - val_acc: 0.5827\n",
      "Epoch 349/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 1.9688 - acc: 0.5854 - val_loss: 2.0784 - val_acc: 0.5829\n",
      "Epoch 350/1000\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 1.9646 - acc: 0.5856 - val_loss: 2.0789 - val_acc: 0.5827\n",
      "Epoch 351/1000\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 1.9835 - acc: 0.5855 - val_loss: 2.0785 - val_acc: 0.5829\n",
      "Epoch 352/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 130ms/step - loss: 1.9696 - acc: 0.5853 - val_loss: 2.0761 - val_acc: 0.5831\n",
      "Epoch 353/1000\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 1.9819 - acc: 0.5855 - val_loss: 2.0745 - val_acc: 0.5829\n",
      "Epoch 354/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9606 - acc: 0.5857 - val_loss: 2.0748 - val_acc: 0.5827\n",
      "Epoch 355/1000\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 1.9658 - acc: 0.5852 - val_loss: 2.0754 - val_acc: 0.5833\n",
      "Epoch 356/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9660 - acc: 0.5856 - val_loss: 2.0744 - val_acc: 0.5831\n",
      "Epoch 357/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 1.9742 - acc: 0.5858 - val_loss: 2.0733 - val_acc: 0.5835\n",
      "Epoch 358/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9682 - acc: 0.5859 - val_loss: 2.0735 - val_acc: 0.5825\n",
      "Epoch 359/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9613 - acc: 0.5862 - val_loss: 2.0720 - val_acc: 0.5827\n",
      "Epoch 360/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9640 - acc: 0.5849 - val_loss: 2.0715 - val_acc: 0.5825\n",
      "Epoch 361/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9730 - acc: 0.5860 - val_loss: 2.0664 - val_acc: 0.5821\n",
      "Epoch 362/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9650 - acc: 0.5859 - val_loss: 2.0647 - val_acc: 0.5829\n",
      "Epoch 363/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9635 - acc: 0.5864 - val_loss: 2.0642 - val_acc: 0.5825\n",
      "Epoch 364/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9633 - acc: 0.5864 - val_loss: 2.0642 - val_acc: 0.5825\n",
      "Epoch 365/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9843 - acc: 0.5861 - val_loss: 2.0644 - val_acc: 0.5821\n",
      "Epoch 366/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9752 - acc: 0.5861 - val_loss: 2.0659 - val_acc: 0.5821\n",
      "Epoch 367/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9640 - acc: 0.5860 - val_loss: 2.0676 - val_acc: 0.5825\n",
      "Epoch 368/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9711 - acc: 0.5867 - val_loss: 2.0673 - val_acc: 0.5829\n",
      "Epoch 369/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9637 - acc: 0.5865 - val_loss: 2.0675 - val_acc: 0.5833\n",
      "Epoch 370/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9602 - acc: 0.5856 - val_loss: 2.0667 - val_acc: 0.5833\n",
      "Epoch 371/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9640 - acc: 0.5861 - val_loss: 2.0673 - val_acc: 0.5831\n",
      "Epoch 372/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9630 - acc: 0.5859 - val_loss: 2.0664 - val_acc: 0.5831\n",
      "Epoch 373/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9588 - acc: 0.5860 - val_loss: 2.0658 - val_acc: 0.5829\n",
      "Epoch 374/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9743 - acc: 0.5859 - val_loss: 2.0648 - val_acc: 0.5825\n",
      "Epoch 375/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9777 - acc: 0.5867 - val_loss: 2.0633 - val_acc: 0.5827\n",
      "Epoch 376/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9686 - acc: 0.5864 - val_loss: 2.0637 - val_acc: 0.5837\n",
      "Epoch 377/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9661 - acc: 0.5868 - val_loss: 2.0624 - val_acc: 0.5829\n",
      "Epoch 378/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9626 - acc: 0.5861 - val_loss: 2.0608 - val_acc: 0.5829\n",
      "Epoch 379/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9649 - acc: 0.5861 - val_loss: 2.0604 - val_acc: 0.5831\n",
      "Epoch 380/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9687 - acc: 0.5862 - val_loss: 2.0615 - val_acc: 0.5831\n",
      "Epoch 381/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9781 - acc: 0.5864 - val_loss: 2.0626 - val_acc: 0.5837\n",
      "Epoch 382/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9659 - acc: 0.5862 - val_loss: 2.0648 - val_acc: 0.5835\n",
      "Epoch 383/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9683 - acc: 0.5862 - val_loss: 2.0650 - val_acc: 0.5831\n",
      "Epoch 384/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9749 - acc: 0.5859 - val_loss: 2.0616 - val_acc: 0.5829\n",
      "Epoch 385/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9766 - acc: 0.5865 - val_loss: 2.0597 - val_acc: 0.5825\n",
      "Epoch 386/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9640 - acc: 0.5862 - val_loss: 2.0597 - val_acc: 0.5825\n",
      "Epoch 387/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9680 - acc: 0.5865 - val_loss: 2.0612 - val_acc: 0.5827\n",
      "Epoch 388/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9579 - acc: 0.5863 - val_loss: 2.0620 - val_acc: 0.5833\n",
      "Epoch 389/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9655 - acc: 0.5860 - val_loss: 2.0617 - val_acc: 0.5831\n",
      "Epoch 390/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9615 - acc: 0.5858 - val_loss: 2.0614 - val_acc: 0.5825\n",
      "Epoch 391/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9690 - acc: 0.5868 - val_loss: 2.0634 - val_acc: 0.5835\n",
      "Epoch 392/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9582 - acc: 0.5868 - val_loss: 2.0651 - val_acc: 0.5829\n",
      "Epoch 393/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9570 - acc: 0.5865 - val_loss: 2.0660 - val_acc: 0.5833\n",
      "Epoch 394/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9611 - acc: 0.5862 - val_loss: 2.0659 - val_acc: 0.5831\n",
      "Epoch 395/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9566 - acc: 0.5869 - val_loss: 2.0640 - val_acc: 0.5827\n",
      "Epoch 396/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9616 - acc: 0.5863 - val_loss: 2.0631 - val_acc: 0.5833\n",
      "Epoch 397/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9602 - acc: 0.5869 - val_loss: 2.0633 - val_acc: 0.5831\n",
      "Epoch 398/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9591 - acc: 0.5869 - val_loss: 2.0641 - val_acc: 0.5837\n",
      "Epoch 399/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9675 - acc: 0.5867 - val_loss: 2.0651 - val_acc: 0.5831\n",
      "Epoch 400/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 1.9652 - acc: 0.5866 - val_loss: 2.0666 - val_acc: 0.5840\n",
      "Epoch 401/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9715 - acc: 0.5868 - val_loss: 2.0665 - val_acc: 0.5831\n",
      "Epoch 402/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9580 - acc: 0.5868 - val_loss: 2.0658 - val_acc: 0.5831\n",
      "Epoch 403/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9714 - acc: 0.5869 - val_loss: 2.0660 - val_acc: 0.5831\n",
      "Epoch 404/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9597 - acc: 0.5867 - val_loss: 2.0650 - val_acc: 0.5829\n",
      "Epoch 405/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9665 - acc: 0.5866 - val_loss: 2.0643 - val_acc: 0.5829\n",
      "Epoch 406/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9685 - acc: 0.5871 - val_loss: 2.0620 - val_acc: 0.5827\n",
      "Epoch 407/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9619 - acc: 0.5870 - val_loss: 2.0612 - val_acc: 0.5831\n",
      "Epoch 408/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9623 - acc: 0.5871 - val_loss: 2.0602 - val_acc: 0.5835\n",
      "Epoch 409/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9583 - acc: 0.5866 - val_loss: 2.0602 - val_acc: 0.5827\n",
      "Epoch 410/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9715 - acc: 0.5868 - val_loss: 2.0599 - val_acc: 0.5831\n",
      "Epoch 411/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9533 - acc: 0.5866 - val_loss: 2.0603 - val_acc: 0.5835\n",
      "Epoch 412/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9650 - acc: 0.5866 - val_loss: 2.0615 - val_acc: 0.5840\n",
      "Epoch 413/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9562 - acc: 0.5870 - val_loss: 2.0633 - val_acc: 0.5835\n",
      "Epoch 414/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9648 - acc: 0.5874 - val_loss: 2.0628 - val_acc: 0.5837\n",
      "Epoch 415/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9537 - acc: 0.5867 - val_loss: 2.0620 - val_acc: 0.5835\n",
      "Epoch 416/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9539 - acc: 0.5866 - val_loss: 2.0624 - val_acc: 0.5835\n",
      "Epoch 417/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9624 - acc: 0.5868 - val_loss: 2.0633 - val_acc: 0.5835\n",
      "Epoch 418/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9605 - acc: 0.5865 - val_loss: 2.0651 - val_acc: 0.5840\n",
      "Epoch 419/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9588 - acc: 0.5867 - val_loss: 2.0661 - val_acc: 0.5846\n",
      "Epoch 420/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9574 - acc: 0.5864 - val_loss: 2.0673 - val_acc: 0.5844\n",
      "Epoch 421/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9618 - acc: 0.5867 - val_loss: 2.0673 - val_acc: 0.5844\n",
      "Epoch 422/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 1.9579 - acc: 0.5871 - val_loss: 2.0673 - val_acc: 0.5844\n",
      "Epoch 423/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9563 - acc: 0.5874 - val_loss: 2.0671 - val_acc: 0.5846\n",
      "Epoch 424/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9590 - acc: 0.5864 - val_loss: 2.0679 - val_acc: 0.5850\n",
      "Epoch 425/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9642 - acc: 0.5870 - val_loss: 2.0687 - val_acc: 0.5848\n",
      "Epoch 426/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9603 - acc: 0.5871 - val_loss: 2.0705 - val_acc: 0.5846\n",
      "Epoch 427/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9678 - acc: 0.5870 - val_loss: 2.0724 - val_acc: 0.5846\n",
      "Epoch 428/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9612 - acc: 0.5872 - val_loss: 2.0727 - val_acc: 0.5844\n",
      "Epoch 429/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9530 - acc: 0.5870 - val_loss: 2.0725 - val_acc: 0.5844\n",
      "Epoch 430/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9544 - acc: 0.5868 - val_loss: 2.0715 - val_acc: 0.5842\n",
      "Epoch 431/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9583 - acc: 0.5867 - val_loss: 2.0690 - val_acc: 0.5842\n",
      "Epoch 432/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9650 - acc: 0.5866 - val_loss: 2.0669 - val_acc: 0.5846\n",
      "Epoch 433/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9700 - acc: 0.5866 - val_loss: 2.0651 - val_acc: 0.5848\n",
      "Epoch 434/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9624 - acc: 0.5870 - val_loss: 2.0635 - val_acc: 0.5848\n",
      "Epoch 435/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9542 - acc: 0.5872 - val_loss: 2.0637 - val_acc: 0.5848\n",
      "Epoch 436/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9551 - acc: 0.5880 - val_loss: 2.0639 - val_acc: 0.5848\n",
      "Epoch 437/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 1.9524 - acc: 0.5877 - val_loss: 2.0640 - val_acc: 0.5852\n",
      "Epoch 438/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9587 - acc: 0.5870 - val_loss: 2.0645 - val_acc: 0.5844\n",
      "Epoch 439/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9674 - acc: 0.5873 - val_loss: 2.0653 - val_acc: 0.5840\n",
      "Epoch 440/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9739 - acc: 0.5873 - val_loss: 2.0668 - val_acc: 0.5844\n",
      "Epoch 441/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9602 - acc: 0.5876 - val_loss: 2.0675 - val_acc: 0.5850\n",
      "Epoch 442/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9615 - acc: 0.5875 - val_loss: 2.0666 - val_acc: 0.5848\n",
      "Epoch 443/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9610 - acc: 0.5879 - val_loss: 2.0660 - val_acc: 0.5846\n",
      "Epoch 444/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9599 - acc: 0.5878 - val_loss: 2.0661 - val_acc: 0.5844\n",
      "Epoch 445/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9515 - acc: 0.5877 - val_loss: 2.0663 - val_acc: 0.5844\n",
      "Epoch 446/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9558 - acc: 0.5880 - val_loss: 2.0668 - val_acc: 0.5840\n",
      "Epoch 447/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9771 - acc: 0.5876 - val_loss: 2.0658 - val_acc: 0.5842\n",
      "Epoch 448/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9598 - acc: 0.5874 - val_loss: 2.0639 - val_acc: 0.5842\n",
      "Epoch 449/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9560 - acc: 0.5877 - val_loss: 2.0629 - val_acc: 0.5848\n",
      "Epoch 450/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9602 - acc: 0.5875 - val_loss: 2.0616 - val_acc: 0.5842\n",
      "Epoch 451/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9637 - acc: 0.5874 - val_loss: 2.0613 - val_acc: 0.5842\n",
      "Epoch 452/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9520 - acc: 0.5876 - val_loss: 2.0626 - val_acc: 0.5848\n",
      "Epoch 453/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9565 - acc: 0.5876 - val_loss: 2.0640 - val_acc: 0.5850\n",
      "Epoch 454/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 1.9528 - acc: 0.5878 - val_loss: 2.0649 - val_acc: 0.5854\n",
      "Epoch 455/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9597 - acc: 0.5872 - val_loss: 2.0645 - val_acc: 0.5846\n",
      "Epoch 456/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9594 - acc: 0.5875 - val_loss: 2.0634 - val_acc: 0.5842\n",
      "Epoch 457/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9508 - acc: 0.5878 - val_loss: 2.0630 - val_acc: 0.5854\n",
      "Epoch 458/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9620 - acc: 0.5879 - val_loss: 2.0636 - val_acc: 0.5846\n",
      "Epoch 459/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9595 - acc: 0.5885 - val_loss: 2.0647 - val_acc: 0.5840\n",
      "Epoch 460/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9603 - acc: 0.5874 - val_loss: 2.0650 - val_acc: 0.5842\n",
      "Epoch 461/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9562 - acc: 0.5881 - val_loss: 2.0643 - val_acc: 0.5842\n",
      "Epoch 462/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9491 - acc: 0.5880 - val_loss: 2.0634 - val_acc: 0.5844\n",
      "Epoch 463/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9589 - acc: 0.5888 - val_loss: 2.0627 - val_acc: 0.5846\n",
      "Epoch 464/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9635 - acc: 0.5879 - val_loss: 2.0619 - val_acc: 0.5848\n",
      "Epoch 465/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9695 - acc: 0.5870 - val_loss: 2.0626 - val_acc: 0.5840\n",
      "Epoch 466/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9601 - acc: 0.5874 - val_loss: 2.0635 - val_acc: 0.5833\n",
      "Epoch 467/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9553 - acc: 0.5882 - val_loss: 2.0625 - val_acc: 0.5835\n",
      "Epoch 468/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9569 - acc: 0.5884 - val_loss: 2.0617 - val_acc: 0.5837\n",
      "Epoch 469/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9510 - acc: 0.5882 - val_loss: 2.0615 - val_acc: 0.5833\n",
      "Epoch 470/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9543 - acc: 0.5877 - val_loss: 2.0613 - val_acc: 0.5840\n",
      "Epoch 471/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9675 - acc: 0.5881 - val_loss: 2.0602 - val_acc: 0.5835\n",
      "Epoch 472/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9518 - acc: 0.5878 - val_loss: 2.0600 - val_acc: 0.5833\n",
      "Epoch 473/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9605 - acc: 0.5884 - val_loss: 2.0608 - val_acc: 0.5837\n",
      "Epoch 474/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9664 - acc: 0.5879 - val_loss: 2.0602 - val_acc: 0.5835\n",
      "Epoch 475/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9485 - acc: 0.5885 - val_loss: 2.0603 - val_acc: 0.5833\n",
      "Epoch 476/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9469 - acc: 0.5878 - val_loss: 2.0613 - val_acc: 0.5833\n",
      "Epoch 477/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9587 - acc: 0.5878 - val_loss: 2.0618 - val_acc: 0.5833\n",
      "Epoch 478/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9573 - acc: 0.5882 - val_loss: 2.0618 - val_acc: 0.5835\n",
      "Epoch 479/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9552 - acc: 0.5874 - val_loss: 2.0623 - val_acc: 0.5833\n",
      "Epoch 480/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9515 - acc: 0.5877 - val_loss: 2.0631 - val_acc: 0.5833\n",
      "Epoch 481/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9564 - acc: 0.5883 - val_loss: 2.0632 - val_acc: 0.5829\n",
      "Epoch 482/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9519 - acc: 0.5879 - val_loss: 2.0636 - val_acc: 0.5833\n",
      "Epoch 483/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9540 - acc: 0.5880 - val_loss: 2.0636 - val_acc: 0.5833\n",
      "Epoch 484/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9547 - acc: 0.5876 - val_loss: 2.0641 - val_acc: 0.5840\n",
      "Epoch 485/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9505 - acc: 0.5883 - val_loss: 2.0640 - val_acc: 0.5840\n",
      "Epoch 486/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9536 - acc: 0.5880 - val_loss: 2.0643 - val_acc: 0.5840\n",
      "Epoch 487/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9623 - acc: 0.5884 - val_loss: 2.0642 - val_acc: 0.5833\n",
      "Epoch 488/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9541 - acc: 0.5879 - val_loss: 2.0640 - val_acc: 0.5829\n",
      "Epoch 489/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9526 - acc: 0.5878 - val_loss: 2.0645 - val_acc: 0.5833\n",
      "Epoch 490/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9511 - acc: 0.5886 - val_loss: 2.0649 - val_acc: 0.5831\n",
      "Epoch 491/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9492 - acc: 0.5880 - val_loss: 2.0652 - val_acc: 0.5835\n",
      "Epoch 492/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9530 - acc: 0.5882 - val_loss: 2.0648 - val_acc: 0.5837\n",
      "Epoch 493/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9500 - acc: 0.5885 - val_loss: 2.0652 - val_acc: 0.5835\n",
      "Epoch 494/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9514 - acc: 0.5887 - val_loss: 2.0656 - val_acc: 0.5837\n",
      "Epoch 495/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9514 - acc: 0.5884 - val_loss: 2.0658 - val_acc: 0.5840\n",
      "Epoch 496/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9570 - acc: 0.5886 - val_loss: 2.0659 - val_acc: 0.5833\n",
      "Epoch 497/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9530 - acc: 0.5880 - val_loss: 2.0666 - val_acc: 0.5831\n",
      "Epoch 498/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9569 - acc: 0.5882 - val_loss: 2.0674 - val_acc: 0.5835\n",
      "Epoch 499/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9500 - acc: 0.5880 - val_loss: 2.0678 - val_acc: 0.5833\n",
      "Epoch 500/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9592 - acc: 0.5886 - val_loss: 2.0678 - val_acc: 0.5840\n",
      "Epoch 501/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9499 - acc: 0.5880 - val_loss: 2.0667 - val_acc: 0.5837\n",
      "Epoch 502/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9498 - acc: 0.5884 - val_loss: 2.0665 - val_acc: 0.5840\n",
      "Epoch 503/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9487 - acc: 0.5875 - val_loss: 2.0657 - val_acc: 0.5835\n",
      "Epoch 504/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9611 - acc: 0.5882 - val_loss: 2.0643 - val_acc: 0.5842\n",
      "Epoch 505/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9495 - acc: 0.5884 - val_loss: 2.0627 - val_acc: 0.5842\n",
      "Epoch 506/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9479 - acc: 0.5886 - val_loss: 2.0624 - val_acc: 0.5835\n",
      "Epoch 507/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9610 - acc: 0.5882 - val_loss: 2.0629 - val_acc: 0.5840\n",
      "Epoch 508/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9583 - acc: 0.5880 - val_loss: 2.0633 - val_acc: 0.5848\n",
      "Epoch 509/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9560 - acc: 0.5884 - val_loss: 2.0628 - val_acc: 0.5842\n",
      "Epoch 510/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9527 - acc: 0.5885 - val_loss: 2.0619 - val_acc: 0.5846\n",
      "Epoch 511/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9447 - acc: 0.5885 - val_loss: 2.0618 - val_acc: 0.5844\n",
      "Epoch 512/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9520 - acc: 0.5886 - val_loss: 2.0621 - val_acc: 0.5844\n",
      "Epoch 513/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9538 - acc: 0.5886 - val_loss: 2.0605 - val_acc: 0.5842\n",
      "Epoch 514/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9506 - acc: 0.5883 - val_loss: 2.0604 - val_acc: 0.5842\n",
      "Epoch 515/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9564 - acc: 0.5880 - val_loss: 2.0599 - val_acc: 0.5842\n",
      "Epoch 516/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9511 - acc: 0.5888 - val_loss: 2.0604 - val_acc: 0.5840\n",
      "Epoch 517/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9482 - acc: 0.5884 - val_loss: 2.0604 - val_acc: 0.5840\n",
      "Epoch 518/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9597 - acc: 0.5884 - val_loss: 2.0609 - val_acc: 0.5842\n",
      "Epoch 519/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9455 - acc: 0.5884 - val_loss: 2.0610 - val_acc: 0.5844\n",
      "Epoch 520/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9506 - acc: 0.5891 - val_loss: 2.0609 - val_acc: 0.5844\n",
      "Epoch 521/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9525 - acc: 0.5889 - val_loss: 2.0604 - val_acc: 0.5842\n",
      "Epoch 522/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9535 - acc: 0.5880 - val_loss: 2.0574 - val_acc: 0.5840\n",
      "Epoch 523/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9624 - acc: 0.5886 - val_loss: 2.0561 - val_acc: 0.5848\n",
      "Epoch 524/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9603 - acc: 0.5884 - val_loss: 2.0559 - val_acc: 0.5837\n",
      "Epoch 525/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9523 - acc: 0.5888 - val_loss: 2.0552 - val_acc: 0.5844\n",
      "Epoch 526/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9494 - acc: 0.5882 - val_loss: 2.0549 - val_acc: 0.5842\n",
      "Epoch 527/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9474 - acc: 0.5885 - val_loss: 2.0549 - val_acc: 0.5850\n",
      "Epoch 528/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9685 - acc: 0.5888 - val_loss: 2.0545 - val_acc: 0.5837\n",
      "Epoch 529/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9582 - acc: 0.5885 - val_loss: 2.0524 - val_acc: 0.5844\n",
      "Epoch 530/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9489 - acc: 0.5888 - val_loss: 2.0514 - val_acc: 0.5842\n",
      "Epoch 531/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9497 - acc: 0.5884 - val_loss: 2.0522 - val_acc: 0.5837\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9505 - acc: 0.5884 - val_loss: 2.0535 - val_acc: 0.5844\n",
      "Epoch 533/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9462 - acc: 0.5884 - val_loss: 2.0545 - val_acc: 0.5842\n",
      "Epoch 534/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9508 - acc: 0.5888 - val_loss: 2.0551 - val_acc: 0.5846\n",
      "Epoch 535/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9601 - acc: 0.5886 - val_loss: 2.0550 - val_acc: 0.5844\n",
      "Epoch 536/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9466 - acc: 0.5886 - val_loss: 2.0544 - val_acc: 0.5842\n",
      "Epoch 537/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9661 - acc: 0.5888 - val_loss: 2.0524 - val_acc: 0.5846\n",
      "Epoch 538/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9573 - acc: 0.5884 - val_loss: 2.0511 - val_acc: 0.5848\n",
      "Epoch 539/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9499 - acc: 0.5889 - val_loss: 2.0518 - val_acc: 0.5846\n",
      "Epoch 540/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9548 - acc: 0.5889 - val_loss: 2.0523 - val_acc: 0.5846\n",
      "Epoch 541/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9544 - acc: 0.5883 - val_loss: 2.0531 - val_acc: 0.5844\n",
      "Epoch 542/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9474 - acc: 0.5889 - val_loss: 2.0544 - val_acc: 0.5840\n",
      "Epoch 543/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9447 - acc: 0.5889 - val_loss: 2.0559 - val_acc: 0.5840\n",
      "Epoch 544/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9492 - acc: 0.5884 - val_loss: 2.0576 - val_acc: 0.5842\n",
      "Epoch 545/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9513 - acc: 0.5882 - val_loss: 2.0586 - val_acc: 0.5844\n",
      "Epoch 546/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9568 - acc: 0.5880 - val_loss: 2.0574 - val_acc: 0.5842\n",
      "Epoch 547/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9538 - acc: 0.5889 - val_loss: 2.0563 - val_acc: 0.5835\n",
      "Epoch 548/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9484 - acc: 0.5890 - val_loss: 2.0553 - val_acc: 0.5842\n",
      "Epoch 549/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9597 - acc: 0.5892 - val_loss: 2.0541 - val_acc: 0.5844\n",
      "Epoch 550/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9454 - acc: 0.5886 - val_loss: 2.0533 - val_acc: 0.5844\n",
      "Epoch 551/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9491 - acc: 0.5888 - val_loss: 2.0530 - val_acc: 0.5844\n",
      "Epoch 552/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9480 - acc: 0.5888 - val_loss: 2.0537 - val_acc: 0.5840\n",
      "Epoch 553/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9586 - acc: 0.5896 - val_loss: 2.0530 - val_acc: 0.5833\n",
      "Epoch 554/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9609 - acc: 0.5888 - val_loss: 2.0526 - val_acc: 0.5837\n",
      "Epoch 555/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9506 - acc: 0.5887 - val_loss: 2.0526 - val_acc: 0.5837\n",
      "Epoch 556/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9465 - acc: 0.5889 - val_loss: 2.0529 - val_acc: 0.5837\n",
      "Epoch 557/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9483 - acc: 0.5889 - val_loss: 2.0541 - val_acc: 0.5846\n",
      "Epoch 558/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9467 - acc: 0.5893 - val_loss: 2.0533 - val_acc: 0.5840\n",
      "Epoch 559/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9471 - acc: 0.5889 - val_loss: 2.0528 - val_acc: 0.5844\n",
      "Epoch 560/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9465 - acc: 0.5895 - val_loss: 2.0535 - val_acc: 0.5837\n",
      "Epoch 561/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9540 - acc: 0.5886 - val_loss: 2.0526 - val_acc: 0.5842\n",
      "Epoch 562/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9518 - acc: 0.5890 - val_loss: 2.0526 - val_acc: 0.5844\n",
      "Epoch 563/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9663 - acc: 0.5889 - val_loss: 2.0533 - val_acc: 0.5837\n",
      "Epoch 564/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9482 - acc: 0.5885 - val_loss: 2.0540 - val_acc: 0.5840\n",
      "Epoch 565/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9468 - acc: 0.5895 - val_loss: 2.0535 - val_acc: 0.5833\n",
      "Epoch 566/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9443 - acc: 0.5895 - val_loss: 2.0537 - val_acc: 0.5833\n",
      "Epoch 567/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9494 - acc: 0.5892 - val_loss: 2.0537 - val_acc: 0.5833\n",
      "Epoch 568/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9537 - acc: 0.5890 - val_loss: 2.0541 - val_acc: 0.5837\n",
      "Epoch 569/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9470 - acc: 0.5887 - val_loss: 2.0547 - val_acc: 0.5835\n",
      "Epoch 570/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9444 - acc: 0.5897 - val_loss: 2.0559 - val_acc: 0.5837\n",
      "Epoch 571/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9508 - acc: 0.5891 - val_loss: 2.0571 - val_acc: 0.5835\n",
      "Epoch 572/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9455 - acc: 0.5892 - val_loss: 2.0575 - val_acc: 0.5833\n",
      "Epoch 573/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9604 - acc: 0.5886 - val_loss: 2.0579 - val_acc: 0.5833\n",
      "Epoch 574/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9514 - acc: 0.5892 - val_loss: 2.0584 - val_acc: 0.5833\n",
      "Epoch 575/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9468 - acc: 0.5900 - val_loss: 2.0585 - val_acc: 0.5840\n",
      "Epoch 576/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9460 - acc: 0.5895 - val_loss: 2.0590 - val_acc: 0.5842\n",
      "Epoch 577/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9466 - acc: 0.5894 - val_loss: 2.0584 - val_acc: 0.5846\n",
      "Epoch 578/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9539 - acc: 0.5894 - val_loss: 2.0574 - val_acc: 0.5846\n",
      "Epoch 579/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9468 - acc: 0.5894 - val_loss: 2.0581 - val_acc: 0.5837\n",
      "Epoch 580/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9517 - acc: 0.5894 - val_loss: 2.0583 - val_acc: 0.5837\n",
      "Epoch 581/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9417 - acc: 0.5894 - val_loss: 2.0586 - val_acc: 0.5840\n",
      "Epoch 582/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9557 - acc: 0.5898 - val_loss: 2.0585 - val_acc: 0.5842\n",
      "Epoch 583/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9569 - acc: 0.5886 - val_loss: 2.0584 - val_acc: 0.5837\n",
      "Epoch 584/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9452 - acc: 0.5892 - val_loss: 2.0590 - val_acc: 0.5842\n",
      "Epoch 585/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9414 - acc: 0.5903 - val_loss: 2.0592 - val_acc: 0.5835\n",
      "Epoch 586/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9486 - acc: 0.5888 - val_loss: 2.0589 - val_acc: 0.5835\n",
      "Epoch 587/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9468 - acc: 0.5887 - val_loss: 2.0589 - val_acc: 0.5837\n",
      "Epoch 588/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9428 - acc: 0.5893 - val_loss: 2.0597 - val_acc: 0.5840\n",
      "Epoch 589/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9658 - acc: 0.5895 - val_loss: 2.0608 - val_acc: 0.5835\n",
      "Epoch 590/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9466 - acc: 0.5890 - val_loss: 2.0611 - val_acc: 0.5833\n",
      "Epoch 591/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9479 - acc: 0.5888 - val_loss: 2.0611 - val_acc: 0.5837\n",
      "Epoch 592/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9406 - acc: 0.5896 - val_loss: 2.0617 - val_acc: 0.5837\n",
      "Epoch 593/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9767 - acc: 0.5899 - val_loss: 2.0622 - val_acc: 0.5835\n",
      "Epoch 594/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9536 - acc: 0.5895 - val_loss: 2.0639 - val_acc: 0.5840\n",
      "Epoch 595/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9571 - acc: 0.5895 - val_loss: 2.0630 - val_acc: 0.5837\n",
      "Epoch 596/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9421 - acc: 0.5895 - val_loss: 2.0604 - val_acc: 0.5835\n",
      "Epoch 597/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9441 - acc: 0.5896 - val_loss: 2.0597 - val_acc: 0.5840\n",
      "Epoch 598/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9401 - acc: 0.5896 - val_loss: 2.0599 - val_acc: 0.5842\n",
      "Epoch 599/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9674 - acc: 0.5896 - val_loss: 2.0588 - val_acc: 0.5840\n",
      "Epoch 600/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9433 - acc: 0.5897 - val_loss: 2.0566 - val_acc: 0.5842\n",
      "Epoch 601/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9508 - acc: 0.5893 - val_loss: 2.0554 - val_acc: 0.5842\n",
      "Epoch 602/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9456 - acc: 0.5892 - val_loss: 2.0551 - val_acc: 0.5844\n",
      "Epoch 603/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9515 - acc: 0.5891 - val_loss: 2.0548 - val_acc: 0.5846\n",
      "Epoch 604/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9736 - acc: 0.5895 - val_loss: 2.0550 - val_acc: 0.5840\n",
      "Epoch 605/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9461 - acc: 0.5897 - val_loss: 2.0563 - val_acc: 0.5835\n",
      "Epoch 606/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9513 - acc: 0.5893 - val_loss: 2.0569 - val_acc: 0.5842\n",
      "Epoch 607/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9425 - acc: 0.5895 - val_loss: 2.0567 - val_acc: 0.5840\n",
      "Epoch 608/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9488 - acc: 0.5899 - val_loss: 2.0567 - val_acc: 0.5840\n",
      "Epoch 609/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9405 - acc: 0.5897 - val_loss: 2.0576 - val_acc: 0.5835\n",
      "Epoch 610/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9447 - acc: 0.5898 - val_loss: 2.0584 - val_acc: 0.5844\n",
      "Epoch 611/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9391 - acc: 0.5896 - val_loss: 2.0590 - val_acc: 0.5842\n",
      "Epoch 612/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9457 - acc: 0.5899 - val_loss: 2.0593 - val_acc: 0.5835\n",
      "Epoch 613/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9451 - acc: 0.5894 - val_loss: 2.0597 - val_acc: 0.5835\n",
      "Epoch 614/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9478 - acc: 0.5900 - val_loss: 2.0600 - val_acc: 0.5833\n",
      "Epoch 615/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9481 - acc: 0.5893 - val_loss: 2.0595 - val_acc: 0.5835\n",
      "Epoch 616/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9420 - acc: 0.5895 - val_loss: 2.0594 - val_acc: 0.5833\n",
      "Epoch 617/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9423 - acc: 0.5903 - val_loss: 2.0595 - val_acc: 0.5835\n",
      "Epoch 618/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9508 - acc: 0.5893 - val_loss: 2.0608 - val_acc: 0.5837\n",
      "Epoch 619/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9444 - acc: 0.5902 - val_loss: 2.0607 - val_acc: 0.5842\n",
      "Epoch 620/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9487 - acc: 0.5897 - val_loss: 2.0609 - val_acc: 0.5840\n",
      "Epoch 621/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9405 - acc: 0.5897 - val_loss: 2.0612 - val_acc: 0.5842\n",
      "Epoch 622/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9475 - acc: 0.5897 - val_loss: 2.0612 - val_acc: 0.5835\n",
      "Epoch 623/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9566 - acc: 0.5902 - val_loss: 2.0610 - val_acc: 0.5837\n",
      "Epoch 624/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 1.9447 - acc: 0.5899 - val_loss: 2.0602 - val_acc: 0.5842\n",
      "Epoch 625/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9453 - acc: 0.5900 - val_loss: 2.0587 - val_acc: 0.5844\n",
      "Epoch 626/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9542 - acc: 0.5902 - val_loss: 2.0564 - val_acc: 0.5844\n",
      "Epoch 627/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9451 - acc: 0.5900 - val_loss: 2.0564 - val_acc: 0.5842\n",
      "Epoch 628/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9524 - acc: 0.5891 - val_loss: 2.0563 - val_acc: 0.5837\n",
      "Epoch 629/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9397 - acc: 0.5895 - val_loss: 2.0563 - val_acc: 0.5831\n",
      "Epoch 630/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9437 - acc: 0.5900 - val_loss: 2.0555 - val_acc: 0.5835\n",
      "Epoch 631/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9436 - acc: 0.5901 - val_loss: 2.0557 - val_acc: 0.5837\n",
      "Epoch 632/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9615 - acc: 0.5900 - val_loss: 2.0571 - val_acc: 0.5840\n",
      "Epoch 633/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9474 - acc: 0.5895 - val_loss: 2.0573 - val_acc: 0.5840\n",
      "Epoch 634/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9504 - acc: 0.5898 - val_loss: 2.0569 - val_acc: 0.5844\n",
      "Epoch 635/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9435 - acc: 0.5897 - val_loss: 2.0569 - val_acc: 0.5842\n",
      "Epoch 636/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9525 - acc: 0.5898 - val_loss: 2.0566 - val_acc: 0.5837\n",
      "Epoch 637/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9464 - acc: 0.5898 - val_loss: 2.0556 - val_acc: 0.5831\n",
      "Epoch 638/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9549 - acc: 0.5904 - val_loss: 2.0552 - val_acc: 0.5835\n",
      "Epoch 639/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9522 - acc: 0.5900 - val_loss: 2.0543 - val_acc: 0.5842\n",
      "Epoch 640/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9482 - acc: 0.5900 - val_loss: 2.0541 - val_acc: 0.5840\n",
      "Epoch 641/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9529 - acc: 0.5903 - val_loss: 2.0536 - val_acc: 0.5835\n",
      "Epoch 642/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9497 - acc: 0.5898 - val_loss: 2.0536 - val_acc: 0.5831\n",
      "Epoch 643/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9421 - acc: 0.5906 - val_loss: 2.0539 - val_acc: 0.5833\n",
      "Epoch 644/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9450 - acc: 0.5902 - val_loss: 2.0550 - val_acc: 0.5835\n",
      "Epoch 645/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9476 - acc: 0.5897 - val_loss: 2.0558 - val_acc: 0.5837\n",
      "Epoch 646/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9456 - acc: 0.5901 - val_loss: 2.0567 - val_acc: 0.5837\n",
      "Epoch 647/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9461 - acc: 0.5898 - val_loss: 2.0568 - val_acc: 0.5831\n",
      "Epoch 648/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9522 - acc: 0.5906 - val_loss: 2.0570 - val_acc: 0.5829\n",
      "Epoch 649/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9537 - acc: 0.5896 - val_loss: 2.0566 - val_acc: 0.5827\n",
      "Epoch 650/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9414 - acc: 0.5900 - val_loss: 2.0573 - val_acc: 0.5831\n",
      "Epoch 651/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9506 - acc: 0.5903 - val_loss: 2.0570 - val_acc: 0.5831\n",
      "Epoch 652/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9382 - acc: 0.5902 - val_loss: 2.0572 - val_acc: 0.5833\n",
      "Epoch 653/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9471 - acc: 0.5898 - val_loss: 2.0578 - val_acc: 0.5846\n",
      "Epoch 654/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9379 - acc: 0.5900 - val_loss: 2.0580 - val_acc: 0.5846\n",
      "Epoch 655/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9435 - acc: 0.5903 - val_loss: 2.0585 - val_acc: 0.5842\n",
      "Epoch 656/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9461 - acc: 0.5896 - val_loss: 2.0591 - val_acc: 0.5833\n",
      "Epoch 657/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9408 - acc: 0.5897 - val_loss: 2.0592 - val_acc: 0.5833\n",
      "Epoch 658/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9665 - acc: 0.5898 - val_loss: 2.0597 - val_acc: 0.5835\n",
      "Epoch 659/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9444 - acc: 0.5900 - val_loss: 2.0597 - val_acc: 0.5837\n",
      "Epoch 660/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9391 - acc: 0.5902 - val_loss: 2.0584 - val_acc: 0.5844\n",
      "Epoch 661/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9440 - acc: 0.5902 - val_loss: 2.0584 - val_acc: 0.5840\n",
      "Epoch 662/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9455 - acc: 0.5898 - val_loss: 2.0579 - val_acc: 0.5837\n",
      "Epoch 663/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9464 - acc: 0.5901 - val_loss: 2.0559 - val_acc: 0.5837\n",
      "Epoch 664/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9426 - acc: 0.5895 - val_loss: 2.0558 - val_acc: 0.5837\n",
      "Epoch 665/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9414 - acc: 0.5907 - val_loss: 2.0566 - val_acc: 0.5842\n",
      "Epoch 666/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9404 - acc: 0.5902 - val_loss: 2.0576 - val_acc: 0.5840\n",
      "Epoch 667/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9403 - acc: 0.5903 - val_loss: 2.0585 - val_acc: 0.5842\n",
      "Epoch 668/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9453 - acc: 0.5901 - val_loss: 2.0588 - val_acc: 0.5840\n",
      "Epoch 669/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9419 - acc: 0.5898 - val_loss: 2.0596 - val_acc: 0.5833\n",
      "Epoch 670/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9426 - acc: 0.5904 - val_loss: 2.0598 - val_acc: 0.5831\n",
      "Epoch 671/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9475 - acc: 0.5903 - val_loss: 2.0600 - val_acc: 0.5837\n",
      "Epoch 672/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9394 - acc: 0.5906 - val_loss: 2.0608 - val_acc: 0.5835\n",
      "Epoch 673/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9484 - acc: 0.5900 - val_loss: 2.0615 - val_acc: 0.5835\n",
      "Epoch 674/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9386 - acc: 0.5904 - val_loss: 2.0612 - val_acc: 0.5831\n",
      "Epoch 675/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9448 - acc: 0.5906 - val_loss: 2.0611 - val_acc: 0.5831\n",
      "Epoch 676/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9370 - acc: 0.5906 - val_loss: 2.0603 - val_acc: 0.5835\n",
      "Epoch 677/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9426 - acc: 0.5905 - val_loss: 2.0596 - val_acc: 0.5835\n",
      "Epoch 678/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9407 - acc: 0.5903 - val_loss: 2.0596 - val_acc: 0.5840\n",
      "Epoch 679/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9409 - acc: 0.5903 - val_loss: 2.0598 - val_acc: 0.5837\n",
      "Epoch 680/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9464 - acc: 0.5902 - val_loss: 2.0598 - val_acc: 0.5837\n",
      "Epoch 681/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9417 - acc: 0.5903 - val_loss: 2.0587 - val_acc: 0.5837\n",
      "Epoch 682/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9461 - acc: 0.5905 - val_loss: 2.0582 - val_acc: 0.5835\n",
      "Epoch 683/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9442 - acc: 0.5905 - val_loss: 2.0579 - val_acc: 0.5837\n",
      "Epoch 684/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9439 - acc: 0.5905 - val_loss: 2.0576 - val_acc: 0.5840\n",
      "Epoch 685/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9365 - acc: 0.5908 - val_loss: 2.0576 - val_acc: 0.5842\n",
      "Epoch 686/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9512 - acc: 0.5904 - val_loss: 2.0564 - val_acc: 0.5837\n",
      "Epoch 687/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9453 - acc: 0.5906 - val_loss: 2.0554 - val_acc: 0.5842\n",
      "Epoch 688/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 1.9467 - acc: 0.5904 - val_loss: 2.0548 - val_acc: 0.5842\n",
      "Epoch 689/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9509 - acc: 0.5910 - val_loss: 2.0546 - val_acc: 0.5844\n",
      "Epoch 690/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9444 - acc: 0.5905 - val_loss: 2.0537 - val_acc: 0.5848\n",
      "Epoch 691/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9415 - acc: 0.5904 - val_loss: 2.0536 - val_acc: 0.5848\n",
      "Epoch 692/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9384 - acc: 0.5905 - val_loss: 2.0519 - val_acc: 0.5840\n",
      "Epoch 693/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9443 - acc: 0.5908 - val_loss: 2.0518 - val_acc: 0.5842\n",
      "Epoch 694/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9440 - acc: 0.5909 - val_loss: 2.0519 - val_acc: 0.5846\n",
      "Epoch 695/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9417 - acc: 0.5903 - val_loss: 2.0518 - val_acc: 0.5844\n",
      "Epoch 696/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9442 - acc: 0.5910 - val_loss: 2.0522 - val_acc: 0.5844\n",
      "Epoch 697/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9391 - acc: 0.5908 - val_loss: 2.0525 - val_acc: 0.5844\n",
      "Epoch 698/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9432 - acc: 0.5904 - val_loss: 2.0532 - val_acc: 0.5842\n",
      "Epoch 699/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9501 - acc: 0.5903 - val_loss: 2.0529 - val_acc: 0.5842\n",
      "Epoch 700/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9515 - acc: 0.5905 - val_loss: 2.0514 - val_acc: 0.5846\n",
      "Epoch 701/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9386 - acc: 0.5900 - val_loss: 2.0504 - val_acc: 0.5850\n",
      "Epoch 702/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9429 - acc: 0.5905 - val_loss: 2.0497 - val_acc: 0.5844\n",
      "Epoch 703/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9422 - acc: 0.5906 - val_loss: 2.0494 - val_acc: 0.5842\n",
      "Epoch 704/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9498 - acc: 0.5903 - val_loss: 2.0490 - val_acc: 0.5850\n",
      "Epoch 705/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9425 - acc: 0.5902 - val_loss: 2.0487 - val_acc: 0.5850\n",
      "Epoch 706/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9511 - acc: 0.5908 - val_loss: 2.0488 - val_acc: 0.5848\n",
      "Epoch 707/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9418 - acc: 0.5907 - val_loss: 2.0493 - val_acc: 0.5850\n",
      "Epoch 708/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9369 - acc: 0.5910 - val_loss: 2.0501 - val_acc: 0.5850\n",
      "Epoch 709/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9447 - acc: 0.5900 - val_loss: 2.0508 - val_acc: 0.5856\n",
      "Epoch 710/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9427 - acc: 0.5910 - val_loss: 2.0511 - val_acc: 0.5854\n",
      "Epoch 711/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9470 - acc: 0.5903 - val_loss: 2.0510 - val_acc: 0.5858\n",
      "Epoch 712/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 126ms/step - loss: 1.9412 - acc: 0.5904 - val_loss: 2.0506 - val_acc: 0.5860\n",
      "Epoch 713/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9454 - acc: 0.5906 - val_loss: 2.0492 - val_acc: 0.5854\n",
      "Epoch 714/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9404 - acc: 0.5903 - val_loss: 2.0491 - val_acc: 0.5858\n",
      "Epoch 715/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9429 - acc: 0.5906 - val_loss: 2.0490 - val_acc: 0.5854\n",
      "Epoch 716/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9376 - acc: 0.5910 - val_loss: 2.0488 - val_acc: 0.5854\n",
      "Epoch 717/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9403 - acc: 0.5904 - val_loss: 2.0488 - val_acc: 0.5854\n",
      "Epoch 718/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9460 - acc: 0.5907 - val_loss: 2.0501 - val_acc: 0.5852\n",
      "Epoch 719/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9374 - acc: 0.5905 - val_loss: 2.0507 - val_acc: 0.5848\n",
      "Epoch 720/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9340 - acc: 0.5911 - val_loss: 2.0503 - val_acc: 0.5852\n",
      "Epoch 721/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9421 - acc: 0.5906 - val_loss: 2.0501 - val_acc: 0.5854\n",
      "Epoch 722/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9433 - acc: 0.5902 - val_loss: 2.0485 - val_acc: 0.5854\n",
      "Epoch 723/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9399 - acc: 0.5910 - val_loss: 2.0489 - val_acc: 0.5858\n",
      "Epoch 724/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9419 - acc: 0.5903 - val_loss: 2.0499 - val_acc: 0.5858\n",
      "Epoch 725/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9386 - acc: 0.5911 - val_loss: 2.0506 - val_acc: 0.5848\n",
      "Epoch 726/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9395 - acc: 0.5915 - val_loss: 2.0498 - val_acc: 0.5848\n",
      "Epoch 727/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9374 - acc: 0.5912 - val_loss: 2.0493 - val_acc: 0.5848\n",
      "Epoch 728/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9444 - acc: 0.5907 - val_loss: 2.0486 - val_acc: 0.5848\n",
      "Epoch 729/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9446 - acc: 0.5906 - val_loss: 2.0472 - val_acc: 0.5850\n",
      "Epoch 730/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9550 - acc: 0.5901 - val_loss: 2.0454 - val_acc: 0.5848\n",
      "Epoch 731/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9393 - acc: 0.5908 - val_loss: 2.0451 - val_acc: 0.5852\n",
      "Epoch 732/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9390 - acc: 0.5903 - val_loss: 2.0454 - val_acc: 0.5854\n",
      "Epoch 733/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9384 - acc: 0.5909 - val_loss: 2.0460 - val_acc: 0.5852\n",
      "Epoch 734/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9383 - acc: 0.5913 - val_loss: 2.0459 - val_acc: 0.5854\n",
      "Epoch 735/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9447 - acc: 0.5910 - val_loss: 2.0448 - val_acc: 0.5852\n",
      "Epoch 736/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9352 - acc: 0.5909 - val_loss: 2.0446 - val_acc: 0.5856\n",
      "Epoch 737/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9509 - acc: 0.5909 - val_loss: 2.0446 - val_acc: 0.5852\n",
      "Epoch 738/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9531 - acc: 0.5904 - val_loss: 2.0438 - val_acc: 0.5852\n",
      "Epoch 739/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9420 - acc: 0.5915 - val_loss: 2.0433 - val_acc: 0.5854\n",
      "Epoch 740/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9401 - acc: 0.5909 - val_loss: 2.0440 - val_acc: 0.5852\n",
      "Epoch 741/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9407 - acc: 0.5902 - val_loss: 2.0442 - val_acc: 0.5846\n",
      "Epoch 742/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9498 - acc: 0.5908 - val_loss: 2.0440 - val_acc: 0.5850\n",
      "Epoch 743/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9453 - acc: 0.5907 - val_loss: 2.0446 - val_acc: 0.5850\n",
      "Epoch 744/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9396 - acc: 0.5906 - val_loss: 2.0449 - val_acc: 0.5850\n",
      "Epoch 745/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9413 - acc: 0.5912 - val_loss: 2.0447 - val_acc: 0.5850\n",
      "Epoch 746/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9416 - acc: 0.5905 - val_loss: 2.0439 - val_acc: 0.5854\n",
      "Epoch 747/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9394 - acc: 0.5907 - val_loss: 2.0436 - val_acc: 0.5850\n",
      "Epoch 748/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9461 - acc: 0.5909 - val_loss: 2.0430 - val_acc: 0.5854\n",
      "Epoch 749/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9394 - acc: 0.5907 - val_loss: 2.0424 - val_acc: 0.5852\n",
      "Epoch 750/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9417 - acc: 0.5916 - val_loss: 2.0423 - val_acc: 0.5850\n",
      "Epoch 751/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9356 - acc: 0.5906 - val_loss: 2.0419 - val_acc: 0.5854\n",
      "Epoch 752/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9372 - acc: 0.5912 - val_loss: 2.0423 - val_acc: 0.5856\n",
      "Epoch 753/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9333 - acc: 0.5916 - val_loss: 2.0427 - val_acc: 0.5854\n",
      "Epoch 754/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9438 - acc: 0.5914 - val_loss: 2.0422 - val_acc: 0.5852\n",
      "Epoch 755/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9481 - acc: 0.5914 - val_loss: 2.0425 - val_acc: 0.5850\n",
      "Epoch 756/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9463 - acc: 0.5914 - val_loss: 2.0427 - val_acc: 0.5852\n",
      "Epoch 757/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9544 - acc: 0.5907 - val_loss: 2.0421 - val_acc: 0.5850\n",
      "Epoch 758/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9366 - acc: 0.5913 - val_loss: 2.0424 - val_acc: 0.5852\n",
      "Epoch 759/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9421 - acc: 0.5913 - val_loss: 2.0426 - val_acc: 0.5850\n",
      "Epoch 760/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9394 - acc: 0.5908 - val_loss: 2.0434 - val_acc: 0.5856\n",
      "Epoch 761/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9392 - acc: 0.5912 - val_loss: 2.0439 - val_acc: 0.5852\n",
      "Epoch 762/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9395 - acc: 0.5912 - val_loss: 2.0442 - val_acc: 0.5854\n",
      "Epoch 763/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9439 - acc: 0.5913 - val_loss: 2.0438 - val_acc: 0.5854\n",
      "Epoch 764/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9479 - acc: 0.5916 - val_loss: 2.0432 - val_acc: 0.5856\n",
      "Epoch 765/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9447 - acc: 0.5915 - val_loss: 2.0426 - val_acc: 0.5854\n",
      "Epoch 766/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9394 - acc: 0.5908 - val_loss: 2.0426 - val_acc: 0.5858\n",
      "Epoch 767/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9384 - acc: 0.5910 - val_loss: 2.0420 - val_acc: 0.5860\n",
      "Epoch 768/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9412 - acc: 0.5907 - val_loss: 2.0420 - val_acc: 0.5856\n",
      "Epoch 769/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9381 - acc: 0.5908 - val_loss: 2.0422 - val_acc: 0.5862\n",
      "Epoch 770/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9450 - acc: 0.5908 - val_loss: 2.0425 - val_acc: 0.5858\n",
      "Epoch 771/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9402 - acc: 0.5909 - val_loss: 2.0427 - val_acc: 0.5858\n",
      "Epoch 772/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9474 - acc: 0.5907 - val_loss: 2.0430 - val_acc: 0.5858\n",
      "Epoch 773/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9475 - acc: 0.5916 - val_loss: 2.0442 - val_acc: 0.5856\n",
      "Epoch 774/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9428 - acc: 0.5907 - val_loss: 2.0442 - val_acc: 0.5858\n",
      "Epoch 775/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9383 - acc: 0.5915 - val_loss: 2.0446 - val_acc: 0.5864\n",
      "Epoch 776/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9388 - acc: 0.5906 - val_loss: 2.0456 - val_acc: 0.5862\n",
      "Epoch 777/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9374 - acc: 0.5916 - val_loss: 2.0472 - val_acc: 0.5866\n",
      "Epoch 778/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9341 - acc: 0.5908 - val_loss: 2.0487 - val_acc: 0.5862\n",
      "Epoch 779/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9374 - acc: 0.5913 - val_loss: 2.0497 - val_acc: 0.5862\n",
      "Epoch 780/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9589 - acc: 0.5907 - val_loss: 2.0498 - val_acc: 0.5860\n",
      "Epoch 781/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9428 - acc: 0.5909 - val_loss: 2.0485 - val_acc: 0.5860\n",
      "Epoch 782/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9684 - acc: 0.5909 - val_loss: 2.0483 - val_acc: 0.5860\n",
      "Epoch 783/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9411 - acc: 0.5912 - val_loss: 2.0481 - val_acc: 0.5862\n",
      "Epoch 784/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9394 - acc: 0.5916 - val_loss: 2.0477 - val_acc: 0.5864\n",
      "Epoch 785/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9410 - acc: 0.5910 - val_loss: 2.0478 - val_acc: 0.5866\n",
      "Epoch 786/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9398 - acc: 0.5914 - val_loss: 2.0487 - val_acc: 0.5866\n",
      "Epoch 787/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9362 - acc: 0.5914 - val_loss: 2.0489 - val_acc: 0.5860\n",
      "Epoch 788/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9402 - acc: 0.5913 - val_loss: 2.0486 - val_acc: 0.5858\n",
      "Epoch 789/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9361 - acc: 0.5908 - val_loss: 2.0482 - val_acc: 0.5862\n",
      "Epoch 790/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9361 - acc: 0.5908 - val_loss: 2.0481 - val_acc: 0.5862\n",
      "Epoch 791/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9346 - acc: 0.5916 - val_loss: 2.0484 - val_acc: 0.5866\n",
      "Epoch 792/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9429 - acc: 0.5911 - val_loss: 2.0488 - val_acc: 0.5864\n",
      "Epoch 793/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9416 - acc: 0.5913 - val_loss: 2.0495 - val_acc: 0.5864\n",
      "Epoch 794/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9385 - acc: 0.5909 - val_loss: 2.0494 - val_acc: 0.5864\n",
      "Epoch 795/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9357 - acc: 0.5914 - val_loss: 2.0492 - val_acc: 0.5866\n",
      "Epoch 796/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9594 - acc: 0.5913 - val_loss: 2.0486 - val_acc: 0.5864\n",
      "Epoch 797/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9371 - acc: 0.5915 - val_loss: 2.0481 - val_acc: 0.5866\n",
      "Epoch 798/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9382 - acc: 0.5908 - val_loss: 2.0472 - val_acc: 0.5864\n",
      "Epoch 799/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9404 - acc: 0.5906 - val_loss: 2.0474 - val_acc: 0.5864\n",
      "Epoch 800/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9363 - acc: 0.5914 - val_loss: 2.0480 - val_acc: 0.5866\n",
      "Epoch 801/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9415 - acc: 0.5917 - val_loss: 2.0483 - val_acc: 0.5864\n",
      "Epoch 802/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9324 - acc: 0.5919 - val_loss: 2.0483 - val_acc: 0.5866\n",
      "Epoch 803/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9445 - acc: 0.5915 - val_loss: 2.0477 - val_acc: 0.5870\n",
      "Epoch 804/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9365 - acc: 0.5914 - val_loss: 2.0476 - val_acc: 0.5868\n",
      "Epoch 805/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9359 - acc: 0.5914 - val_loss: 2.0474 - val_acc: 0.5873\n",
      "Epoch 806/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9352 - acc: 0.5916 - val_loss: 2.0478 - val_acc: 0.5866\n",
      "Epoch 807/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9360 - acc: 0.5910 - val_loss: 2.0478 - val_acc: 0.5860\n",
      "Epoch 808/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9327 - acc: 0.5916 - val_loss: 2.0483 - val_acc: 0.5864\n",
      "Epoch 809/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9488 - acc: 0.5911 - val_loss: 2.0487 - val_acc: 0.5866\n",
      "Epoch 810/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9392 - acc: 0.5915 - val_loss: 2.0488 - val_acc: 0.5864\n",
      "Epoch 811/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9438 - acc: 0.5912 - val_loss: 2.0482 - val_acc: 0.5862\n",
      "Epoch 812/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9378 - acc: 0.5909 - val_loss: 2.0472 - val_acc: 0.5860\n",
      "Epoch 813/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9412 - acc: 0.5911 - val_loss: 2.0471 - val_acc: 0.5860\n",
      "Epoch 814/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9317 - acc: 0.5915 - val_loss: 2.0469 - val_acc: 0.5862\n",
      "Epoch 815/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9401 - acc: 0.5911 - val_loss: 2.0465 - val_acc: 0.5860\n",
      "Epoch 816/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9455 - acc: 0.5909 - val_loss: 2.0461 - val_acc: 0.5864\n",
      "Epoch 817/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9395 - acc: 0.5912 - val_loss: 2.0459 - val_acc: 0.5868\n",
      "Epoch 818/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9420 - acc: 0.5913 - val_loss: 2.0458 - val_acc: 0.5862\n",
      "Epoch 819/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9531 - acc: 0.5912 - val_loss: 2.0455 - val_acc: 0.5862\n",
      "Epoch 820/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9399 - acc: 0.5911 - val_loss: 2.0455 - val_acc: 0.5858\n",
      "Epoch 821/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9360 - acc: 0.5920 - val_loss: 2.0455 - val_acc: 0.5860\n",
      "Epoch 822/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9420 - acc: 0.5913 - val_loss: 2.0452 - val_acc: 0.5860\n",
      "Epoch 823/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9371 - acc: 0.5914 - val_loss: 2.0452 - val_acc: 0.5860\n",
      "Epoch 824/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9357 - acc: 0.5917 - val_loss: 2.0459 - val_acc: 0.5862\n",
      "Epoch 825/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9359 - acc: 0.5913 - val_loss: 2.0458 - val_acc: 0.5858\n",
      "Epoch 826/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9389 - acc: 0.5920 - val_loss: 2.0450 - val_acc: 0.5862\n",
      "Epoch 827/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9333 - acc: 0.5914 - val_loss: 2.0439 - val_acc: 0.5864\n",
      "Epoch 828/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9358 - acc: 0.5921 - val_loss: 2.0438 - val_acc: 0.5862\n",
      "Epoch 829/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9368 - acc: 0.5914 - val_loss: 2.0439 - val_acc: 0.5862\n",
      "Epoch 830/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9323 - acc: 0.5914 - val_loss: 2.0445 - val_acc: 0.5864\n",
      "Epoch 831/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9336 - acc: 0.5911 - val_loss: 2.0456 - val_acc: 0.5862\n",
      "Epoch 832/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9353 - acc: 0.5917 - val_loss: 2.0460 - val_acc: 0.5862\n",
      "Epoch 833/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9342 - acc: 0.5909 - val_loss: 2.0455 - val_acc: 0.5864\n",
      "Epoch 834/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9382 - acc: 0.5911 - val_loss: 2.0445 - val_acc: 0.5864\n",
      "Epoch 835/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9387 - acc: 0.5913 - val_loss: 2.0444 - val_acc: 0.5864\n",
      "Epoch 836/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9397 - acc: 0.5918 - val_loss: 2.0440 - val_acc: 0.5866\n",
      "Epoch 837/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9357 - acc: 0.5914 - val_loss: 2.0432 - val_acc: 0.5868\n",
      "Epoch 838/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9332 - acc: 0.5915 - val_loss: 2.0426 - val_acc: 0.5868\n",
      "Epoch 839/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9367 - acc: 0.5917 - val_loss: 2.0430 - val_acc: 0.5866\n",
      "Epoch 840/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9326 - acc: 0.5917 - val_loss: 2.0432 - val_acc: 0.5868\n",
      "Epoch 841/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9349 - acc: 0.5918 - val_loss: 2.0427 - val_acc: 0.5866\n",
      "Epoch 842/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9383 - acc: 0.5915 - val_loss: 2.0430 - val_acc: 0.5868\n",
      "Epoch 843/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9364 - acc: 0.5915 - val_loss: 2.0431 - val_acc: 0.5866\n",
      "Epoch 844/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9322 - acc: 0.5913 - val_loss: 2.0433 - val_acc: 0.5868\n",
      "Epoch 845/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9340 - acc: 0.5916 - val_loss: 2.0439 - val_acc: 0.5866\n",
      "Epoch 846/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9366 - acc: 0.5914 - val_loss: 2.0447 - val_acc: 0.5866\n",
      "Epoch 847/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9420 - acc: 0.5912 - val_loss: 2.0451 - val_acc: 0.5866\n",
      "Epoch 848/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9369 - acc: 0.5916 - val_loss: 2.0460 - val_acc: 0.5866\n",
      "Epoch 849/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9468 - acc: 0.5913 - val_loss: 2.0462 - val_acc: 0.5866\n",
      "Epoch 850/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9351 - acc: 0.5919 - val_loss: 2.0470 - val_acc: 0.5866\n",
      "Epoch 851/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9461 - acc: 0.5912 - val_loss: 2.0451 - val_acc: 0.5866\n",
      "Epoch 852/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9376 - acc: 0.5915 - val_loss: 2.0432 - val_acc: 0.5864\n",
      "Epoch 853/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9406 - acc: 0.5916 - val_loss: 2.0420 - val_acc: 0.5862\n",
      "Epoch 854/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9378 - acc: 0.5913 - val_loss: 2.0417 - val_acc: 0.5864\n",
      "Epoch 855/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9384 - acc: 0.5917 - val_loss: 2.0412 - val_acc: 0.5862\n",
      "Epoch 856/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9338 - acc: 0.5921 - val_loss: 2.0407 - val_acc: 0.5862\n",
      "Epoch 857/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9359 - acc: 0.5918 - val_loss: 2.0407 - val_acc: 0.5862\n",
      "Epoch 858/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9341 - acc: 0.5919 - val_loss: 2.0410 - val_acc: 0.5864\n",
      "Epoch 859/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9381 - acc: 0.5907 - val_loss: 2.0415 - val_acc: 0.5862\n",
      "Epoch 860/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9321 - acc: 0.5922 - val_loss: 2.0419 - val_acc: 0.5864\n",
      "Epoch 861/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9335 - acc: 0.5920 - val_loss: 2.0420 - val_acc: 0.5866\n",
      "Epoch 862/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9330 - acc: 0.5917 - val_loss: 2.0424 - val_acc: 0.5866\n",
      "Epoch 863/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9429 - acc: 0.5912 - val_loss: 2.0434 - val_acc: 0.5864\n",
      "Epoch 864/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9405 - acc: 0.5915 - val_loss: 2.0429 - val_acc: 0.5864\n",
      "Epoch 865/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9362 - acc: 0.5915 - val_loss: 2.0415 - val_acc: 0.5866\n",
      "Epoch 866/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9370 - acc: 0.5918 - val_loss: 2.0409 - val_acc: 0.5866\n",
      "Epoch 867/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9474 - acc: 0.5917 - val_loss: 2.0409 - val_acc: 0.5868\n",
      "Epoch 868/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9433 - acc: 0.5917 - val_loss: 2.0417 - val_acc: 0.5868\n",
      "Epoch 869/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9360 - acc: 0.5917 - val_loss: 2.0415 - val_acc: 0.5866\n",
      "Epoch 870/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9357 - acc: 0.5918 - val_loss: 2.0415 - val_acc: 0.5866\n",
      "Epoch 871/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9329 - acc: 0.5916 - val_loss: 2.0414 - val_acc: 0.5862\n",
      "Epoch 872/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9389 - acc: 0.5919 - val_loss: 2.0415 - val_acc: 0.5860\n",
      "Epoch 873/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9471 - acc: 0.5918 - val_loss: 2.0416 - val_acc: 0.5864\n",
      "Epoch 874/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9345 - acc: 0.5922 - val_loss: 2.0416 - val_acc: 0.5862\n",
      "Epoch 875/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9415 - acc: 0.5920 - val_loss: 2.0420 - val_acc: 0.5868\n",
      "Epoch 876/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9313 - acc: 0.5920 - val_loss: 2.0426 - val_acc: 0.5868\n",
      "Epoch 877/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9379 - acc: 0.5916 - val_loss: 2.0431 - val_acc: 0.5864\n",
      "Epoch 878/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9361 - acc: 0.5916 - val_loss: 2.0428 - val_acc: 0.5864\n",
      "Epoch 879/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9322 - acc: 0.5920 - val_loss: 2.0430 - val_acc: 0.5864\n",
      "Epoch 880/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9338 - acc: 0.5921 - val_loss: 2.0431 - val_acc: 0.5864\n",
      "Epoch 881/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9300 - acc: 0.5921 - val_loss: 2.0434 - val_acc: 0.5862\n",
      "Epoch 882/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9323 - acc: 0.5919 - val_loss: 2.0435 - val_acc: 0.5862\n",
      "Epoch 883/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9349 - acc: 0.5921 - val_loss: 2.0439 - val_acc: 0.5862\n",
      "Epoch 884/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9432 - acc: 0.5915 - val_loss: 2.0442 - val_acc: 0.5866\n",
      "Epoch 885/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9359 - acc: 0.5919 - val_loss: 2.0423 - val_acc: 0.5866\n",
      "Epoch 886/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9365 - acc: 0.5919 - val_loss: 2.0417 - val_acc: 0.5864\n",
      "Epoch 887/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9343 - acc: 0.5918 - val_loss: 2.0414 - val_acc: 0.5864\n",
      "Epoch 888/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9327 - acc: 0.5921 - val_loss: 2.0410 - val_acc: 0.5866\n",
      "Epoch 889/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9337 - acc: 0.5922 - val_loss: 2.0407 - val_acc: 0.5864\n",
      "Epoch 890/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9332 - acc: 0.5922 - val_loss: 2.0406 - val_acc: 0.5866\n",
      "Epoch 891/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9300 - acc: 0.5921 - val_loss: 2.0412 - val_acc: 0.5866\n",
      "Epoch 892/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9340 - acc: 0.5919 - val_loss: 2.0420 - val_acc: 0.5866\n",
      "Epoch 893/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9325 - acc: 0.5919 - val_loss: 2.0428 - val_acc: 0.5864\n",
      "Epoch 894/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9341 - acc: 0.5920 - val_loss: 2.0433 - val_acc: 0.5866\n",
      "Epoch 895/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9328 - acc: 0.5924 - val_loss: 2.0442 - val_acc: 0.5864\n",
      "Epoch 896/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9357 - acc: 0.5920 - val_loss: 2.0448 - val_acc: 0.5866\n",
      "Epoch 897/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9377 - acc: 0.5918 - val_loss: 2.0448 - val_acc: 0.5864\n",
      "Epoch 898/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9379 - acc: 0.5919 - val_loss: 2.0457 - val_acc: 0.5866\n",
      "Epoch 899/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9449 - acc: 0.5914 - val_loss: 2.0454 - val_acc: 0.5864\n",
      "Epoch 900/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9342 - acc: 0.5922 - val_loss: 2.0456 - val_acc: 0.5864\n",
      "Epoch 901/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9385 - acc: 0.5923 - val_loss: 2.0449 - val_acc: 0.5864\n",
      "Epoch 902/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9351 - acc: 0.5926 - val_loss: 2.0455 - val_acc: 0.5866\n",
      "Epoch 903/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9337 - acc: 0.5924 - val_loss: 2.0460 - val_acc: 0.5862\n",
      "Epoch 904/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9398 - acc: 0.5918 - val_loss: 2.0458 - val_acc: 0.5866\n",
      "Epoch 905/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9447 - acc: 0.5920 - val_loss: 2.0459 - val_acc: 0.5868\n",
      "Epoch 906/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9471 - acc: 0.5917 - val_loss: 2.0462 - val_acc: 0.5868\n",
      "Epoch 907/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9271 - acc: 0.5921 - val_loss: 2.0460 - val_acc: 0.5866\n",
      "Epoch 908/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9482 - acc: 0.5924 - val_loss: 2.0461 - val_acc: 0.5864\n",
      "Epoch 909/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9414 - acc: 0.5923 - val_loss: 2.0472 - val_acc: 0.5866\n",
      "Epoch 910/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9346 - acc: 0.5918 - val_loss: 2.0484 - val_acc: 0.5860\n",
      "Epoch 911/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9319 - acc: 0.5922 - val_loss: 2.0491 - val_acc: 0.5864\n",
      "Epoch 912/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9320 - acc: 0.5921 - val_loss: 2.0492 - val_acc: 0.5866\n",
      "Epoch 913/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9325 - acc: 0.5921 - val_loss: 2.0495 - val_acc: 0.5870\n",
      "Epoch 914/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9356 - acc: 0.5921 - val_loss: 2.0494 - val_acc: 0.5868\n",
      "Epoch 915/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9320 - acc: 0.5922 - val_loss: 2.0492 - val_acc: 0.5866\n",
      "Epoch 916/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9333 - acc: 0.5924 - val_loss: 2.0493 - val_acc: 0.5873\n",
      "Epoch 917/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9328 - acc: 0.5918 - val_loss: 2.0497 - val_acc: 0.5875\n",
      "Epoch 918/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 1.9398 - acc: 0.5913 - val_loss: 2.0496 - val_acc: 0.5879\n",
      "Epoch 919/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9340 - acc: 0.5917 - val_loss: 2.0484 - val_acc: 0.5873\n",
      "Epoch 920/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9355 - acc: 0.5923 - val_loss: 2.0474 - val_acc: 0.5873\n",
      "Epoch 921/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9341 - acc: 0.5919 - val_loss: 2.0467 - val_acc: 0.5868\n",
      "Epoch 922/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9388 - acc: 0.5921 - val_loss: 2.0466 - val_acc: 0.5873\n",
      "Epoch 923/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9407 - acc: 0.5920 - val_loss: 2.0449 - val_acc: 0.5868\n",
      "Epoch 924/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9323 - acc: 0.5916 - val_loss: 2.0446 - val_acc: 0.5868\n",
      "Epoch 925/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9309 - acc: 0.5921 - val_loss: 2.0445 - val_acc: 0.5866\n",
      "Epoch 926/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9328 - acc: 0.5924 - val_loss: 2.0447 - val_acc: 0.5866\n",
      "Epoch 927/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9267 - acc: 0.5919 - val_loss: 2.0450 - val_acc: 0.5864\n",
      "Epoch 928/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9392 - acc: 0.5924 - val_loss: 2.0452 - val_acc: 0.5866\n",
      "Epoch 929/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9343 - acc: 0.5924 - val_loss: 2.0455 - val_acc: 0.5875\n",
      "Epoch 930/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9396 - acc: 0.5917 - val_loss: 2.0458 - val_acc: 0.5864\n",
      "Epoch 931/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9464 - acc: 0.5919 - val_loss: 2.0450 - val_acc: 0.5870\n",
      "Epoch 932/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9334 - acc: 0.5917 - val_loss: 2.0443 - val_acc: 0.5866\n",
      "Epoch 933/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9347 - acc: 0.5924 - val_loss: 2.0427 - val_acc: 0.5864\n",
      "Epoch 934/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9363 - acc: 0.5922 - val_loss: 2.0413 - val_acc: 0.5862\n",
      "Epoch 935/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9338 - acc: 0.5924 - val_loss: 2.0398 - val_acc: 0.5860\n",
      "Epoch 936/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9325 - acc: 0.5926 - val_loss: 2.0395 - val_acc: 0.5866\n",
      "Epoch 937/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9362 - acc: 0.5922 - val_loss: 2.0388 - val_acc: 0.5864\n",
      "Epoch 938/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9332 - acc: 0.5924 - val_loss: 2.0385 - val_acc: 0.5866\n",
      "Epoch 939/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9497 - acc: 0.5922 - val_loss: 2.0389 - val_acc: 0.5864\n",
      "Epoch 940/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9287 - acc: 0.5925 - val_loss: 2.0394 - val_acc: 0.5866\n",
      "Epoch 941/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9330 - acc: 0.5924 - val_loss: 2.0406 - val_acc: 0.5873\n",
      "Epoch 942/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9364 - acc: 0.5919 - val_loss: 2.0417 - val_acc: 0.5870\n",
      "Epoch 943/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9318 - acc: 0.5923 - val_loss: 2.0424 - val_acc: 0.5868\n",
      "Epoch 944/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9352 - acc: 0.5924 - val_loss: 2.0419 - val_acc: 0.5868\n",
      "Epoch 945/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9402 - acc: 0.5921 - val_loss: 2.0418 - val_acc: 0.5870\n",
      "Epoch 946/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9352 - acc: 0.5919 - val_loss: 2.0407 - val_acc: 0.5868\n",
      "Epoch 947/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9296 - acc: 0.5922 - val_loss: 2.0403 - val_acc: 0.5870\n",
      "Epoch 948/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9377 - acc: 0.5924 - val_loss: 2.0405 - val_acc: 0.5870\n",
      "Epoch 949/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9336 - acc: 0.5916 - val_loss: 2.0401 - val_acc: 0.5866\n",
      "Epoch 950/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9339 - acc: 0.5919 - val_loss: 2.0403 - val_acc: 0.5868\n",
      "Epoch 951/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9266 - acc: 0.5925 - val_loss: 2.0402 - val_acc: 0.5864\n",
      "Epoch 952/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9353 - acc: 0.5919 - val_loss: 2.0402 - val_acc: 0.5866\n",
      "Epoch 953/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9371 - acc: 0.5921 - val_loss: 2.0402 - val_acc: 0.5870\n",
      "Epoch 954/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9284 - acc: 0.5928 - val_loss: 2.0400 - val_acc: 0.5870\n",
      "Epoch 955/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9469 - acc: 0.5923 - val_loss: 2.0403 - val_acc: 0.5879\n",
      "Epoch 956/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9342 - acc: 0.5920 - val_loss: 2.0414 - val_acc: 0.5868\n",
      "Epoch 957/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9337 - acc: 0.5919 - val_loss: 2.0419 - val_acc: 0.5868\n",
      "Epoch 958/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9304 - acc: 0.5923 - val_loss: 2.0428 - val_acc: 0.5873\n",
      "Epoch 959/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9360 - acc: 0.5920 - val_loss: 2.0425 - val_acc: 0.5875\n",
      "Epoch 960/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9305 - acc: 0.5928 - val_loss: 2.0420 - val_acc: 0.5873\n",
      "Epoch 961/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9303 - acc: 0.5921 - val_loss: 2.0416 - val_acc: 0.5870\n",
      "Epoch 962/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9343 - acc: 0.5919 - val_loss: 2.0414 - val_acc: 0.5873\n",
      "Epoch 963/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9282 - acc: 0.5925 - val_loss: 2.0416 - val_acc: 0.5870\n",
      "Epoch 964/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9398 - acc: 0.5925 - val_loss: 2.0420 - val_acc: 0.5870\n",
      "Epoch 965/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9347 - acc: 0.5922 - val_loss: 2.0427 - val_acc: 0.5870\n",
      "Epoch 966/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9355 - acc: 0.5923 - val_loss: 2.0429 - val_acc: 0.5868\n",
      "Epoch 967/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9301 - acc: 0.5922 - val_loss: 2.0434 - val_acc: 0.5870\n",
      "Epoch 968/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9432 - acc: 0.5927 - val_loss: 2.0436 - val_acc: 0.5870\n",
      "Epoch 969/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9340 - acc: 0.5923 - val_loss: 2.0436 - val_acc: 0.5866\n",
      "Epoch 970/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9349 - acc: 0.5920 - val_loss: 2.0444 - val_acc: 0.5870\n",
      "Epoch 971/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9354 - acc: 0.5923 - val_loss: 2.0453 - val_acc: 0.5870\n",
      "Epoch 972/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9358 - acc: 0.5922 - val_loss: 2.0465 - val_acc: 0.5875\n",
      "Epoch 973/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9361 - acc: 0.5924 - val_loss: 2.0477 - val_acc: 0.5870\n",
      "Epoch 974/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9419 - acc: 0.5924 - val_loss: 2.0478 - val_acc: 0.5870\n",
      "Epoch 975/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9548 - acc: 0.5922 - val_loss: 2.0485 - val_acc: 0.5875\n",
      "Epoch 976/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9322 - acc: 0.5923 - val_loss: 2.0490 - val_acc: 0.5870\n",
      "Epoch 977/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9340 - acc: 0.5925 - val_loss: 2.0490 - val_acc: 0.5873\n",
      "Epoch 978/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9289 - acc: 0.5921 - val_loss: 2.0479 - val_acc: 0.5870\n",
      "Epoch 979/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9307 - acc: 0.5924 - val_loss: 2.0472 - val_acc: 0.5870\n",
      "Epoch 980/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9363 - acc: 0.5920 - val_loss: 2.0478 - val_acc: 0.5870\n",
      "Epoch 981/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9418 - acc: 0.5924 - val_loss: 2.0477 - val_acc: 0.5868\n",
      "Epoch 982/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9350 - acc: 0.5923 - val_loss: 2.0481 - val_acc: 0.5868\n",
      "Epoch 983/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9327 - acc: 0.5922 - val_loss: 2.0492 - val_acc: 0.5870\n",
      "Epoch 984/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9307 - acc: 0.5925 - val_loss: 2.0496 - val_acc: 0.5868\n",
      "Epoch 985/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9319 - acc: 0.5929 - val_loss: 2.0488 - val_acc: 0.5866\n",
      "Epoch 986/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9338 - acc: 0.5924 - val_loss: 2.0486 - val_acc: 0.5864\n",
      "Epoch 987/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9367 - acc: 0.5922 - val_loss: 2.0485 - val_acc: 0.5866\n",
      "Epoch 988/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9278 - acc: 0.5923 - val_loss: 2.0481 - val_acc: 0.5866\n",
      "Epoch 989/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9307 - acc: 0.5922 - val_loss: 2.0483 - val_acc: 0.5870\n",
      "Epoch 990/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9307 - acc: 0.5925 - val_loss: 2.0484 - val_acc: 0.5868\n",
      "Epoch 991/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9306 - acc: 0.5924 - val_loss: 2.0484 - val_acc: 0.5870\n",
      "Epoch 992/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9411 - acc: 0.5931 - val_loss: 2.0475 - val_acc: 0.5868\n",
      "Epoch 993/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9328 - acc: 0.5926 - val_loss: 2.0464 - val_acc: 0.5868\n",
      "Epoch 994/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9320 - acc: 0.5927 - val_loss: 2.0462 - val_acc: 0.5866\n",
      "Epoch 995/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9292 - acc: 0.5932 - val_loss: 2.0458 - val_acc: 0.5868\n",
      "Epoch 996/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9283 - acc: 0.5918 - val_loss: 2.0460 - val_acc: 0.5875\n",
      "Epoch 997/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9365 - acc: 0.5926 - val_loss: 2.0458 - val_acc: 0.5868\n",
      "Epoch 998/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9329 - acc: 0.5924 - val_loss: 2.0446 - val_acc: 0.5864\n",
      "Epoch 999/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9287 - acc: 0.5923 - val_loss: 2.0440 - val_acc: 0.5868\n",
      "Epoch 1000/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9373 - acc: 0.5925 - val_loss: 2.0430 - val_acc: 0.5868\n",
      "Model: \"trans_race\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Encoder)            multiple                  144488    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             multiple                  2210      \n",
      "=================================================================\n",
      "Total params: 146,698\n",
      "Trainable params: 146,698\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "trans_race.compile(\n",
    "    optimizer=opt,\n",
    "    loss=loss,#'categorical_crossentropy', # 'sigmoid_focal_crossentropy'\n",
    "    metrics=['accuracy'], #['categorical_accuracy']\n",
    ")\n",
    "\n",
    "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath='../models/results/transformer.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "history = trans_race.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=1000,\n",
    "    verbose=True, # hide the output because we have so many epochs\n",
    "    callbacks=[model_checkpoint_callback]\n",
    ")\n",
    "print(trans_race.summary())\n",
    "# trans_race.save_weights(\"../models/results/transformer.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f4bd4034940>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot \n",
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "# for loss\n",
    "axL.plot(history.history['loss'],label=\"loss for training\")\n",
    "axL.plot(history.history['val_loss'],label=\"loss for validation\")\n",
    "axL.set_title('model loss')\n",
    "axL.set_xlabel('epoch')\n",
    "axL.set_ylabel('loss')\n",
    "axL.legend(loc='upper right')\n",
    "axR.plot(history.history['acc'],label=\"acc for training\")\n",
    "axR.plot(history.history['val_acc'],label=\"acc for validation\")\n",
    "axR.set_title('model accuracy')\n",
    "axR.set_xlabel('epoch')\n",
    "axR.set_ylabel('accuracy')\n",
    "axR.legend(loc='lower right')\n",
    "# figureの保存\n",
    "# plt.savefig(\"../models/results/trans_race_result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "preds = trans_race.predict(X_valid)\n",
    "y_pred = np.argmax(preds, axis = 2)\n",
    "y_ans = np.argmax(y_valid, axis = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def order_algorithm(preds):\n",
    "#     num_race = preds.shape[0]\n",
    "#     y_preds = np.full((num_race, 24), 25)\n",
    "#     for i in range(num_race):\n",
    "#         one_race = preds[i,:,:]\n",
    "#         init_preds = np.argmax(one_race, axis = -1)\n",
    "#         exist_horse = np.delete(one_race, np.where(init_preds == 25)[0], 0)\n",
    "#         for j in range(1,exist_horse.shape[0]+1):\n",
    "#             one_order = np.argmax(exist_horse[:,j])\n",
    "#             for k in range(one_race.shape[0]):\n",
    "#                 if np.array_equal(one_race[k], exist_horse[one_order]):\n",
    "#                     y_preds[i][k] = j\n",
    "#                     exist_horse = np.delete(exist_horse, one_order, 0)\n",
    "#                     break\n",
    "#     return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.DataFrame(preds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def order_algorithm(preds):\n",
    "#     num_race = preds.shape[0]\n",
    "#     y_preds = np.full((num_race, 24), 25)\n",
    "#     for i in range(num_race): # iterate all race\n",
    "#         one_race = preds[i,:,:] # shape = (24, 26) ,so (num of horse, num of target 0-25)\n",
    "#         init_preds = np.argmax(one_race, axis = -1)\n",
    "#         exist_horse = np.delete(one_race, np.where(init_preds == 25)[0], 0) # shape = (num of exist horse, 26)\n",
    "#         for j in range(1,exist_horse.shape[0]+1): # iterate 1-num of exist horse\n",
    "#             one_order = np.argmax(exist_horse[:,j]) # this is a target order\n",
    "#             for k in range(one_race.shape[0]): # search the horse k = (0, 23)\n",
    "#                 if np.array_equal(one_race[k], exist_horse[one_order]):\n",
    "#                     y_preds[i][k] = j\n",
    "#                     exist_horse = np.delete(exist_horse, one_order, 0)\n",
    "#                     exist_horse[:,j+1] += exist_horse[:,j]\n",
    "#                     one_race[:,j+1] += one_race[:,j]\n",
    "#                     break\n",
    "#     return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202, 24)\n",
      "[ 4  1  6  8  3  2  5  9  7 10 25 25 25 25 25 25 25 25 25 25 25 25 25 25]\n"
     ]
    }
   ],
   "source": [
    "y_preds = order_algorithm(preds)\n",
    "print(y_preds.shape)\n",
    "print(y_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202, 24, 26)\n",
      "(202, 24)\n",
      "(202, 24)\n"
     ]
    }
   ],
   "source": [
    "print(preds.shape)\n",
    "print(y_preds.shape)\n",
    "print(y_ans.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11  4  5 12  8  1  2  9  6 10 25 25 25 25 25 25 25 25 25 25 25 25 25 25]\n",
      "[ 4  1  6  8  3  2  5  9  7 10 25 25 25 25 25 25 25 25 25 25 25 25 25 25]\n",
      "[ 4  1  6  8  1  1  2 10  8 10 25 25 25 25 25 25 25 25 25 25 25 25 25 25]\n"
     ]
    }
   ],
   "source": [
    "print(y_ans[0])\n",
    "print(y_preds[0])\n",
    "print(y_pred[0])\n",
    "# print(np.mean(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  14.05940594059406\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(y_preds.shape[0]):\n",
    "    for j in range(y_preds.shape[1]):\n",
    "        if (y_preds[i][j] == y_ans[i][j]):\n",
    "            correct += 1\n",
    "accuracy = correct / len(y_pred)\n",
    "print(\"accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the distribution of prediction\n",
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "label = ['0','1','2','3','4','5','6','7','8','9',' 10',' 11',' 12',' 13',' 14',' 15',' 16',' 17',' 18',' 19',' 20',' 21',' 22',' 23',' 24']\n",
    "\n",
    "axL.hist(y_preds.flatten(), bins = 25, label = \"pred_order\")##, range = (1,21)\n",
    "axL.set_xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24])\n",
    "axL.set_xticklabels(label)\n",
    "axL.set_title('pred order distribution')\n",
    "axL.legend()\n",
    "axR.hist(y_ans.flatten(), bins = 25, label = \"ans_order\")##, range = (1,21)\n",
    "axR.set_xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24])\n",
    "axR.set_xticklabels(label)\n",
    "axR.set_title('ans order distribution')\n",
    "axR.legend()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_f:  202\n",
      "correct_first:  98\n",
      "precision:  0.48514851485148514\n"
     ]
    }
   ],
   "source": [
    "# precision = TP / (TP + FP)\n",
    "# the accuracy of predected True\n",
    "i = 0\n",
    "correct_first = 0\n",
    "all_f = 0\n",
    "increase = 0\n",
    "# X_valid_inv = standard_scale.inverse_transform(X_valid)\n",
    "# X_valid_inv_df = pd.DataFrame(X_valid_inv)\n",
    "# odds = X_test_inv_df['odds'].values\n",
    "# hit_odds = []\n",
    "# select = []\n",
    "\n",
    "for i in range(y_preds.shape[0]):\n",
    "    for j in range(y_preds.shape[1]):\n",
    "        if (y_preds[i][j] == 1):  # total nubber of predicted 1st (TP + FP)  & (pred[i][1]*odds[i] > 1.0)) | (pred[i][1]> 0.25)\n",
    "            all_f += 1\n",
    "            if (y_ans[i][j] == 1) or (y_ans[i][j] == 2) or (y_ans[i][j] == 3):\n",
    "                correct_first += 1   #　True Positive\n",
    "            \n",
    "# for i in range(len(y_ans)):\n",
    "#     if (y_preds[i] == 1):  # total nubber of predicted 1st (TP + FP)  & (pred[i][1]*odds[i] > 1.0)) | (pred[i][1]> 0.25)\n",
    "#         all_f = all_f + 1\n",
    "#         if (y_ans[i] == 1):\n",
    "#             correct_first = correct_first + 1   #　True Positive\n",
    "# #             increase += odds[i]\n",
    "# #             hit_odds.append(odds[i])\n",
    "            \n",
    "            \n",
    "            \n",
    "print(\"all_f: \", all_f)\n",
    "print(\"correct_first: \", correct_first)\n",
    "# print(\"hit odds average: \", np.array(hit_odds).mean())\n",
    "# print(\"spent money:\", all_f * 100)\n",
    "# revenue = (increase - all_f) * 100\n",
    "# retrive = increase / all_f\n",
    " \n",
    "# print(\"retrive rate: \", retrive) \n",
    "# print(\"revenue: \", revenue)\n",
    "precision = correct_first / all_f\n",
    "print(\"precision: \",precision)\n",
    "#print(\"\\n\".join(map(str,hit_odds)))\n",
    "# print(\"min: \", min(hit_odds))\n",
    "# print(\"mid: \", np.median(np.array(hit_odds)))\n",
    "# print(\"max: \", max(hit_odds))\n",
    "\n",
    "# fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "# axL.hist(hit_odds, bins = 50, label = \"hit_odds\")\n",
    "# axL.set_title('hit odds distribution')\n",
    "# axL.legend()\n",
    "# axR.hist(odds, bins = 50, label = \"odds\", range = (0,40))\n",
    "# axR.set_title('all odds distribution')\n",
    "# axR.legend()\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.2543352601156069\n"
     ]
    }
   ],
   "source": [
    "# Recall = TP / (TP + FN)\n",
    "# the accuracy of label True\n",
    "i = 0\n",
    "correct_first = 0\n",
    "all_f = 0\n",
    "# odds_f = []\n",
    "p_rate_f = []\n",
    "\n",
    "# all_f_odds = []\n",
    "\n",
    "for i in range(y_preds.shape[0]):\n",
    "    for j in range(y_preds.shape[1]):\n",
    "        if (y_ans[i][j] == 1):  # total nubber of predicted 1st (TP + FP)  & (pred[i][1]*odds[i] > 1.0)) | (pred[i][1]> 0.25)\n",
    "            all_f += 1\n",
    "            if (y_preds[i][j] == 1):\n",
    "                correct_first += 1   #　True Positive\n",
    "                \n",
    "                \n",
    "# for i in range(len(y_ans)):\n",
    "#     if (y_ans[i] == 1):  # TP + FN\n",
    "#         all_f = all_f + 1\n",
    "# #         all_f_odds.append(odds[i])\n",
    "#         if (y_preds[i] == 1):\n",
    "#             correct_first = correct_first + 1   #　TP\n",
    "#             odds_f.append(odds[i])\n",
    "#             p_rate_f.append(pred[i][1])\n",
    "\n",
    "# fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "# axL.scatter(p_rate_f, odds_f)  \n",
    "# axL.set_title('correlation odss and prediction')\n",
    "# #axL.xlabel('prediction rate first')\n",
    "# #axL.ylabel('odds')\n",
    "# axR.hist(odds_f, bins = 50, label = \"odds\")\n",
    "# axR.set_title('all first odds distribution')\n",
    "# axR.legend()\n",
    "\n",
    "# fig.show()\n",
    "Recall = correct_first / all_f\n",
    "print(\"Recall: \",Recall)\n",
    "# print(\"all_f_odds average: \", np.array(all_f_odds).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision of 3 renpuku:  0.06518151815181518\n"
     ]
    }
   ],
   "source": [
    "# 3 renpuku\n",
    "within_3 = 0\n",
    "hit = 0\n",
    "\n",
    "for i in range(y_preds.shape[0]):\n",
    "    for j in range(y_preds.shape[1]):\n",
    "        within_3 += 1\n",
    "        if (y_ans[i][j] == 1) or (y_ans[i][j] == 2) or (y_ans[i][j] == 3):  # total nubber of predicted 1st (TP + FP)  & (pred[i][1]*odds[i] > 1.0)) | (pred[i][1]> 0.25)\n",
    "            if (y_preds[i][j] == 1) or (y_preds[i][j] == 2) or (y_preds[i][j] == 3) or (y_preds[i][j] == 4):\n",
    "                hit += 1   #　True Positive\n",
    "                \n",
    "precision = hit / within_3\n",
    "print(\"precision of 3 renpuku: \", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
