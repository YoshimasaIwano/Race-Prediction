{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pprint\n",
    "# import sys\n",
    "\n",
    "# pprint.pprint(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fundamental libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/mnt/c/Users/44yos/RacePrediction/resnet_win5')\n",
    "from os import path\n",
    "import time\n",
    "\n",
    "# preporcessing libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt \n",
    "from pickle import dump\n",
    "\n",
    "# tesndorflow libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from tensorflow.python.keras import optimizers, callbacks\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "# from models import resnet\n",
    "from models import transformer\n",
    "\n",
    "# from utils import functions\n",
    "from utils import create_time_series_data, smooth_label, categorical_focal_loss, order_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race_id                        int64\n",
      "race_round                     int64\n",
      "ground_condition               int64\n",
      "total_horse_number             int64\n",
      "order                          int64\n",
      "frame_number                   int64\n",
      "horse_number                   int64\n",
      "age                            int64\n",
      "burden_weight                float64\n",
      "goal_time                    float64\n",
      "half_order                   float64\n",
      "last_time                    float64\n",
      "odds                         float64\n",
      "horse_weight                 float64\n",
      "pop                          float64\n",
      "race_rank                      int64\n",
      "distance                       int64\n",
      "ground_type_ダ                  int64\n",
      "ground_type_芝                  int64\n",
      "circle_右                       int64\n",
      "circle_左                       int64\n",
      "weather_circumstance_小雨        int64\n",
      "weather_circumstance_小雪        int64\n",
      "weather_circumstance_晴         int64\n",
      "weather_circumstance_曇         int64\n",
      "weather_circumstance_雨         int64\n",
      "weather_circumstance_雪         int64\n",
      "place_中京                       int64\n",
      "place_中山                       int64\n",
      "place_京都                       int64\n",
      "place_函館                       int64\n",
      "place_小倉                       int64\n",
      "place_新潟                       int64\n",
      "place_札幌                       int64\n",
      "place_東京                       int64\n",
      "place_福島                       int64\n",
      "place_阪神                       int64\n",
      "sex_セ                          int64\n",
      "sex_牝                          int64\n",
      "sex_牡                          int64\n",
      "horse_weight_dif             float64\n",
      "f_grass_win_rate             float64\n",
      "f_dart_win_rate              float64\n",
      "f_win_rate                   float64\n",
      "g_f_grass_win_rate           float64\n",
      "g_f_dart_win_rate            float64\n",
      "g_f_win_rate                 float64\n",
      "m_grass_win_rate             float64\n",
      "m_dart_win_rate              float64\n",
      "m_win_rate                   float64\n",
      "whole_horse_number_1         float64\n",
      "odds_1                       float64\n",
      "order_1                        int64\n",
      "burden_weight_1              float64\n",
      "race_distance_1              float64\n",
      "ground_condition_1           float64\n",
      "goal_time_1                  float64\n",
      "half_order_1                 float64\n",
      "last_time_1                  float64\n",
      "horse_weight_1               float64\n",
      "weather_circumstance_小雨_1    float64\n",
      "weather_circumstance_小雪_1    float64\n",
      "weather_circumstance_晴_1     float64\n",
      "weather_circumstance_曇_1     float64\n",
      "weather_circumstance_雨_1     float64\n",
      "weather_circumstance_雪_1     float64\n",
      "main_place_その他_1             float64\n",
      "main_place_中京_1              float64\n",
      "main_place_中山_1              float64\n",
      "main_place_京都_1              float64\n",
      "main_place_函館_1              float64\n",
      "main_place_小倉_1              float64\n",
      "main_place_新潟_1              float64\n",
      "main_place_札幌_1              float64\n",
      "main_place_東京_1              float64\n",
      "main_place_福島_1              float64\n",
      "main_place_阪神_1              float64\n",
      "race_rank_1                  float64\n",
      "ground_type_ダ_1              float64\n",
      "ground_type_芝_1              float64\n",
      "ground_type_障_1              float64\n",
      "horse_weight_dif_1           float64\n",
      "same_jockey_1                float64\n",
      "whole_horse_number_2         float64\n",
      "odds_2                       float64\n",
      "order_2                        int64\n",
      "burden_weight_2              float64\n",
      "race_distance_2              float64\n",
      "ground_condition_2           float64\n",
      "goal_time_2                  float64\n",
      "half_order_2                 float64\n",
      "last_time_2                  float64\n",
      "horse_weight_2               float64\n",
      "weather_circumstance_小雨_2    float64\n",
      "weather_circumstance_小雪_2    float64\n",
      "weather_circumstance_晴_2     float64\n",
      "weather_circumstance_曇_2     float64\n",
      "weather_circumstance_雨_2     float64\n",
      "weather_circumstance_雪_2     float64\n",
      "main_place_その他_2             float64\n",
      "main_place_中京_2              float64\n",
      "main_place_中山_2              float64\n",
      "main_place_京都_2              float64\n",
      "main_place_函館_2              float64\n",
      "main_place_小倉_2              float64\n",
      "main_place_新潟_2              float64\n",
      "main_place_札幌_2              float64\n",
      "main_place_東京_2              float64\n",
      "main_place_福島_2              float64\n",
      "main_place_阪神_2              float64\n",
      "race_rank_2                  float64\n",
      "ground_type_ダ_2              float64\n",
      "ground_type_芝_2              float64\n",
      "ground_type_障_2              float64\n",
      "horse_weight_dif_2           float64\n",
      "same_jockey_2                float64\n",
      "whole_horse_number_3         float64\n",
      "odds_3                       float64\n",
      "order_3                        int64\n",
      "burden_weight_3              float64\n",
      "race_distance_3              float64\n",
      "ground_condition_3           float64\n",
      "goal_time_3                  float64\n",
      "half_order_3                 float64\n",
      "last_time_3                  float64\n",
      "horse_weight_3               float64\n",
      "weather_circumstance_小雨_3    float64\n",
      "weather_circumstance_小雪_3    float64\n",
      "weather_circumstance_晴_3     float64\n",
      "weather_circumstance_曇_3     float64\n",
      "weather_circumstance_雨_3     float64\n",
      "weather_circumstance_雪_3     float64\n",
      "main_place_その他_3             float64\n",
      "main_place_中京_3              float64\n",
      "main_place_中山_3              float64\n",
      "main_place_京都_3              float64\n",
      "main_place_函館_3              float64\n",
      "main_place_小倉_3              float64\n",
      "main_place_新潟_3              float64\n",
      "main_place_札幌_3              float64\n",
      "main_place_東京_3              float64\n",
      "main_place_福島_3              float64\n",
      "main_place_阪神_3              float64\n",
      "race_rank_3                  float64\n",
      "ground_type_ダ_3              float64\n",
      "ground_type_芝_3              float64\n",
      "ground_type_障_3              float64\n",
      "horse_weight_dif_3           float64\n",
      "same_jockey_3                float64\n",
      "same_jockey                  float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# # load data\n",
    "data = pd.read_csv(\"../data/csv/data.csv\", sep = \",\")\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217032\n"
     ]
    }
   ],
   "source": [
    "data.dropna(inplace=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215967\n"
     ]
    }
   ],
   "source": [
    "data.drop_duplicates(inplace=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adjust columns type\n",
    "data['race_id'] = data['race_id'].astype(str)\n",
    "data['order'] = data['order'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete race day information\n",
    "data.drop([\"horse_number\", \"half_order\", \"goal_time\" ,\"last_time\", \"horse_weight\", \"horse_weight_dif\", \"frame_number\",\"pop\"], axis = 1, inplace=True)\n",
    "# \"race_round\",\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # standarlization \n",
    "# no_scale_data = data[['race_id','order']]\n",
    "# scale_columns = data.drop(['race_id','order'], axis=1).columns.values\n",
    "# standard_scale = StandardScaler()\n",
    "# data = pd.DataFrame(standard_scale.fit_transform(data[scale_columns]))\n",
    "\n",
    "# # PCA\n",
    "# pca = PCA()\n",
    "# data = pd.DataFrame(pca.fit_transform(data))\n",
    "# contrb_rate = pd.DataFrame(pca.explained_variance_ratio_, columns = ['rate'])\n",
    "# sum_rate = 0\n",
    "\n",
    "# #  # to get the colum of the specific contribution rate\n",
    "# # for i in range(len(contrb_rate)):\n",
    "# #     sum_rate += contrb_rate.rate[i]\n",
    "# #     if sum_rate >= 0.9:\n",
    "# #         max_col = i + 1\n",
    "# #         break\n",
    "\n",
    "max_col = 84\n",
    "# # print(max_col)\n",
    "# data = data.loc[:, :max_col-1]\n",
    "# print(data.shape[1])\n",
    "# # print(data.head(5))\n",
    "# # print(len(data), len(no_scale_data))\n",
    "# # print(no_scale_data[no_scale_data['race_id'].isnull()])\n",
    "# data = pd.concat([data, no_scale_data], axis=1)\n",
    "# dump(standard_scale, open(\"standard_scale.pkl\", \"wb\"))\n",
    "# dump(pca, open(\"pca.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sorted(no_scale_data['order'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.shape)\n",
    "# print(data.dtypes)\n",
    "# print(data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_time_series_data(raw_data):\n",
    "#     number_of_race = raw_data.race_id.nunique()\n",
    "#     time_series_data = np.full((number_of_race, 24, max_col), 0.0)#-float('inf')\n",
    "#     label = np.full((number_of_race, 24), 25)\n",
    "#     race_number = 0\n",
    "#     horse_number = 0\n",
    "#     for i in range(len(raw_data)):\n",
    "#         if i == 0:\n",
    "#             label[race_number][horse_number] = float(raw_data.iloc[i].order)\n",
    "#             time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "#             horse_number += 1\n",
    "#             continue\n",
    "#         # add new race\n",
    "#         if data.iloc[i].race_id != data.iloc[i-1].race_id:\n",
    "#             race_number += 1\n",
    "#             horse_number = 0\n",
    "#             label[race_number][horse_number] = float(raw_data.iloc[i].order)\n",
    "#             time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "#             horse_number += 1\n",
    "#         # add new horse to the same race\n",
    "#         else:\n",
    "# #             print(data.iloc[i].race_id ,race_number, horse_number)\n",
    "#             label[race_number][horse_number] = float(raw_data.iloc[i].order)\n",
    "#             time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "#             horse_number += 1\n",
    "#     del raw_data\n",
    "#     return time_series_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20132, 24, 84)\n",
      "(20132, 24)\n"
     ]
    }
   ],
   "source": [
    "# X, y_order = create_time_series_data(data)\n",
    "# np.save('X', X)\n",
    "# np.save('y_order', y_order)\n",
    "X = np.load('X.npy')\n",
    "y_order = np.load('y_order.npy')\n",
    "# del data\n",
    "print(X.shape)\n",
    "print(y_order.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "X = X.astype('float32')\n",
    "print(X.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[ 7 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25]\n"
     ]
    }
   ],
   "source": [
    "print(X[0][11])\n",
    "print(y_order[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26,)\n",
      "[0.00000000e+00 1.21687621e+00 1.21555368e+00 1.23063757e+00\n",
      " 1.24041898e+00 1.25990362e+00 1.26100846e+00 1.28155834e+00\n",
      " 1.32325490e+00 1.37157651e+00 1.45736210e+00 1.56413643e+00\n",
      " 1.73521807e+00 1.98052140e+00 2.27608819e+00 2.71833648e+00\n",
      " 3.58156912e+00 1.49347181e+01 2.00918164e+01 1.25825000e+03\n",
      " 1.43800000e+03 2.51650000e+03 3.35533333e+03 4.02640000e+03\n",
      " 5.03300000e+03 7.53440294e-02]\n"
     ]
    }
   ],
   "source": [
    "alpha = len(y_order) / pd.DataFrame(y_order.flatten()).value_counts()\n",
    "alpha = alpha.sort_index()\n",
    "alpha = np.array(alpha)\n",
    "alpha = np.append(0,alpha)\n",
    "print(alpha.shape)\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]]\n",
      "(20132, 24, 26)\n",
      "(20132, 24, 84)\n"
     ]
    }
   ],
   "source": [
    "# creating X,y (parameters and target)\n",
    "y = np_utils.to_categorical(y_order, dtype='float32')\n",
    "print(y[5])\n",
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def smooth_label(label, factor=0.03):\n",
    "#     # smooth label\n",
    "#     label *= (1 - factor)\n",
    "# #     label[:,:,1:4] += (factor / 3)\n",
    "\n",
    "#     for i in range(label.shape[0]):\n",
    "#         for j in range(label.shape[1]):\n",
    "#             t = np.where(label[i][j] == 1 - factor)\n",
    "#             label[i,j,max(0,t[0][0]-1):min(26,t[0][0]+2)] += (factor / 3)\n",
    "#     return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.   0.   0.   0.01 0.98 0.01 0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.01 0.98 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.97]]\n",
      "(20132, 24, 26)\n",
      "(20132, 24, 84)\n"
     ]
    }
   ],
   "source": [
    "y = smooth_label(y) \n",
    "print(y[4])\n",
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.race_id.value_counts().plot.hist(bins=25,range=(1,25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(pd.DataFrame(y_order.flatten()).plot.hist(bins=25))## ,ylim=(0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.01, random_state = 0)\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.0841513e+00  7.0771635e-02 -5.8418870e-01  2.4646287e-01\n",
      "  1.8285137e+00 -1.1262701e+00  6.5735537e-01  1.5329328e+00\n",
      "  6.2066919e-01 -6.7069030e-01  1.0873965e+00  1.7795300e-01\n",
      " -2.2035263e+00 -5.5455178e-01  1.5312845e+00  1.9849390e+00\n",
      " -1.9881687e+00 -4.5285341e-01 -1.1557992e+00  3.0845302e-01\n",
      "  6.5695131e-01 -1.6801572e+00  1.2509111e-01 -6.3480353e-01\n",
      "  2.6424465e-01 -1.8350914e+00  1.3959821e+00 -1.1001785e+00\n",
      " -2.4194989e+00  2.9623857e+00  1.4317343e+00  1.9310854e-02\n",
      "  3.4215423e-01  7.4966168e-01 -1.0088371e+00  8.0969352e-01\n",
      " -2.5986239e-01 -8.4669787e-01  1.0779321e+00  6.1363038e-02\n",
      " -1.5148355e+00 -1.8475902e-03 -8.8226789e-01 -6.8742210e-01\n",
      " -2.6793274e-01  1.6574528e+00 -2.0822718e+00 -1.7538213e+00\n",
      "  4.2922177e+00  1.6022534e+00 -4.4454589e-01 -6.7625672e-01\n",
      " -8.8914499e-02 -1.1668312e-01 -3.3394665e-01 -7.5860035e-01\n",
      "  3.7061727e-01 -6.4284933e-01  3.2509527e-01  4.8748016e-01\n",
      "  1.5954223e+00  1.4406800e-01 -8.7014019e-01  6.6753399e-01\n",
      " -1.0578674e+00  4.2397591e-01  6.7731267e-01  3.3324012e-01\n",
      " -1.4028546e+00 -1.2829390e+00 -9.2539579e-01  3.5360447e-01\n",
      " -5.5841304e-02  2.7825090e-01  1.1097189e-01  1.2788044e+00\n",
      "  4.5809287e-01  1.7082896e+00 -6.1947507e-01  1.6214646e+00\n",
      " -8.2937258e-01  6.4784966e-02  9.0706927e-01 -1.0770866e+00]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19930, 24, 84)\n",
      "(202, 24, 84)\n",
      "(19930, 24, 26)\n",
      "(202, 24, 26)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def categorical_focal_loss(alpha, gamma):\n",
    "#     \"\"\"\n",
    "#     Softmax version of focal loss.\n",
    "#     When there is a skew between different categories/labels in your data set, you can try to apply this function as a\n",
    "#     loss.\n",
    "#            m\n",
    "#       FL = ∑  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n",
    "#           c=1\n",
    "#       where m = number of classes, c = class and o = observation\n",
    "#     Parameters:\n",
    "#       alpha -- the same as weighing factor in balanced cross entropy. Alpha is used to specify the weight of different\n",
    "#       categories/labels, the size of the array needs to be consistent with the number of classes.\n",
    "#       gamma -- focusing parameter for modulating factor (1-p)\n",
    "#     Default value:\n",
    "#       gamma -- 2.0 as mentioned in the paper\n",
    "#       alpha -- 0.25 as mentioned in the paper\n",
    "#     References:\n",
    "#         Official paper: https://arxiv.org/pdf/1708.02002.pdf\n",
    "#         https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy\n",
    "#     Usage:\n",
    "#      model.compile(loss=[categorical_focal_loss(alpha=[[.25, .25, .25]], gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "#     \"\"\"\n",
    "\n",
    "#     alpha = np.array(alpha, dtype=np.float32)\n",
    "\n",
    "#     def categorical_focal_loss_fixed(y_true, y_pred):\n",
    "#         \"\"\"\n",
    "#         :param y_true: A tensor of the same shape as `y_pred`\n",
    "#         :param y_pred: A tensor resulting from a softmax\n",
    "#         :return: Output tensor.\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Clip the prediction value to prevent NaN's and Inf's\n",
    "#         epsilon = K.epsilon()\n",
    "#         y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "#         # Calculate Cross Entropy\n",
    "#         cross_entropy = -y_true * K.log(y_pred)\n",
    "\n",
    "#         # Calculate Focal Loss\n",
    "#         loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
    "\n",
    "#         # Compute mean loss in mini_batch\n",
    "#         return K.mean(K.sum(loss, axis=-1))\n",
    "\n",
    "#     return categorical_focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataset with batch size\n",
    "batch_size = 2048 # hyperparameter\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=19930).batch(batch_size)\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "valid_dataset = valid_dataset.shuffle(buffer_size=202).batch(batch_size)\n",
    "\n",
    "\n",
    "del X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters\n",
    "num_layers = 2 # hyperparameter\n",
    "d_model = max_col # 4*35 84=4*3*7\n",
    "num_heads = 28 # hyperparameter *must be a factor of d_model*\n",
    "d_ffn = 256 # hyperparameter\n",
    "pe_input = 24\n",
    "target_size = 26\n",
    "dropout_rate = 0.1 # hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# set up model\n",
    "trans_race = transformer.TransRace(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ffn=d_ffn,\n",
    "    pe_input=pe_input,\n",
    "    target_size=target_size,\n",
    "    rate=dropout_rate,\n",
    ")\n",
    "opt = optimizers.Adam(decay=0.01)\n",
    "loss = categorical_focal_loss(alpha=[alpha], gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method TransRace.call of <models.transformer.TransRace object at 0x7fb4400617f0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TransRace.call of <models.transformer.TransRace object at 0x7fb4400617f0>>, which Python reported as:\n",
      "    def call(self, inputs, training=True):\n",
      "        inp = inputs\n",
      "\n",
      "        enc_padding_mask = self.create_masks(inp) #, tar\n",
      "        # , look_ahead_mask, dec_padding_mask\n",
      "\n",
      "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
      "\n",
      "        final_output = self.final_layer(enc_output)\n",
      "#         print(\"final output shape\", final_output.shape)\n",
      "\n",
      "        return final_output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method TransRace.call of <models.transformer.TransRace object at 0x7fb4400617f0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TransRace.call of <models.transformer.TransRace object at 0x7fb4400617f0>>, which Python reported as:\n",
      "    def call(self, inputs, training=True):\n",
      "        inp = inputs\n",
      "\n",
      "        enc_padding_mask = self.create_masks(inp) #, tar\n",
      "        # , look_ahead_mask, dec_padding_mask\n",
      "\n",
      "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
      "\n",
      "        final_output = self.final_layer(enc_output)\n",
      "#         print(\"final output shape\", final_output.shape)\n",
      "\n",
      "        return final_output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fb442d62ba8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fb442d62ba8>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fb442d62ba8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fb442d62ba8>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fb442d62fd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fb442d62fd0>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fb442d62fd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fb442d62fd0>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fb442d62fd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fb442d62fd0>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fb442d62fd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fb442d62fd0>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fb442d62fd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fb442d62fd0>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fb442d62fd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fb442d62fd0>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fb442d80c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fb442d80c18>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fb442d80c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7fb442d80c18>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fb442d80c88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fb442d80c88>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fb442d80c88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fb442d80c88>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fb442d80c88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fb442d80c88>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fb442d80c88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fb442d80c88>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fb442d80c88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fb442d80c88>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fb442d80c88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7fb442d80c88>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:From /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 10 steps, validate on 1 steps\n",
      "Epoch 1/1000\n",
      "10/10 [==============================] - 10s 964ms/step - loss: 3.6672 - acc: 0.0703 - val_loss: 3.0827 - val_acc: 0.1130\n",
      "Epoch 2/1000\n",
      "10/10 [==============================] - 1s 138ms/step - loss: 3.0360 - acc: 0.1149 - val_loss: 2.9339 - val_acc: 0.1300\n",
      "Epoch 3/1000\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 2.8293 - acc: 0.1366 - val_loss: 2.5775 - val_acc: 0.2069\n",
      "Epoch 4/1000\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 2.7199 - acc: 0.1795 - val_loss: 2.5674 - val_acc: 0.2735\n",
      "Epoch 5/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.6580 - acc: 0.2027 - val_loss: 2.4643 - val_acc: 0.3055\n",
      "Epoch 6/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.6138 - acc: 0.2328 - val_loss: 2.4049 - val_acc: 0.3694\n",
      "Epoch 7/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.5553 - acc: 0.2675 - val_loss: 2.3850 - val_acc: 0.3899\n",
      "Epoch 8/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.5448 - acc: 0.2962 - val_loss: 2.3577 - val_acc: 0.4193\n",
      "Epoch 9/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.5301 - acc: 0.3285 - val_loss: 2.3708 - val_acc: 0.4410\n",
      "Epoch 10/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.4976 - acc: 0.3604 - val_loss: 2.3406 - val_acc: 0.4505\n",
      "Epoch 11/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.4481 - acc: 0.3837 - val_loss: 2.3178 - val_acc: 0.4618\n",
      "Epoch 12/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.4195 - acc: 0.3988 - val_loss: 2.3022 - val_acc: 0.4792\n",
      "Epoch 13/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.3724 - acc: 0.4249 - val_loss: 2.2964 - val_acc: 0.4845\n",
      "Epoch 14/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.3455 - acc: 0.4372 - val_loss: 2.2857 - val_acc: 0.4938\n",
      "Epoch 15/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.3571 - acc: 0.4519 - val_loss: 2.2687 - val_acc: 0.5027\n",
      "Epoch 16/1000\n",
      "10/10 [==============================] - 1s 130ms/step - loss: 2.3506 - acc: 0.4605 - val_loss: 2.2606 - val_acc: 0.5066\n",
      "Epoch 17/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.3284 - acc: 0.4723 - val_loss: 2.2613 - val_acc: 0.5165\n",
      "Epoch 18/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.3188 - acc: 0.4786 - val_loss: 2.2423 - val_acc: 0.5204\n",
      "Epoch 19/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.2940 - acc: 0.4876 - val_loss: 2.2384 - val_acc: 0.5227\n",
      "Epoch 20/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.2484 - acc: 0.4881 - val_loss: 2.2317 - val_acc: 0.5295\n",
      "Epoch 21/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.2730 - acc: 0.4985 - val_loss: 2.2219 - val_acc: 0.5270\n",
      "Epoch 22/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.2141 - acc: 0.5007 - val_loss: 2.2167 - val_acc: 0.5330\n",
      "Epoch 23/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.2348 - acc: 0.5058 - val_loss: 2.2116 - val_acc: 0.5351\n",
      "Epoch 24/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.2398 - acc: 0.5084 - val_loss: 2.1991 - val_acc: 0.5382\n",
      "Epoch 25/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.2263 - acc: 0.5134 - val_loss: 2.2093 - val_acc: 0.5396\n",
      "Epoch 26/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.1916 - acc: 0.5177 - val_loss: 2.1922 - val_acc: 0.5417\n",
      "Epoch 27/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.2024 - acc: 0.5179 - val_loss: 2.1813 - val_acc: 0.5415\n",
      "Epoch 28/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.1906 - acc: 0.5241 - val_loss: 2.1793 - val_acc: 0.5439\n",
      "Epoch 29/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.1908 - acc: 0.5229 - val_loss: 2.1764 - val_acc: 0.5433\n",
      "Epoch 30/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.1673 - acc: 0.5281 - val_loss: 2.1746 - val_acc: 0.5466\n",
      "Epoch 31/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.1626 - acc: 0.5279 - val_loss: 2.1684 - val_acc: 0.5468\n",
      "Epoch 32/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.1721 - acc: 0.5306 - val_loss: 2.1667 - val_acc: 0.5493\n",
      "Epoch 33/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.1653 - acc: 0.5314 - val_loss: 2.1632 - val_acc: 0.5487\n",
      "Epoch 34/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.1835 - acc: 0.5344 - val_loss: 2.1634 - val_acc: 0.5509\n",
      "Epoch 35/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.1855 - acc: 0.5372 - val_loss: 2.1586 - val_acc: 0.5524\n",
      "Epoch 36/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.1223 - acc: 0.5354 - val_loss: 2.1551 - val_acc: 0.5522\n",
      "Epoch 37/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.1447 - acc: 0.5382 - val_loss: 2.1491 - val_acc: 0.5520\n",
      "Epoch 38/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.1491 - acc: 0.5393 - val_loss: 2.1480 - val_acc: 0.5555\n",
      "Epoch 39/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 2.1323 - acc: 0.5416 - val_loss: 2.1514 - val_acc: 0.5561\n",
      "Epoch 40/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 2.1180 - acc: 0.5421 - val_loss: 2.1492 - val_acc: 0.5571\n",
      "Epoch 41/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.1264 - acc: 0.5429 - val_loss: 2.1475 - val_acc: 0.5582\n",
      "Epoch 42/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.1273 - acc: 0.5445 - val_loss: 2.1463 - val_acc: 0.5588\n",
      "Epoch 43/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.1222 - acc: 0.5457 - val_loss: 2.1442 - val_acc: 0.5600\n",
      "Epoch 44/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.1145 - acc: 0.5460 - val_loss: 2.1437 - val_acc: 0.5604\n",
      "Epoch 45/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.1195 - acc: 0.5457 - val_loss: 2.1364 - val_acc: 0.5617\n",
      "Epoch 46/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.1193 - acc: 0.5480 - val_loss: 2.1338 - val_acc: 0.5608\n",
      "Epoch 47/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.1104 - acc: 0.5483 - val_loss: 2.1332 - val_acc: 0.5619\n",
      "Epoch 48/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.1006 - acc: 0.5499 - val_loss: 2.1324 - val_acc: 0.5619\n",
      "Epoch 49/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.1096 - acc: 0.5506 - val_loss: 2.1317 - val_acc: 0.5642\n",
      "Epoch 50/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.1005 - acc: 0.5523 - val_loss: 2.1324 - val_acc: 0.5639\n",
      "Epoch 51/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 125ms/step - loss: 2.1018 - acc: 0.5518 - val_loss: 2.1334 - val_acc: 0.5654\n",
      "Epoch 52/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0809 - acc: 0.5530 - val_loss: 2.1268 - val_acc: 0.5652\n",
      "Epoch 53/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0985 - acc: 0.5531 - val_loss: 2.1252 - val_acc: 0.5660\n",
      "Epoch 54/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.0893 - acc: 0.5541 - val_loss: 2.1230 - val_acc: 0.5666\n",
      "Epoch 55/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0948 - acc: 0.5546 - val_loss: 2.1249 - val_acc: 0.5670\n",
      "Epoch 56/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0780 - acc: 0.5542 - val_loss: 2.1238 - val_acc: 0.5668\n",
      "Epoch 57/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0823 - acc: 0.5561 - val_loss: 2.1243 - val_acc: 0.5687\n",
      "Epoch 58/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0963 - acc: 0.5564 - val_loss: 2.1239 - val_acc: 0.5685\n",
      "Epoch 59/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0909 - acc: 0.5568 - val_loss: 2.1246 - val_acc: 0.5708\n",
      "Epoch 60/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0924 - acc: 0.5579 - val_loss: 2.1175 - val_acc: 0.5720\n",
      "Epoch 61/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0723 - acc: 0.5585 - val_loss: 2.1160 - val_acc: 0.5718\n",
      "Epoch 62/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0629 - acc: 0.5575 - val_loss: 2.1148 - val_acc: 0.5726\n",
      "Epoch 63/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.0759 - acc: 0.5587 - val_loss: 2.1171 - val_acc: 0.5734\n",
      "Epoch 64/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.1080 - acc: 0.5598 - val_loss: 2.1208 - val_acc: 0.5743\n",
      "Epoch 65/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0667 - acc: 0.5601 - val_loss: 2.1186 - val_acc: 0.5743\n",
      "Epoch 66/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0625 - acc: 0.5602 - val_loss: 2.1164 - val_acc: 0.5751\n",
      "Epoch 67/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0572 - acc: 0.5610 - val_loss: 2.1185 - val_acc: 0.5757\n",
      "Epoch 68/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0750 - acc: 0.5605 - val_loss: 2.1184 - val_acc: 0.5743\n",
      "Epoch 69/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0624 - acc: 0.5615 - val_loss: 2.1144 - val_acc: 0.5751\n",
      "Epoch 70/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0848 - acc: 0.5617 - val_loss: 2.1158 - val_acc: 0.5755\n",
      "Epoch 71/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0788 - acc: 0.5621 - val_loss: 2.1208 - val_acc: 0.5753\n",
      "Epoch 72/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0796 - acc: 0.5624 - val_loss: 2.1133 - val_acc: 0.5749\n",
      "Epoch 73/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0481 - acc: 0.5631 - val_loss: 2.1088 - val_acc: 0.5743\n",
      "Epoch 74/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0609 - acc: 0.5634 - val_loss: 2.1076 - val_acc: 0.5749\n",
      "Epoch 75/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0839 - acc: 0.5635 - val_loss: 2.1064 - val_acc: 0.5753\n",
      "Epoch 76/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0524 - acc: 0.5639 - val_loss: 2.1068 - val_acc: 0.5763\n",
      "Epoch 77/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0716 - acc: 0.5644 - val_loss: 2.1048 - val_acc: 0.5747\n",
      "Epoch 78/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0389 - acc: 0.5645 - val_loss: 2.1046 - val_acc: 0.5749\n",
      "Epoch 79/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0776 - acc: 0.5641 - val_loss: 2.1051 - val_acc: 0.5757\n",
      "Epoch 80/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0621 - acc: 0.5651 - val_loss: 2.1045 - val_acc: 0.5761\n",
      "Epoch 81/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0515 - acc: 0.5660 - val_loss: 2.1049 - val_acc: 0.5761\n",
      "Epoch 82/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0357 - acc: 0.5650 - val_loss: 2.1056 - val_acc: 0.5761\n",
      "Epoch 83/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0511 - acc: 0.5654 - val_loss: 2.1057 - val_acc: 0.5759\n",
      "Epoch 84/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0517 - acc: 0.5664 - val_loss: 2.1060 - val_acc: 0.5749\n",
      "Epoch 85/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0381 - acc: 0.5659 - val_loss: 2.1059 - val_acc: 0.5767\n",
      "Epoch 86/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0496 - acc: 0.5673 - val_loss: 2.1065 - val_acc: 0.5763\n",
      "Epoch 87/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 2.0343 - acc: 0.5663 - val_loss: 2.1081 - val_acc: 0.5769\n",
      "Epoch 88/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0382 - acc: 0.5673 - val_loss: 2.1075 - val_acc: 0.5769\n",
      "Epoch 89/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0481 - acc: 0.5680 - val_loss: 2.1072 - val_acc: 0.5769\n",
      "Epoch 90/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0737 - acc: 0.5679 - val_loss: 2.1075 - val_acc: 0.5771\n",
      "Epoch 91/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0403 - acc: 0.5679 - val_loss: 2.1114 - val_acc: 0.5763\n",
      "Epoch 92/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0401 - acc: 0.5676 - val_loss: 2.1100 - val_acc: 0.5767\n",
      "Epoch 93/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0332 - acc: 0.5672 - val_loss: 2.1105 - val_acc: 0.5765\n",
      "Epoch 94/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0372 - acc: 0.5676 - val_loss: 2.1120 - val_acc: 0.5767\n",
      "Epoch 95/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0478 - acc: 0.5674 - val_loss: 2.1162 - val_acc: 0.5749\n",
      "Epoch 96/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0381 - acc: 0.5683 - val_loss: 2.1222 - val_acc: 0.5755\n",
      "Epoch 97/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0488 - acc: 0.5685 - val_loss: 2.1228 - val_acc: 0.5769\n",
      "Epoch 98/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0295 - acc: 0.5685 - val_loss: 2.1274 - val_acc: 0.5763\n",
      "Epoch 99/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0529 - acc: 0.5691 - val_loss: 2.1314 - val_acc: 0.5776\n",
      "Epoch 100/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0281 - acc: 0.5706 - val_loss: 2.1311 - val_acc: 0.5778\n",
      "Epoch 101/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0168 - acc: 0.5695 - val_loss: 2.1332 - val_acc: 0.5774\n",
      "Epoch 102/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0350 - acc: 0.5693 - val_loss: 2.1411 - val_acc: 0.5771\n",
      "Epoch 103/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0494 - acc: 0.5695 - val_loss: 2.1397 - val_acc: 0.5765\n",
      "Epoch 104/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0248 - acc: 0.5699 - val_loss: 2.1301 - val_acc: 0.5771\n",
      "Epoch 105/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0633 - acc: 0.5709 - val_loss: 2.1208 - val_acc: 0.5769\n",
      "Epoch 106/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0462 - acc: 0.5703 - val_loss: 2.1167 - val_acc: 0.5765\n",
      "Epoch 107/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0249 - acc: 0.5710 - val_loss: 2.1177 - val_acc: 0.5771\n",
      "Epoch 108/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0257 - acc: 0.5707 - val_loss: 2.1209 - val_acc: 0.5769\n",
      "Epoch 109/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0271 - acc: 0.5715 - val_loss: 2.1220 - val_acc: 0.5782\n",
      "Epoch 110/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0202 - acc: 0.5710 - val_loss: 2.1244 - val_acc: 0.5771\n",
      "Epoch 111/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 2.0275 - acc: 0.5710 - val_loss: 2.1261 - val_acc: 0.5786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0162 - acc: 0.5718 - val_loss: 2.1252 - val_acc: 0.5778\n",
      "Epoch 113/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0673 - acc: 0.5721 - val_loss: 2.1314 - val_acc: 0.5780\n",
      "Epoch 114/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0226 - acc: 0.5713 - val_loss: 2.1289 - val_acc: 0.5778\n",
      "Epoch 115/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0282 - acc: 0.5726 - val_loss: 2.1260 - val_acc: 0.5778\n",
      "Epoch 116/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0152 - acc: 0.5723 - val_loss: 2.1259 - val_acc: 0.5776\n",
      "Epoch 117/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0302 - acc: 0.5727 - val_loss: 2.1244 - val_acc: 0.5784\n",
      "Epoch 118/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0340 - acc: 0.5725 - val_loss: 2.1212 - val_acc: 0.5780\n",
      "Epoch 119/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0283 - acc: 0.5727 - val_loss: 2.1185 - val_acc: 0.5788\n",
      "Epoch 120/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0322 - acc: 0.5731 - val_loss: 2.1163 - val_acc: 0.5784\n",
      "Epoch 121/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0342 - acc: 0.5730 - val_loss: 2.1139 - val_acc: 0.5778\n",
      "Epoch 122/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0120 - acc: 0.5738 - val_loss: 2.1126 - val_acc: 0.5782\n",
      "Epoch 123/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0131 - acc: 0.5730 - val_loss: 2.1140 - val_acc: 0.5784\n",
      "Epoch 124/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0239 - acc: 0.5734 - val_loss: 2.1150 - val_acc: 0.5786\n",
      "Epoch 125/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0126 - acc: 0.5735 - val_loss: 2.1159 - val_acc: 0.5794\n",
      "Epoch 126/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0169 - acc: 0.5734 - val_loss: 2.1174 - val_acc: 0.5788\n",
      "Epoch 127/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0245 - acc: 0.5730 - val_loss: 2.1198 - val_acc: 0.5792\n",
      "Epoch 128/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0108 - acc: 0.5741 - val_loss: 2.1176 - val_acc: 0.5792\n",
      "Epoch 129/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0160 - acc: 0.5744 - val_loss: 2.1157 - val_acc: 0.5788\n",
      "Epoch 130/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0024 - acc: 0.5737 - val_loss: 2.1120 - val_acc: 0.5794\n",
      "Epoch 131/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0163 - acc: 0.5744 - val_loss: 2.1103 - val_acc: 0.5790\n",
      "Epoch 132/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0129 - acc: 0.5738 - val_loss: 2.1098 - val_acc: 0.5790\n",
      "Epoch 133/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0177 - acc: 0.5744 - val_loss: 2.1099 - val_acc: 0.5796\n",
      "Epoch 134/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0119 - acc: 0.5745 - val_loss: 2.1101 - val_acc: 0.5796\n",
      "Epoch 135/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0200 - acc: 0.5745 - val_loss: 2.1115 - val_acc: 0.5796\n",
      "Epoch 136/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0296 - acc: 0.5745 - val_loss: 2.1128 - val_acc: 0.5790\n",
      "Epoch 137/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0116 - acc: 0.5747 - val_loss: 2.1097 - val_acc: 0.5804\n",
      "Epoch 138/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.0323 - acc: 0.5747 - val_loss: 2.1071 - val_acc: 0.5807\n",
      "Epoch 139/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0202 - acc: 0.5757 - val_loss: 2.1079 - val_acc: 0.5804\n",
      "Epoch 140/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0084 - acc: 0.5751 - val_loss: 2.1073 - val_acc: 0.5802\n",
      "Epoch 141/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0089 - acc: 0.5750 - val_loss: 2.1068 - val_acc: 0.5800\n",
      "Epoch 142/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0062 - acc: 0.5753 - val_loss: 2.1050 - val_acc: 0.5813\n",
      "Epoch 143/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0106 - acc: 0.5756 - val_loss: 2.1053 - val_acc: 0.5811\n",
      "Epoch 144/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0001 - acc: 0.5758 - val_loss: 2.1067 - val_acc: 0.5809\n",
      "Epoch 145/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0007 - acc: 0.5753 - val_loss: 2.1064 - val_acc: 0.5809\n",
      "Epoch 146/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0130 - acc: 0.5763 - val_loss: 2.1062 - val_acc: 0.5813\n",
      "Epoch 147/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0053 - acc: 0.5757 - val_loss: 2.1069 - val_acc: 0.5811\n",
      "Epoch 148/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0018 - acc: 0.5760 - val_loss: 2.1086 - val_acc: 0.5807\n",
      "Epoch 149/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.0092 - acc: 0.5759 - val_loss: 2.1069 - val_acc: 0.5815\n",
      "Epoch 150/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0097 - acc: 0.5753 - val_loss: 2.1054 - val_acc: 0.5807\n",
      "Epoch 151/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0315 - acc: 0.5769 - val_loss: 2.1025 - val_acc: 0.5804\n",
      "Epoch 152/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0128 - acc: 0.5768 - val_loss: 2.1014 - val_acc: 0.5802\n",
      "Epoch 153/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9939 - acc: 0.5766 - val_loss: 2.1027 - val_acc: 0.5798\n",
      "Epoch 154/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0049 - acc: 0.5761 - val_loss: 2.1048 - val_acc: 0.5798\n",
      "Epoch 155/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0135 - acc: 0.5770 - val_loss: 2.1042 - val_acc: 0.5804\n",
      "Epoch 156/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9975 - acc: 0.5768 - val_loss: 2.1041 - val_acc: 0.5800\n",
      "Epoch 157/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9971 - acc: 0.5769 - val_loss: 2.1020 - val_acc: 0.5798\n",
      "Epoch 158/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0181 - acc: 0.5772 - val_loss: 2.1018 - val_acc: 0.5802\n",
      "Epoch 159/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9966 - acc: 0.5767 - val_loss: 2.1011 - val_acc: 0.5809\n",
      "Epoch 160/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0021 - acc: 0.5770 - val_loss: 2.1007 - val_acc: 0.5804\n",
      "Epoch 161/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0012 - acc: 0.5766 - val_loss: 2.1040 - val_acc: 0.5798\n",
      "Epoch 162/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0151 - acc: 0.5782 - val_loss: 2.1039 - val_acc: 0.5817\n",
      "Epoch 163/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9961 - acc: 0.5768 - val_loss: 2.1026 - val_acc: 0.5815\n",
      "Epoch 164/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0025 - acc: 0.5777 - val_loss: 2.1012 - val_acc: 0.5813\n",
      "Epoch 165/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9895 - acc: 0.5776 - val_loss: 2.1030 - val_acc: 0.5815\n",
      "Epoch 166/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9948 - acc: 0.5770 - val_loss: 2.1028 - val_acc: 0.5827\n",
      "Epoch 167/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 2.0061 - acc: 0.5778 - val_loss: 2.1009 - val_acc: 0.5829\n",
      "Epoch 168/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9926 - acc: 0.5776 - val_loss: 2.0971 - val_acc: 0.5825\n",
      "Epoch 169/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9941 - acc: 0.5772 - val_loss: 2.0968 - val_acc: 0.5823\n",
      "Epoch 170/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0097 - acc: 0.5778 - val_loss: 2.0938 - val_acc: 0.5821\n",
      "Epoch 171/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0174 - acc: 0.5781 - val_loss: 2.0910 - val_acc: 0.5823\n",
      "Epoch 172/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0173 - acc: 0.5784 - val_loss: 2.0909 - val_acc: 0.5825\n",
      "Epoch 173/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0012 - acc: 0.5781 - val_loss: 2.0920 - val_acc: 0.5821\n",
      "Epoch 174/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9929 - acc: 0.5785 - val_loss: 2.0929 - val_acc: 0.5829\n",
      "Epoch 175/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9918 - acc: 0.5785 - val_loss: 2.0937 - val_acc: 0.5815\n",
      "Epoch 176/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0046 - acc: 0.5785 - val_loss: 2.0963 - val_acc: 0.5819\n",
      "Epoch 177/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9904 - acc: 0.5784 - val_loss: 2.0967 - val_acc: 0.5821\n",
      "Epoch 178/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9987 - acc: 0.5785 - val_loss: 2.0960 - val_acc: 0.5815\n",
      "Epoch 179/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9867 - acc: 0.5788 - val_loss: 2.0960 - val_acc: 0.5811\n",
      "Epoch 180/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9821 - acc: 0.5786 - val_loss: 2.0960 - val_acc: 0.5817\n",
      "Epoch 181/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9972 - acc: 0.5790 - val_loss: 2.0966 - val_acc: 0.5819\n",
      "Epoch 182/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9952 - acc: 0.5784 - val_loss: 2.0934 - val_acc: 0.5825\n",
      "Epoch 183/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9989 - acc: 0.5796 - val_loss: 2.0921 - val_acc: 0.5823\n",
      "Epoch 184/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9951 - acc: 0.5787 - val_loss: 2.0916 - val_acc: 0.5835\n",
      "Epoch 185/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9913 - acc: 0.5792 - val_loss: 2.0911 - val_acc: 0.5831\n",
      "Epoch 186/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0078 - acc: 0.5794 - val_loss: 2.0918 - val_acc: 0.5827\n",
      "Epoch 187/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0052 - acc: 0.5790 - val_loss: 2.0937 - val_acc: 0.5833\n",
      "Epoch 188/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0099 - acc: 0.5787 - val_loss: 2.0987 - val_acc: 0.5829\n",
      "Epoch 189/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9936 - acc: 0.5791 - val_loss: 2.1029 - val_acc: 0.5829\n",
      "Epoch 190/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9914 - acc: 0.5793 - val_loss: 2.1028 - val_acc: 0.5829\n",
      "Epoch 191/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9929 - acc: 0.5798 - val_loss: 2.1010 - val_acc: 0.5825\n",
      "Epoch 192/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0081 - acc: 0.5798 - val_loss: 2.0982 - val_acc: 0.5833\n",
      "Epoch 193/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9889 - acc: 0.5795 - val_loss: 2.0978 - val_acc: 0.5827\n",
      "Epoch 194/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9821 - acc: 0.5794 - val_loss: 2.0977 - val_acc: 0.5821\n",
      "Epoch 195/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9811 - acc: 0.5796 - val_loss: 2.0985 - val_acc: 0.5821\n",
      "Epoch 196/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9853 - acc: 0.5794 - val_loss: 2.0982 - val_acc: 0.5827\n",
      "Epoch 197/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9837 - acc: 0.5793 - val_loss: 2.0979 - val_acc: 0.5817\n",
      "Epoch 198/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9882 - acc: 0.5802 - val_loss: 2.0979 - val_acc: 0.5821\n",
      "Epoch 199/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9874 - acc: 0.5802 - val_loss: 2.0963 - val_acc: 0.5825\n",
      "Epoch 200/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9913 - acc: 0.5813 - val_loss: 2.0964 - val_acc: 0.5823\n",
      "Epoch 201/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9869 - acc: 0.5803 - val_loss: 2.0993 - val_acc: 0.5821\n",
      "Epoch 202/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9950 - acc: 0.5802 - val_loss: 2.1014 - val_acc: 0.5823\n",
      "Epoch 203/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9879 - acc: 0.5801 - val_loss: 2.0997 - val_acc: 0.5829\n",
      "Epoch 204/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 2.0049 - acc: 0.5800 - val_loss: 2.0999 - val_acc: 0.5833\n",
      "Epoch 205/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9922 - acc: 0.5799 - val_loss: 2.0983 - val_acc: 0.5825\n",
      "Epoch 206/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9859 - acc: 0.5799 - val_loss: 2.0972 - val_acc: 0.5831\n",
      "Epoch 207/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9797 - acc: 0.5807 - val_loss: 2.0971 - val_acc: 0.5827\n",
      "Epoch 208/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9842 - acc: 0.5807 - val_loss: 2.0954 - val_acc: 0.5837\n",
      "Epoch 209/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9895 - acc: 0.5805 - val_loss: 2.0941 - val_acc: 0.5840\n",
      "Epoch 210/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9866 - acc: 0.5809 - val_loss: 2.0933 - val_acc: 0.5831\n",
      "Epoch 211/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9862 - acc: 0.5805 - val_loss: 2.0919 - val_acc: 0.5831\n",
      "Epoch 212/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9870 - acc: 0.5813 - val_loss: 2.0910 - val_acc: 0.5833\n",
      "Epoch 213/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9862 - acc: 0.5810 - val_loss: 2.0904 - val_acc: 0.5840\n",
      "Epoch 214/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 1.9999 - acc: 0.5810 - val_loss: 2.0911 - val_acc: 0.5848\n",
      "Epoch 215/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9937 - acc: 0.5808 - val_loss: 2.0930 - val_acc: 0.5840\n",
      "Epoch 216/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9854 - acc: 0.5805 - val_loss: 2.0905 - val_acc: 0.5850\n",
      "Epoch 217/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9885 - acc: 0.5808 - val_loss: 2.0888 - val_acc: 0.5844\n",
      "Epoch 218/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9895 - acc: 0.5812 - val_loss: 2.0878 - val_acc: 0.5837\n",
      "Epoch 219/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9844 - acc: 0.5809 - val_loss: 2.0881 - val_acc: 0.5848\n",
      "Epoch 220/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9856 - acc: 0.5809 - val_loss: 2.0893 - val_acc: 0.5852\n",
      "Epoch 221/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9887 - acc: 0.5814 - val_loss: 2.0906 - val_acc: 0.5848\n",
      "Epoch 222/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9870 - acc: 0.5814 - val_loss: 2.0919 - val_acc: 0.5858\n",
      "Epoch 223/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9828 - acc: 0.5806 - val_loss: 2.0933 - val_acc: 0.5840\n",
      "Epoch 224/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0002 - acc: 0.5808 - val_loss: 2.0920 - val_acc: 0.5846\n",
      "Epoch 225/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9888 - acc: 0.5811 - val_loss: 2.0909 - val_acc: 0.5848\n",
      "Epoch 226/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9792 - acc: 0.5808 - val_loss: 2.0899 - val_acc: 0.5844\n",
      "Epoch 227/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9837 - acc: 0.5816 - val_loss: 2.0900 - val_acc: 0.5850\n",
      "Epoch 228/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9799 - acc: 0.5815 - val_loss: 2.0920 - val_acc: 0.5848\n",
      "Epoch 229/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0030 - acc: 0.5806 - val_loss: 2.0926 - val_acc: 0.5850\n",
      "Epoch 230/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9817 - acc: 0.5814 - val_loss: 2.0911 - val_acc: 0.5852\n",
      "Epoch 231/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9749 - acc: 0.5818 - val_loss: 2.0905 - val_acc: 0.5858\n",
      "Epoch 232/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9899 - acc: 0.5815 - val_loss: 2.0905 - val_acc: 0.5858\n",
      "Epoch 233/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9776 - acc: 0.5817 - val_loss: 2.0882 - val_acc: 0.5864\n",
      "Epoch 234/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9731 - acc: 0.5819 - val_loss: 2.0879 - val_acc: 0.5856\n",
      "Epoch 235/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9790 - acc: 0.5822 - val_loss: 2.0902 - val_acc: 0.5862\n",
      "Epoch 236/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9784 - acc: 0.5814 - val_loss: 2.0933 - val_acc: 0.5858\n",
      "Epoch 237/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9862 - acc: 0.5814 - val_loss: 2.0927 - val_acc: 0.5858\n",
      "Epoch 238/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9790 - acc: 0.5819 - val_loss: 2.0925 - val_acc: 0.5860\n",
      "Epoch 239/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9747 - acc: 0.5823 - val_loss: 2.0919 - val_acc: 0.5864\n",
      "Epoch 240/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9759 - acc: 0.5819 - val_loss: 2.0896 - val_acc: 0.5856\n",
      "Epoch 241/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9795 - acc: 0.5818 - val_loss: 2.0911 - val_acc: 0.5856\n",
      "Epoch 242/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9805 - acc: 0.5820 - val_loss: 2.0905 - val_acc: 0.5852\n",
      "Epoch 243/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9778 - acc: 0.5817 - val_loss: 2.0920 - val_acc: 0.5844\n",
      "Epoch 244/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9965 - acc: 0.5819 - val_loss: 2.0928 - val_acc: 0.5848\n",
      "Epoch 245/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9831 - acc: 0.5822 - val_loss: 2.0940 - val_acc: 0.5846\n",
      "Epoch 246/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9775 - acc: 0.5822 - val_loss: 2.0944 - val_acc: 0.5842\n",
      "Epoch 247/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9858 - acc: 0.5829 - val_loss: 2.0948 - val_acc: 0.5846\n",
      "Epoch 248/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9814 - acc: 0.5832 - val_loss: 2.0946 - val_acc: 0.5846\n",
      "Epoch 249/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9686 - acc: 0.5821 - val_loss: 2.0939 - val_acc: 0.5848\n",
      "Epoch 250/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9742 - acc: 0.5824 - val_loss: 2.0936 - val_acc: 0.5850\n",
      "Epoch 251/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9855 - acc: 0.5823 - val_loss: 2.0923 - val_acc: 0.5842\n",
      "Epoch 252/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9871 - acc: 0.5828 - val_loss: 2.0919 - val_acc: 0.5842\n",
      "Epoch 253/1000\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 1.9716 - acc: 0.5822 - val_loss: 2.0944 - val_acc: 0.5848\n",
      "Epoch 254/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 1.9756 - acc: 0.5828 - val_loss: 2.0958 - val_acc: 0.5844\n",
      "Epoch 255/1000\n",
      "10/10 [==============================] - 1s 132ms/step - loss: 1.9758 - acc: 0.5823 - val_loss: 2.0957 - val_acc: 0.5844\n",
      "Epoch 256/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9730 - acc: 0.5824 - val_loss: 2.0938 - val_acc: 0.5852\n",
      "Epoch 257/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9699 - acc: 0.5830 - val_loss: 2.0914 - val_acc: 0.5854\n",
      "Epoch 258/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9793 - acc: 0.5835 - val_loss: 2.0913 - val_acc: 0.5850\n",
      "Epoch 259/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9799 - acc: 0.5833 - val_loss: 2.0914 - val_acc: 0.5850\n",
      "Epoch 260/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 1.9774 - acc: 0.5827 - val_loss: 2.0910 - val_acc: 0.5848\n",
      "Epoch 261/1000\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 1.9766 - acc: 0.5823 - val_loss: 2.0917 - val_acc: 0.5846\n",
      "Epoch 262/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9776 - acc: 0.5828 - val_loss: 2.0914 - val_acc: 0.5856\n",
      "Epoch 263/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9791 - acc: 0.5839 - val_loss: 2.0931 - val_acc: 0.5856\n",
      "Epoch 264/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 1.9744 - acc: 0.5833 - val_loss: 2.0936 - val_acc: 0.5866\n",
      "Epoch 265/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9805 - acc: 0.5829 - val_loss: 2.0934 - val_acc: 0.5858\n",
      "Epoch 266/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9821 - acc: 0.5831 - val_loss: 2.0935 - val_acc: 0.5862\n",
      "Epoch 267/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 1.9673 - acc: 0.5831 - val_loss: 2.0928 - val_acc: 0.5862\n",
      "Epoch 268/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9662 - acc: 0.5838 - val_loss: 2.0929 - val_acc: 0.5866\n",
      "Epoch 269/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9710 - acc: 0.5834 - val_loss: 2.0941 - val_acc: 0.5868\n",
      "Epoch 270/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9668 - acc: 0.5833 - val_loss: 2.0941 - val_acc: 0.5868\n",
      "Epoch 271/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9710 - acc: 0.5836 - val_loss: 2.0927 - val_acc: 0.5866\n",
      "Epoch 272/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9740 - acc: 0.5834 - val_loss: 2.0931 - val_acc: 0.5866\n",
      "Epoch 273/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9771 - acc: 0.5833 - val_loss: 2.0927 - val_acc: 0.5873\n",
      "Epoch 274/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9700 - acc: 0.5838 - val_loss: 2.0935 - val_acc: 0.5870\n",
      "Epoch 275/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9774 - acc: 0.5838 - val_loss: 2.0945 - val_acc: 0.5866\n",
      "Epoch 276/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9787 - acc: 0.5834 - val_loss: 2.0937 - val_acc: 0.5866\n",
      "Epoch 277/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9758 - acc: 0.5832 - val_loss: 2.0938 - val_acc: 0.5870\n",
      "Epoch 278/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9827 - acc: 0.5833 - val_loss: 2.0915 - val_acc: 0.5862\n",
      "Epoch 279/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9716 - acc: 0.5837 - val_loss: 2.0901 - val_acc: 0.5864\n",
      "Epoch 280/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9712 - acc: 0.5838 - val_loss: 2.0908 - val_acc: 0.5866\n",
      "Epoch 281/1000\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 1.9694 - acc: 0.5837 - val_loss: 2.0921 - val_acc: 0.5866\n",
      "Epoch 282/1000\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 1.9779 - acc: 0.5831 - val_loss: 2.0924 - val_acc: 0.5873\n",
      "Epoch 283/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9779 - acc: 0.5836 - val_loss: 2.0915 - val_acc: 0.5864\n",
      "Epoch 284/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9726 - acc: 0.5839 - val_loss: 2.0904 - val_acc: 0.5862\n",
      "Epoch 285/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9697 - acc: 0.5839 - val_loss: 2.0895 - val_acc: 0.5864\n",
      "Epoch 286/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9801 - acc: 0.5837 - val_loss: 2.0908 - val_acc: 0.5868\n",
      "Epoch 287/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9747 - acc: 0.5841 - val_loss: 2.0914 - val_acc: 0.5868\n",
      "Epoch 288/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9794 - acc: 0.5838 - val_loss: 2.0918 - val_acc: 0.5862\n",
      "Epoch 289/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9740 - acc: 0.5834 - val_loss: 2.0907 - val_acc: 0.5873\n",
      "Epoch 290/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9682 - acc: 0.5838 - val_loss: 2.0903 - val_acc: 0.5868\n",
      "Epoch 291/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9628 - acc: 0.5841 - val_loss: 2.0893 - val_acc: 0.5870\n",
      "Epoch 292/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9746 - acc: 0.5846 - val_loss: 2.0878 - val_acc: 0.5862\n",
      "Epoch 293/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9660 - acc: 0.5839 - val_loss: 2.0877 - val_acc: 0.5866\n",
      "Epoch 294/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9785 - acc: 0.5844 - val_loss: 2.0853 - val_acc: 0.5873\n",
      "Epoch 295/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9754 - acc: 0.5844 - val_loss: 2.0843 - val_acc: 0.5862\n",
      "Epoch 296/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9694 - acc: 0.5843 - val_loss: 2.0843 - val_acc: 0.5860\n",
      "Epoch 297/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9707 - acc: 0.5845 - val_loss: 2.0853 - val_acc: 0.5856\n",
      "Epoch 298/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9656 - acc: 0.5840 - val_loss: 2.0860 - val_acc: 0.5856\n",
      "Epoch 299/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9868 - acc: 0.5844 - val_loss: 2.0866 - val_acc: 0.5858\n",
      "Epoch 300/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9686 - acc: 0.5845 - val_loss: 2.0848 - val_acc: 0.5862\n",
      "Epoch 301/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9785 - acc: 0.5845 - val_loss: 2.0853 - val_acc: 0.5856\n",
      "Epoch 302/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9643 - acc: 0.5845 - val_loss: 2.0852 - val_acc: 0.5860\n",
      "Epoch 303/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9662 - acc: 0.5844 - val_loss: 2.0847 - val_acc: 0.5866\n",
      "Epoch 304/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9682 - acc: 0.5838 - val_loss: 2.0846 - val_acc: 0.5862\n",
      "Epoch 305/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9694 - acc: 0.5840 - val_loss: 2.0859 - val_acc: 0.5858\n",
      "Epoch 306/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9669 - acc: 0.5851 - val_loss: 2.0860 - val_acc: 0.5860\n",
      "Epoch 307/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9641 - acc: 0.5846 - val_loss: 2.0863 - val_acc: 0.5860\n",
      "Epoch 308/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9694 - acc: 0.5837 - val_loss: 2.0849 - val_acc: 0.5856\n",
      "Epoch 309/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9637 - acc: 0.5844 - val_loss: 2.0837 - val_acc: 0.5858\n",
      "Epoch 310/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9707 - acc: 0.5848 - val_loss: 2.0833 - val_acc: 0.5860\n",
      "Epoch 311/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9755 - acc: 0.5849 - val_loss: 2.0855 - val_acc: 0.5858\n",
      "Epoch 312/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9710 - acc: 0.5847 - val_loss: 2.0850 - val_acc: 0.5862\n",
      "Epoch 313/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9701 - acc: 0.5852 - val_loss: 2.0835 - val_acc: 0.5864\n",
      "Epoch 314/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9636 - acc: 0.5845 - val_loss: 2.0817 - val_acc: 0.5854\n",
      "Epoch 315/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9883 - acc: 0.5847 - val_loss: 2.0812 - val_acc: 0.5856\n",
      "Epoch 316/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9619 - acc: 0.5847 - val_loss: 2.0813 - val_acc: 0.5856\n",
      "Epoch 317/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9649 - acc: 0.5847 - val_loss: 2.0816 - val_acc: 0.5856\n",
      "Epoch 318/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9643 - acc: 0.5854 - val_loss: 2.0814 - val_acc: 0.5854\n",
      "Epoch 319/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9692 - acc: 0.5856 - val_loss: 2.0810 - val_acc: 0.5854\n",
      "Epoch 320/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9724 - acc: 0.5852 - val_loss: 2.0804 - val_acc: 0.5856\n",
      "Epoch 321/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9766 - acc: 0.5853 - val_loss: 2.0788 - val_acc: 0.5852\n",
      "Epoch 322/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9802 - acc: 0.5849 - val_loss: 2.0787 - val_acc: 0.5854\n",
      "Epoch 323/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9615 - acc: 0.5858 - val_loss: 2.0790 - val_acc: 0.5858\n",
      "Epoch 324/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9712 - acc: 0.5847 - val_loss: 2.0780 - val_acc: 0.5852\n",
      "Epoch 325/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9670 - acc: 0.5850 - val_loss: 2.0762 - val_acc: 0.5854\n",
      "Epoch 326/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9651 - acc: 0.5857 - val_loss: 2.0744 - val_acc: 0.5852\n",
      "Epoch 327/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9790 - acc: 0.5852 - val_loss: 2.0728 - val_acc: 0.5864\n",
      "Epoch 328/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9620 - acc: 0.5857 - val_loss: 2.0737 - val_acc: 0.5858\n",
      "Epoch 329/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9690 - acc: 0.5855 - val_loss: 2.0729 - val_acc: 0.5866\n",
      "Epoch 330/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9616 - acc: 0.5853 - val_loss: 2.0728 - val_acc: 0.5858\n",
      "Epoch 331/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9739 - acc: 0.5853 - val_loss: 2.0721 - val_acc: 0.5854\n",
      "Epoch 332/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9697 - acc: 0.5849 - val_loss: 2.0726 - val_acc: 0.5860\n",
      "Epoch 333/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9663 - acc: 0.5854 - val_loss: 2.0737 - val_acc: 0.5860\n",
      "Epoch 334/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9664 - acc: 0.5859 - val_loss: 2.0735 - val_acc: 0.5864\n",
      "Epoch 335/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9633 - acc: 0.5855 - val_loss: 2.0735 - val_acc: 0.5866\n",
      "Epoch 336/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9727 - acc: 0.5865 - val_loss: 2.0739 - val_acc: 0.5858\n",
      "Epoch 337/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9601 - acc: 0.5849 - val_loss: 2.0756 - val_acc: 0.5860\n",
      "Epoch 338/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9627 - acc: 0.5858 - val_loss: 2.0751 - val_acc: 0.5870\n",
      "Epoch 339/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9646 - acc: 0.5850 - val_loss: 2.0744 - val_acc: 0.5862\n",
      "Epoch 340/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9753 - acc: 0.5853 - val_loss: 2.0743 - val_acc: 0.5860\n",
      "Epoch 341/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9694 - acc: 0.5857 - val_loss: 2.0735 - val_acc: 0.5860\n",
      "Epoch 342/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9823 - acc: 0.5857 - val_loss: 2.0745 - val_acc: 0.5864\n",
      "Epoch 343/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9611 - acc: 0.5851 - val_loss: 2.0767 - val_acc: 0.5862\n",
      "Epoch 344/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9715 - acc: 0.5865 - val_loss: 2.0786 - val_acc: 0.5870\n",
      "Epoch 345/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9601 - acc: 0.5859 - val_loss: 2.0800 - val_acc: 0.5862\n",
      "Epoch 346/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9606 - acc: 0.5857 - val_loss: 2.0803 - val_acc: 0.5866\n",
      "Epoch 347/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9678 - acc: 0.5854 - val_loss: 2.0798 - val_acc: 0.5858\n",
      "Epoch 348/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9717 - acc: 0.5851 - val_loss: 2.0789 - val_acc: 0.5862\n",
      "Epoch 349/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9682 - acc: 0.5855 - val_loss: 2.0793 - val_acc: 0.5860\n",
      "Epoch 350/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9546 - acc: 0.5863 - val_loss: 2.0794 - val_acc: 0.5864\n",
      "Epoch 351/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9628 - acc: 0.5861 - val_loss: 2.0804 - val_acc: 0.5864\n",
      "Epoch 352/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9790 - acc: 0.5860 - val_loss: 2.0808 - val_acc: 0.5864\n",
      "Epoch 353/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9734 - acc: 0.5858 - val_loss: 2.0801 - val_acc: 0.5864\n",
      "Epoch 354/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9628 - acc: 0.5860 - val_loss: 2.0799 - val_acc: 0.5866\n",
      "Epoch 355/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9622 - acc: 0.5855 - val_loss: 2.0779 - val_acc: 0.5866\n",
      "Epoch 356/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9728 - acc: 0.5860 - val_loss: 2.0757 - val_acc: 0.5862\n",
      "Epoch 357/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9589 - acc: 0.5857 - val_loss: 2.0751 - val_acc: 0.5864\n",
      "Epoch 358/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9654 - acc: 0.5865 - val_loss: 2.0759 - val_acc: 0.5862\n",
      "Epoch 359/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9621 - acc: 0.5866 - val_loss: 2.0752 - val_acc: 0.5862\n",
      "Epoch 360/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9585 - acc: 0.5867 - val_loss: 2.0755 - val_acc: 0.5866\n",
      "Epoch 361/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9622 - acc: 0.5863 - val_loss: 2.0765 - val_acc: 0.5868\n",
      "Epoch 362/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9585 - acc: 0.5863 - val_loss: 2.0781 - val_acc: 0.5862\n",
      "Epoch 363/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9600 - acc: 0.5864 - val_loss: 2.0790 - val_acc: 0.5870\n",
      "Epoch 364/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9632 - acc: 0.5862 - val_loss: 2.0794 - val_acc: 0.5868\n",
      "Epoch 365/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9772 - acc: 0.5862 - val_loss: 2.0801 - val_acc: 0.5873\n",
      "Epoch 366/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9665 - acc: 0.5861 - val_loss: 2.0802 - val_acc: 0.5870\n",
      "Epoch 367/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9596 - acc: 0.5862 - val_loss: 2.0802 - val_acc: 0.5877\n",
      "Epoch 368/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9691 - acc: 0.5868 - val_loss: 2.0803 - val_acc: 0.5877\n",
      "Epoch 369/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9575 - acc: 0.5863 - val_loss: 2.0810 - val_acc: 0.5870\n",
      "Epoch 370/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9580 - acc: 0.5863 - val_loss: 2.0792 - val_acc: 0.5873\n",
      "Epoch 371/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9648 - acc: 0.5858 - val_loss: 2.0766 - val_acc: 0.5877\n",
      "Epoch 372/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9704 - acc: 0.5862 - val_loss: 2.0756 - val_acc: 0.5873\n",
      "Epoch 373/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9684 - acc: 0.5860 - val_loss: 2.0774 - val_acc: 0.5873\n",
      "Epoch 374/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9584 - acc: 0.5863 - val_loss: 2.0781 - val_acc: 0.5870\n",
      "Epoch 375/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9702 - acc: 0.5860 - val_loss: 2.0796 - val_acc: 0.5866\n",
      "Epoch 376/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9628 - acc: 0.5861 - val_loss: 2.0798 - val_acc: 0.5866\n",
      "Epoch 377/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9632 - acc: 0.5865 - val_loss: 2.0799 - val_acc: 0.5868\n",
      "Epoch 378/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9568 - acc: 0.5862 - val_loss: 2.0781 - val_acc: 0.5875\n",
      "Epoch 379/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9571 - acc: 0.5865 - val_loss: 2.0784 - val_acc: 0.5875\n",
      "Epoch 380/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9596 - acc: 0.5865 - val_loss: 2.0794 - val_acc: 0.5870\n",
      "Epoch 381/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9626 - acc: 0.5864 - val_loss: 2.0808 - val_acc: 0.5875\n",
      "Epoch 382/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9646 - acc: 0.5868 - val_loss: 2.0813 - val_acc: 0.5875\n",
      "Epoch 383/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9598 - acc: 0.5867 - val_loss: 2.0816 - val_acc: 0.5870\n",
      "Epoch 384/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9546 - acc: 0.5865 - val_loss: 2.0813 - val_acc: 0.5875\n",
      "Epoch 385/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9552 - acc: 0.5865 - val_loss: 2.0805 - val_acc: 0.5875\n",
      "Epoch 386/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9538 - acc: 0.5869 - val_loss: 2.0813 - val_acc: 0.5875\n",
      "Epoch 387/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9688 - acc: 0.5869 - val_loss: 2.0808 - val_acc: 0.5866\n",
      "Epoch 388/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9621 - acc: 0.5865 - val_loss: 2.0799 - val_acc: 0.5870\n",
      "Epoch 389/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9616 - acc: 0.5869 - val_loss: 2.0796 - val_acc: 0.5877\n",
      "Epoch 390/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9611 - acc: 0.5867 - val_loss: 2.0797 - val_acc: 0.5870\n",
      "Epoch 391/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9526 - acc: 0.5865 - val_loss: 2.0802 - val_acc: 0.5873\n",
      "Epoch 392/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9719 - acc: 0.5869 - val_loss: 2.0821 - val_acc: 0.5873\n",
      "Epoch 393/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9585 - acc: 0.5874 - val_loss: 2.0817 - val_acc: 0.5875\n",
      "Epoch 394/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9544 - acc: 0.5869 - val_loss: 2.0829 - val_acc: 0.5870\n",
      "Epoch 395/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9548 - acc: 0.5875 - val_loss: 2.0842 - val_acc: 0.5873\n",
      "Epoch 396/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9568 - acc: 0.5867 - val_loss: 2.0847 - val_acc: 0.5873\n",
      "Epoch 397/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9551 - acc: 0.5868 - val_loss: 2.0837 - val_acc: 0.5870\n",
      "Epoch 398/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9622 - acc: 0.5867 - val_loss: 2.0822 - val_acc: 0.5873\n",
      "Epoch 399/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9554 - acc: 0.5866 - val_loss: 2.0805 - val_acc: 0.5868\n",
      "Epoch 400/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9599 - acc: 0.5868 - val_loss: 2.0782 - val_acc: 0.5868\n",
      "Epoch 401/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9736 - acc: 0.5866 - val_loss: 2.0762 - val_acc: 0.5866\n",
      "Epoch 402/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9591 - acc: 0.5869 - val_loss: 2.0765 - val_acc: 0.5870\n",
      "Epoch 403/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9547 - acc: 0.5876 - val_loss: 2.0757 - val_acc: 0.5866\n",
      "Epoch 404/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9554 - acc: 0.5868 - val_loss: 2.0765 - val_acc: 0.5864\n",
      "Epoch 405/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9613 - acc: 0.5868 - val_loss: 2.0774 - val_acc: 0.5868\n",
      "Epoch 406/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9538 - acc: 0.5866 - val_loss: 2.0768 - val_acc: 0.5862\n",
      "Epoch 407/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9696 - acc: 0.5870 - val_loss: 2.0759 - val_acc: 0.5860\n",
      "Epoch 408/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9589 - acc: 0.5870 - val_loss: 2.0764 - val_acc: 0.5864\n",
      "Epoch 409/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9521 - acc: 0.5868 - val_loss: 2.0764 - val_acc: 0.5864\n",
      "Epoch 410/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9521 - acc: 0.5872 - val_loss: 2.0764 - val_acc: 0.5860\n",
      "Epoch 411/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9562 - acc: 0.5875 - val_loss: 2.0759 - val_acc: 0.5864\n",
      "Epoch 412/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9633 - acc: 0.5877 - val_loss: 2.0756 - val_acc: 0.5864\n",
      "Epoch 413/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9630 - acc: 0.5870 - val_loss: 2.0734 - val_acc: 0.5868\n",
      "Epoch 414/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9575 - acc: 0.5871 - val_loss: 2.0729 - val_acc: 0.5868\n",
      "Epoch 415/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9551 - acc: 0.5871 - val_loss: 2.0732 - val_acc: 0.5868\n",
      "Epoch 416/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9564 - acc: 0.5873 - val_loss: 2.0741 - val_acc: 0.5870\n",
      "Epoch 417/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9588 - acc: 0.5873 - val_loss: 2.0745 - val_acc: 0.5870\n",
      "Epoch 418/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9487 - acc: 0.5870 - val_loss: 2.0747 - val_acc: 0.5860\n",
      "Epoch 419/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9509 - acc: 0.5871 - val_loss: 2.0748 - val_acc: 0.5858\n",
      "Epoch 420/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9551 - acc: 0.5867 - val_loss: 2.0759 - val_acc: 0.5862\n",
      "Epoch 421/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9554 - acc: 0.5874 - val_loss: 2.0767 - val_acc: 0.5870\n",
      "Epoch 422/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9563 - acc: 0.5871 - val_loss: 2.0766 - val_acc: 0.5866\n",
      "Epoch 423/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9517 - acc: 0.5877 - val_loss: 2.0765 - val_acc: 0.5864\n",
      "Epoch 424/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9511 - acc: 0.5871 - val_loss: 2.0762 - val_acc: 0.5868\n",
      "Epoch 425/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9585 - acc: 0.5876 - val_loss: 2.0760 - val_acc: 0.5873\n",
      "Epoch 426/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9582 - acc: 0.5879 - val_loss: 2.0758 - val_acc: 0.5866\n",
      "Epoch 427/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9570 - acc: 0.5877 - val_loss: 2.0761 - val_acc: 0.5866\n",
      "Epoch 428/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9509 - acc: 0.5873 - val_loss: 2.0771 - val_acc: 0.5864\n",
      "Epoch 429/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9535 - acc: 0.5874 - val_loss: 2.0779 - val_acc: 0.5864\n",
      "Epoch 430/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9564 - acc: 0.5872 - val_loss: 2.0785 - val_acc: 0.5866\n",
      "Epoch 431/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9637 - acc: 0.5868 - val_loss: 2.0788 - val_acc: 0.5866\n",
      "Epoch 432/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9540 - acc: 0.5873 - val_loss: 2.0783 - val_acc: 0.5868\n",
      "Epoch 433/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9521 - acc: 0.5873 - val_loss: 2.0774 - val_acc: 0.5868\n",
      "Epoch 434/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9606 - acc: 0.5873 - val_loss: 2.0777 - val_acc: 0.5866\n",
      "Epoch 435/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9629 - acc: 0.5880 - val_loss: 2.0794 - val_acc: 0.5864\n",
      "Epoch 436/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9560 - acc: 0.5876 - val_loss: 2.0782 - val_acc: 0.5864\n",
      "Epoch 437/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9602 - acc: 0.5873 - val_loss: 2.0757 - val_acc: 0.5860\n",
      "Epoch 438/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9543 - acc: 0.5877 - val_loss: 2.0732 - val_acc: 0.5860\n",
      "Epoch 439/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9575 - acc: 0.5878 - val_loss: 2.0728 - val_acc: 0.5862\n",
      "Epoch 440/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9653 - acc: 0.5880 - val_loss: 2.0731 - val_acc: 0.5866\n",
      "Epoch 441/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9520 - acc: 0.5882 - val_loss: 2.0741 - val_acc: 0.5868\n",
      "Epoch 442/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9552 - acc: 0.5877 - val_loss: 2.0725 - val_acc: 0.5870\n",
      "Epoch 443/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9512 - acc: 0.5881 - val_loss: 2.0728 - val_acc: 0.5870\n",
      "Epoch 444/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9490 - acc: 0.5879 - val_loss: 2.0733 - val_acc: 0.5870\n",
      "Epoch 445/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9520 - acc: 0.5881 - val_loss: 2.0736 - val_acc: 0.5870\n",
      "Epoch 446/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9592 - acc: 0.5877 - val_loss: 2.0739 - val_acc: 0.5870\n",
      "Epoch 447/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9517 - acc: 0.5874 - val_loss: 2.0738 - val_acc: 0.5866\n",
      "Epoch 448/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9551 - acc: 0.5882 - val_loss: 2.0735 - val_acc: 0.5870\n",
      "Epoch 449/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9519 - acc: 0.5881 - val_loss: 2.0731 - val_acc: 0.5870\n",
      "Epoch 450/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9529 - acc: 0.5879 - val_loss: 2.0733 - val_acc: 0.5870\n",
      "Epoch 451/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9491 - acc: 0.5881 - val_loss: 2.0742 - val_acc: 0.5870\n",
      "Epoch 452/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9561 - acc: 0.5879 - val_loss: 2.0742 - val_acc: 0.5870\n",
      "Epoch 453/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9573 - acc: 0.5878 - val_loss: 2.0747 - val_acc: 0.5875\n",
      "Epoch 454/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9522 - acc: 0.5877 - val_loss: 2.0755 - val_acc: 0.5875\n",
      "Epoch 455/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9542 - acc: 0.5878 - val_loss: 2.0770 - val_acc: 0.5873\n",
      "Epoch 456/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9558 - acc: 0.5874 - val_loss: 2.0771 - val_acc: 0.5873\n",
      "Epoch 457/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9524 - acc: 0.5877 - val_loss: 2.0771 - val_acc: 0.5870\n",
      "Epoch 458/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9501 - acc: 0.5880 - val_loss: 2.0784 - val_acc: 0.5870\n",
      "Epoch 459/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9541 - acc: 0.5882 - val_loss: 2.0792 - val_acc: 0.5873\n",
      "Epoch 460/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9553 - acc: 0.5879 - val_loss: 2.0788 - val_acc: 0.5868\n",
      "Epoch 461/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9481 - acc: 0.5884 - val_loss: 2.0793 - val_acc: 0.5870\n",
      "Epoch 462/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9636 - acc: 0.5878 - val_loss: 2.0807 - val_acc: 0.5870\n",
      "Epoch 463/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9484 - acc: 0.5880 - val_loss: 2.0807 - val_acc: 0.5870\n",
      "Epoch 464/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9481 - acc: 0.5878 - val_loss: 2.0808 - val_acc: 0.5873\n",
      "Epoch 465/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9602 - acc: 0.5884 - val_loss: 2.0807 - val_acc: 0.5875\n",
      "Epoch 466/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9590 - acc: 0.5882 - val_loss: 2.0799 - val_acc: 0.5873\n",
      "Epoch 467/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9535 - acc: 0.5886 - val_loss: 2.0794 - val_acc: 0.5873\n",
      "Epoch 468/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9520 - acc: 0.5882 - val_loss: 2.0793 - val_acc: 0.5875\n",
      "Epoch 469/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9469 - acc: 0.5882 - val_loss: 2.0797 - val_acc: 0.5870\n",
      "Epoch 470/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9462 - acc: 0.5886 - val_loss: 2.0796 - val_acc: 0.5868\n",
      "Epoch 471/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9473 - acc: 0.5885 - val_loss: 2.0784 - val_acc: 0.5870\n",
      "Epoch 472/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9465 - acc: 0.5888 - val_loss: 2.0774 - val_acc: 0.5873\n",
      "Epoch 473/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9632 - acc: 0.5879 - val_loss: 2.0778 - val_acc: 0.5875\n",
      "Epoch 474/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9534 - acc: 0.5881 - val_loss: 2.0778 - val_acc: 0.5875\n",
      "Epoch 475/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9507 - acc: 0.5881 - val_loss: 2.0776 - val_acc: 0.5875\n",
      "Epoch 476/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9644 - acc: 0.5882 - val_loss: 2.0779 - val_acc: 0.5875\n",
      "Epoch 477/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9474 - acc: 0.5883 - val_loss: 2.0777 - val_acc: 0.5879\n",
      "Epoch 478/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9485 - acc: 0.5883 - val_loss: 2.0768 - val_acc: 0.5873\n",
      "Epoch 479/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9467 - acc: 0.5883 - val_loss: 2.0756 - val_acc: 0.5873\n",
      "Epoch 480/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9497 - acc: 0.5882 - val_loss: 2.0766 - val_acc: 0.5870\n",
      "Epoch 481/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9532 - acc: 0.5881 - val_loss: 2.0779 - val_acc: 0.5870\n",
      "Epoch 482/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9490 - acc: 0.5884 - val_loss: 2.0786 - val_acc: 0.5875\n",
      "Epoch 483/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9636 - acc: 0.5886 - val_loss: 2.0789 - val_acc: 0.5875\n",
      "Epoch 484/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9541 - acc: 0.5883 - val_loss: 2.0776 - val_acc: 0.5873\n",
      "Epoch 485/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9611 - acc: 0.5883 - val_loss: 2.0772 - val_acc: 0.5873\n",
      "Epoch 486/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9561 - acc: 0.5888 - val_loss: 2.0769 - val_acc: 0.5873\n",
      "Epoch 487/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9603 - acc: 0.5885 - val_loss: 2.0779 - val_acc: 0.5870\n",
      "Epoch 488/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9483 - acc: 0.5886 - val_loss: 2.0792 - val_acc: 0.5870\n",
      "Epoch 489/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9515 - acc: 0.5887 - val_loss: 2.0800 - val_acc: 0.5875\n",
      "Epoch 490/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 1.9491 - acc: 0.5888 - val_loss: 2.0793 - val_acc: 0.5883\n",
      "Epoch 491/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9546 - acc: 0.5889 - val_loss: 2.0792 - val_acc: 0.5881\n",
      "Epoch 492/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9488 - acc: 0.5890 - val_loss: 2.0783 - val_acc: 0.5879\n",
      "Epoch 493/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9515 - acc: 0.5883 - val_loss: 2.0777 - val_acc: 0.5877\n",
      "Epoch 494/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9446 - acc: 0.5884 - val_loss: 2.0785 - val_acc: 0.5881\n",
      "Epoch 495/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9478 - acc: 0.5884 - val_loss: 2.0791 - val_acc: 0.5881\n",
      "Epoch 496/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9458 - acc: 0.5883 - val_loss: 2.0798 - val_acc: 0.5881\n",
      "Epoch 497/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9507 - acc: 0.5884 - val_loss: 2.0793 - val_acc: 0.5879\n",
      "Epoch 498/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9497 - acc: 0.5884 - val_loss: 2.0761 - val_acc: 0.5877\n",
      "Epoch 499/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9541 - acc: 0.5888 - val_loss: 2.0749 - val_acc: 0.5873\n",
      "Epoch 500/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9435 - acc: 0.5884 - val_loss: 2.0755 - val_acc: 0.5873\n",
      "Epoch 501/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9501 - acc: 0.5891 - val_loss: 2.0755 - val_acc: 0.5879\n",
      "Epoch 502/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9644 - acc: 0.5887 - val_loss: 2.0757 - val_acc: 0.5879\n",
      "Epoch 503/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9549 - acc: 0.5884 - val_loss: 2.0781 - val_acc: 0.5875\n",
      "Epoch 504/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9498 - acc: 0.5887 - val_loss: 2.0789 - val_acc: 0.5875\n",
      "Epoch 505/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9590 - acc: 0.5893 - val_loss: 2.0790 - val_acc: 0.5875\n",
      "Epoch 506/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9462 - acc: 0.5890 - val_loss: 2.0785 - val_acc: 0.5881\n",
      "Epoch 507/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9484 - acc: 0.5887 - val_loss: 2.0788 - val_acc: 0.5881\n",
      "Epoch 508/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9455 - acc: 0.5892 - val_loss: 2.0794 - val_acc: 0.5877\n",
      "Epoch 509/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9493 - acc: 0.5890 - val_loss: 2.0809 - val_acc: 0.5877\n",
      "Epoch 510/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9535 - acc: 0.5888 - val_loss: 2.0806 - val_acc: 0.5877\n",
      "Epoch 511/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9479 - acc: 0.5890 - val_loss: 2.0802 - val_acc: 0.5875\n",
      "Epoch 512/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9474 - acc: 0.5890 - val_loss: 2.0802 - val_acc: 0.5873\n",
      "Epoch 513/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9491 - acc: 0.5889 - val_loss: 2.0794 - val_acc: 0.5873\n",
      "Epoch 514/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9588 - acc: 0.5888 - val_loss: 2.0784 - val_acc: 0.5877\n",
      "Epoch 515/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9524 - acc: 0.5889 - val_loss: 2.0771 - val_acc: 0.5875\n",
      "Epoch 516/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9416 - acc: 0.5892 - val_loss: 2.0767 - val_acc: 0.5881\n",
      "Epoch 517/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9444 - acc: 0.5894 - val_loss: 2.0776 - val_acc: 0.5879\n",
      "Epoch 518/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9455 - acc: 0.5894 - val_loss: 2.0777 - val_acc: 0.5879\n",
      "Epoch 519/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9505 - acc: 0.5889 - val_loss: 2.0770 - val_acc: 0.5879\n",
      "Epoch 520/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9536 - acc: 0.5891 - val_loss: 2.0759 - val_acc: 0.5881\n",
      "Epoch 521/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9572 - acc: 0.5895 - val_loss: 2.0766 - val_acc: 0.5877\n",
      "Epoch 522/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9485 - acc: 0.5895 - val_loss: 2.0771 - val_acc: 0.5870\n",
      "Epoch 523/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9494 - acc: 0.5886 - val_loss: 2.0771 - val_acc: 0.5875\n",
      "Epoch 524/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9632 - acc: 0.5896 - val_loss: 2.0775 - val_acc: 0.5870\n",
      "Epoch 525/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9463 - acc: 0.5888 - val_loss: 2.0776 - val_acc: 0.5877\n",
      "Epoch 526/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9497 - acc: 0.5890 - val_loss: 2.0784 - val_acc: 0.5873\n",
      "Epoch 527/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9445 - acc: 0.5892 - val_loss: 2.0774 - val_acc: 0.5877\n",
      "Epoch 528/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9448 - acc: 0.5889 - val_loss: 2.0774 - val_acc: 0.5875\n",
      "Epoch 529/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9607 - acc: 0.5891 - val_loss: 2.0765 - val_acc: 0.5873\n",
      "Epoch 530/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9425 - acc: 0.5892 - val_loss: 2.0751 - val_acc: 0.5875\n",
      "Epoch 531/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9478 - acc: 0.5893 - val_loss: 2.0749 - val_acc: 0.5870\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9400 - acc: 0.5888 - val_loss: 2.0742 - val_acc: 0.5868\n",
      "Epoch 533/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9460 - acc: 0.5888 - val_loss: 2.0740 - val_acc: 0.5870\n",
      "Epoch 534/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9459 - acc: 0.5892 - val_loss: 2.0741 - val_acc: 0.5870\n",
      "Epoch 535/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9473 - acc: 0.5896 - val_loss: 2.0741 - val_acc: 0.5868\n",
      "Epoch 536/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9481 - acc: 0.5895 - val_loss: 2.0728 - val_acc: 0.5868\n",
      "Epoch 537/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9494 - acc: 0.5892 - val_loss: 2.0728 - val_acc: 0.5870\n",
      "Epoch 538/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9428 - acc: 0.5895 - val_loss: 2.0736 - val_acc: 0.5875\n",
      "Epoch 539/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 1.9412 - acc: 0.5888 - val_loss: 2.0743 - val_acc: 0.5877\n",
      "Epoch 540/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9459 - acc: 0.5889 - val_loss: 2.0746 - val_acc: 0.5870\n",
      "Epoch 541/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9470 - acc: 0.5894 - val_loss: 2.0749 - val_acc: 0.5870\n",
      "Epoch 542/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9483 - acc: 0.5897 - val_loss: 2.0753 - val_acc: 0.5873\n",
      "Epoch 543/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9444 - acc: 0.5896 - val_loss: 2.0759 - val_acc: 0.5870\n",
      "Epoch 544/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9466 - acc: 0.5894 - val_loss: 2.0759 - val_acc: 0.5877\n",
      "Epoch 545/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9407 - acc: 0.5897 - val_loss: 2.0758 - val_acc: 0.5868\n",
      "Epoch 546/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9495 - acc: 0.5896 - val_loss: 2.0761 - val_acc: 0.5868\n",
      "Epoch 547/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9636 - acc: 0.5904 - val_loss: 2.0744 - val_acc: 0.5866\n",
      "Epoch 548/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9424 - acc: 0.5893 - val_loss: 2.0729 - val_acc: 0.5868\n",
      "Epoch 549/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9448 - acc: 0.5894 - val_loss: 2.0708 - val_acc: 0.5868\n",
      "Epoch 550/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9492 - acc: 0.5889 - val_loss: 2.0699 - val_acc: 0.5873\n",
      "Epoch 551/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9579 - acc: 0.5892 - val_loss: 2.0689 - val_acc: 0.5877\n",
      "Epoch 552/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9543 - acc: 0.5891 - val_loss: 2.0706 - val_acc: 0.5875\n",
      "Epoch 553/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9453 - acc: 0.5895 - val_loss: 2.0719 - val_acc: 0.5875\n",
      "Epoch 554/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9533 - acc: 0.5894 - val_loss: 2.0733 - val_acc: 0.5877\n",
      "Epoch 555/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9491 - acc: 0.5892 - val_loss: 2.0735 - val_acc: 0.5877\n",
      "Epoch 556/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9430 - acc: 0.5895 - val_loss: 2.0730 - val_acc: 0.5875\n",
      "Epoch 557/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9421 - acc: 0.5894 - val_loss: 2.0733 - val_acc: 0.5873\n",
      "Epoch 558/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9439 - acc: 0.5897 - val_loss: 2.0736 - val_acc: 0.5877\n",
      "Epoch 559/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9460 - acc: 0.5893 - val_loss: 2.0735 - val_acc: 0.5873\n",
      "Epoch 560/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9383 - acc: 0.5900 - val_loss: 2.0735 - val_acc: 0.5870\n",
      "Epoch 561/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9470 - acc: 0.5889 - val_loss: 2.0740 - val_acc: 0.5877\n",
      "Epoch 562/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9406 - acc: 0.5895 - val_loss: 2.0745 - val_acc: 0.5875\n",
      "Epoch 563/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9472 - acc: 0.5896 - val_loss: 2.0751 - val_acc: 0.5875\n",
      "Epoch 564/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9564 - acc: 0.5896 - val_loss: 2.0752 - val_acc: 0.5877\n",
      "Epoch 565/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9474 - acc: 0.5900 - val_loss: 2.0746 - val_acc: 0.5875\n",
      "Epoch 566/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9428 - acc: 0.5893 - val_loss: 2.0743 - val_acc: 0.5873\n",
      "Epoch 567/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9408 - acc: 0.5896 - val_loss: 2.0740 - val_acc: 0.5873\n",
      "Epoch 568/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9474 - acc: 0.5896 - val_loss: 2.0745 - val_acc: 0.5873\n",
      "Epoch 569/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9492 - acc: 0.5892 - val_loss: 2.0746 - val_acc: 0.5873\n",
      "Epoch 570/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9419 - acc: 0.5896 - val_loss: 2.0753 - val_acc: 0.5870\n",
      "Epoch 571/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9467 - acc: 0.5897 - val_loss: 2.0763 - val_acc: 0.5879\n",
      "Epoch 572/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9403 - acc: 0.5897 - val_loss: 2.0764 - val_acc: 0.5875\n",
      "Epoch 573/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9415 - acc: 0.5892 - val_loss: 2.0756 - val_acc: 0.5881\n",
      "Epoch 574/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9412 - acc: 0.5897 - val_loss: 2.0752 - val_acc: 0.5877\n",
      "Epoch 575/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9424 - acc: 0.5898 - val_loss: 2.0757 - val_acc: 0.5875\n",
      "Epoch 576/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9408 - acc: 0.5903 - val_loss: 2.0769 - val_acc: 0.5875\n",
      "Epoch 577/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9411 - acc: 0.5892 - val_loss: 2.0773 - val_acc: 0.5868\n",
      "Epoch 578/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9476 - acc: 0.5900 - val_loss: 2.0771 - val_acc: 0.5873\n",
      "Epoch 579/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9417 - acc: 0.5902 - val_loss: 2.0777 - val_acc: 0.5877\n",
      "Epoch 580/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9462 - acc: 0.5895 - val_loss: 2.0782 - val_acc: 0.5877\n",
      "Epoch 581/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9417 - acc: 0.5900 - val_loss: 2.0790 - val_acc: 0.5870\n",
      "Epoch 582/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9390 - acc: 0.5898 - val_loss: 2.0797 - val_acc: 0.5873\n",
      "Epoch 583/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9414 - acc: 0.5900 - val_loss: 2.0792 - val_acc: 0.5873\n",
      "Epoch 584/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9398 - acc: 0.5901 - val_loss: 2.0781 - val_acc: 0.5870\n",
      "Epoch 585/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9627 - acc: 0.5896 - val_loss: 2.0761 - val_acc: 0.5866\n",
      "Epoch 586/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9533 - acc: 0.5902 - val_loss: 2.0731 - val_acc: 0.5864\n",
      "Epoch 587/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9431 - acc: 0.5900 - val_loss: 2.0713 - val_acc: 0.5868\n",
      "Epoch 588/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9464 - acc: 0.5895 - val_loss: 2.0704 - val_acc: 0.5873\n",
      "Epoch 589/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9390 - acc: 0.5900 - val_loss: 2.0708 - val_acc: 0.5870\n",
      "Epoch 590/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9418 - acc: 0.5899 - val_loss: 2.0710 - val_acc: 0.5868\n",
      "Epoch 591/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9420 - acc: 0.5897 - val_loss: 2.0712 - val_acc: 0.5864\n",
      "Epoch 592/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9447 - acc: 0.5897 - val_loss: 2.0711 - val_acc: 0.5862\n",
      "Epoch 593/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9504 - acc: 0.5899 - val_loss: 2.0712 - val_acc: 0.5866\n",
      "Epoch 594/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9430 - acc: 0.5900 - val_loss: 2.0697 - val_acc: 0.5873\n",
      "Epoch 595/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9432 - acc: 0.5893 - val_loss: 2.0687 - val_acc: 0.5875\n",
      "Epoch 596/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9473 - acc: 0.5902 - val_loss: 2.0672 - val_acc: 0.5870\n",
      "Epoch 597/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9431 - acc: 0.5898 - val_loss: 2.0667 - val_acc: 0.5862\n",
      "Epoch 598/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9482 - acc: 0.5903 - val_loss: 2.0667 - val_acc: 0.5862\n",
      "Epoch 599/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9435 - acc: 0.5897 - val_loss: 2.0669 - val_acc: 0.5866\n",
      "Epoch 600/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9484 - acc: 0.5900 - val_loss: 2.0678 - val_acc: 0.5866\n",
      "Epoch 601/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9477 - acc: 0.5902 - val_loss: 2.0677 - val_acc: 0.5862\n",
      "Epoch 602/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9426 - acc: 0.5906 - val_loss: 2.0672 - val_acc: 0.5860\n",
      "Epoch 603/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9451 - acc: 0.5900 - val_loss: 2.0672 - val_acc: 0.5864\n",
      "Epoch 604/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9400 - acc: 0.5903 - val_loss: 2.0675 - val_acc: 0.5864\n",
      "Epoch 605/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9480 - acc: 0.5902 - val_loss: 2.0677 - val_acc: 0.5864\n",
      "Epoch 606/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9383 - acc: 0.5902 - val_loss: 2.0668 - val_acc: 0.5866\n",
      "Epoch 607/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9427 - acc: 0.5899 - val_loss: 2.0666 - val_acc: 0.5866\n",
      "Epoch 608/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9450 - acc: 0.5897 - val_loss: 2.0671 - val_acc: 0.5870\n",
      "Epoch 609/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9393 - acc: 0.5901 - val_loss: 2.0675 - val_acc: 0.5868\n",
      "Epoch 610/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9445 - acc: 0.5898 - val_loss: 2.0678 - val_acc: 0.5870\n",
      "Epoch 611/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9417 - acc: 0.5904 - val_loss: 2.0680 - val_acc: 0.5868\n",
      "Epoch 612/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9430 - acc: 0.5902 - val_loss: 2.0681 - val_acc: 0.5868\n",
      "Epoch 613/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9401 - acc: 0.5906 - val_loss: 2.0686 - val_acc: 0.5868\n",
      "Epoch 614/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9393 - acc: 0.5902 - val_loss: 2.0696 - val_acc: 0.5866\n",
      "Epoch 615/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 1.9466 - acc: 0.5903 - val_loss: 2.0696 - val_acc: 0.5868\n",
      "Epoch 616/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9424 - acc: 0.5901 - val_loss: 2.0701 - val_acc: 0.5870\n",
      "Epoch 617/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9415 - acc: 0.5905 - val_loss: 2.0714 - val_acc: 0.5873\n",
      "Epoch 618/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9482 - acc: 0.5898 - val_loss: 2.0724 - val_acc: 0.5870\n",
      "Epoch 619/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9464 - acc: 0.5897 - val_loss: 2.0732 - val_acc: 0.5873\n",
      "Epoch 620/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9394 - acc: 0.5910 - val_loss: 2.0720 - val_acc: 0.5875\n",
      "Epoch 621/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9443 - acc: 0.5900 - val_loss: 2.0722 - val_acc: 0.5877\n",
      "Epoch 622/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9486 - acc: 0.5903 - val_loss: 2.0701 - val_acc: 0.5875\n",
      "Epoch 623/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9438 - acc: 0.5905 - val_loss: 2.0678 - val_acc: 0.5877\n",
      "Epoch 624/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9469 - acc: 0.5905 - val_loss: 2.0669 - val_acc: 0.5875\n",
      "Epoch 625/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9530 - acc: 0.5905 - val_loss: 2.0669 - val_acc: 0.5875\n",
      "Epoch 626/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 1.9493 - acc: 0.5906 - val_loss: 2.0676 - val_acc: 0.5870\n",
      "Epoch 627/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9429 - acc: 0.5907 - val_loss: 2.0663 - val_acc: 0.5877\n",
      "Epoch 628/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9712 - acc: 0.5905 - val_loss: 2.0660 - val_acc: 0.5875\n",
      "Epoch 629/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9448 - acc: 0.5907 - val_loss: 2.0662 - val_acc: 0.5873\n",
      "Epoch 630/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9390 - acc: 0.5906 - val_loss: 2.0675 - val_acc: 0.5875\n",
      "Epoch 631/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9438 - acc: 0.5906 - val_loss: 2.0685 - val_acc: 0.5877\n",
      "Epoch 632/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9488 - acc: 0.5900 - val_loss: 2.0688 - val_acc: 0.5875\n",
      "Epoch 633/1000\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 1.9535 - acc: 0.5908 - val_loss: 2.0690 - val_acc: 0.5870\n",
      "Epoch 634/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9499 - acc: 0.5908 - val_loss: 2.0681 - val_acc: 0.5870\n",
      "Epoch 635/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9432 - acc: 0.5901 - val_loss: 2.0670 - val_acc: 0.5870\n",
      "Epoch 636/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9419 - acc: 0.5904 - val_loss: 2.0666 - val_acc: 0.5873\n",
      "Epoch 637/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9403 - acc: 0.5905 - val_loss: 2.0671 - val_acc: 0.5875\n",
      "Epoch 638/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9500 - acc: 0.5903 - val_loss: 2.0674 - val_acc: 0.5875\n",
      "Epoch 639/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 1.9379 - acc: 0.5901 - val_loss: 2.0684 - val_acc: 0.5875\n",
      "Epoch 640/1000\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 1.9421 - acc: 0.5905 - val_loss: 2.0697 - val_acc: 0.5875\n",
      "Epoch 641/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9454 - acc: 0.5912 - val_loss: 2.0703 - val_acc: 0.5877\n",
      "Epoch 642/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9414 - acc: 0.5897 - val_loss: 2.0710 - val_acc: 0.5879\n",
      "Epoch 643/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 1.9417 - acc: 0.5904 - val_loss: 2.0717 - val_acc: 0.5879\n",
      "Epoch 644/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9403 - acc: 0.5901 - val_loss: 2.0718 - val_acc: 0.5881\n",
      "Epoch 645/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9366 - acc: 0.5907 - val_loss: 2.0714 - val_acc: 0.5881\n",
      "Epoch 646/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 1.9397 - acc: 0.5903 - val_loss: 2.0712 - val_acc: 0.5877\n",
      "Epoch 647/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9570 - acc: 0.5904 - val_loss: 2.0700 - val_acc: 0.5877\n",
      "Epoch 648/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9427 - acc: 0.5903 - val_loss: 2.0693 - val_acc: 0.5877\n",
      "Epoch 649/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9400 - acc: 0.5907 - val_loss: 2.0693 - val_acc: 0.5877\n",
      "Epoch 650/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 1.9472 - acc: 0.5910 - val_loss: 2.0677 - val_acc: 0.5877\n",
      "Epoch 651/1000\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 1.9585 - acc: 0.5910 - val_loss: 2.0666 - val_acc: 0.5875\n",
      "Epoch 652/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9395 - acc: 0.5905 - val_loss: 2.0667 - val_acc: 0.5877\n",
      "Epoch 653/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9423 - acc: 0.5904 - val_loss: 2.0666 - val_acc: 0.5877\n",
      "Epoch 654/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9409 - acc: 0.5909 - val_loss: 2.0671 - val_acc: 0.5877\n",
      "Epoch 655/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9458 - acc: 0.5910 - val_loss: 2.0676 - val_acc: 0.5879\n",
      "Epoch 656/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9392 - acc: 0.5901 - val_loss: 2.0667 - val_acc: 0.5881\n",
      "Epoch 657/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 1.9541 - acc: 0.5913 - val_loss: 2.0658 - val_acc: 0.5881\n",
      "Epoch 658/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9408 - acc: 0.5902 - val_loss: 2.0624 - val_acc: 0.5881\n",
      "Epoch 659/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9474 - acc: 0.5904 - val_loss: 2.0621 - val_acc: 0.5879\n",
      "Epoch 660/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9403 - acc: 0.5905 - val_loss: 2.0625 - val_acc: 0.5868\n",
      "Epoch 661/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9414 - acc: 0.5909 - val_loss: 2.0632 - val_acc: 0.5873\n",
      "Epoch 662/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9404 - acc: 0.5908 - val_loss: 2.0637 - val_acc: 0.5879\n",
      "Epoch 663/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9405 - acc: 0.5903 - val_loss: 2.0645 - val_acc: 0.5875\n",
      "Epoch 664/1000\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 1.9383 - acc: 0.5907 - val_loss: 2.0649 - val_acc: 0.5873\n",
      "Epoch 665/1000\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 1.9371 - acc: 0.5902 - val_loss: 2.0651 - val_acc: 0.5875\n",
      "Epoch 666/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9475 - acc: 0.5904 - val_loss: 2.0653 - val_acc: 0.5873\n",
      "Epoch 667/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9372 - acc: 0.5905 - val_loss: 2.0658 - val_acc: 0.5877\n",
      "Epoch 668/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9522 - acc: 0.5909 - val_loss: 2.0650 - val_acc: 0.5883\n",
      "Epoch 669/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9484 - acc: 0.5909 - val_loss: 2.0642 - val_acc: 0.5883\n",
      "Epoch 670/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 1.9402 - acc: 0.5906 - val_loss: 2.0638 - val_acc: 0.5885\n",
      "Epoch 671/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 1.9327 - acc: 0.5906 - val_loss: 2.0633 - val_acc: 0.5877\n",
      "Epoch 672/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9371 - acc: 0.5910 - val_loss: 2.0634 - val_acc: 0.5885\n",
      "Epoch 673/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9412 - acc: 0.5909 - val_loss: 2.0640 - val_acc: 0.5881\n",
      "Epoch 674/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9400 - acc: 0.5907 - val_loss: 2.0636 - val_acc: 0.5879\n",
      "Epoch 675/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9373 - acc: 0.5911 - val_loss: 2.0635 - val_acc: 0.5883\n",
      "Epoch 676/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 1.9539 - acc: 0.5906 - val_loss: 2.0635 - val_acc: 0.5883\n",
      "Epoch 677/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 1.9399 - acc: 0.5908 - val_loss: 2.0637 - val_acc: 0.5883\n",
      "Epoch 678/1000\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 1.9385 - acc: 0.5915 - val_loss: 2.0641 - val_acc: 0.5887\n",
      "Epoch 679/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9422 - acc: 0.5912 - val_loss: 2.0645 - val_acc: 0.5885\n",
      "Epoch 680/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9400 - acc: 0.5911 - val_loss: 2.0648 - val_acc: 0.5887\n",
      "Epoch 681/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9378 - acc: 0.5910 - val_loss: 2.0636 - val_acc: 0.5883\n",
      "Epoch 682/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9387 - acc: 0.5908 - val_loss: 2.0628 - val_acc: 0.5879\n",
      "Epoch 683/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9414 - acc: 0.5907 - val_loss: 2.0628 - val_acc: 0.5881\n",
      "Epoch 684/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9386 - acc: 0.5905 - val_loss: 2.0629 - val_acc: 0.5879\n",
      "Epoch 685/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9460 - acc: 0.5907 - val_loss: 2.0630 - val_acc: 0.5875\n",
      "Epoch 686/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9425 - acc: 0.5905 - val_loss: 2.0635 - val_acc: 0.5873\n",
      "Epoch 687/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9369 - acc: 0.5905 - val_loss: 2.0632 - val_acc: 0.5879\n",
      "Epoch 688/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9404 - acc: 0.5911 - val_loss: 2.0630 - val_acc: 0.5877\n",
      "Epoch 689/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9398 - acc: 0.5916 - val_loss: 2.0618 - val_acc: 0.5879\n",
      "Epoch 690/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9422 - acc: 0.5909 - val_loss: 2.0618 - val_acc: 0.5877\n",
      "Epoch 691/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9504 - acc: 0.5909 - val_loss: 2.0617 - val_acc: 0.5879\n",
      "Epoch 692/1000\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.9432 - acc: 0.5911 - val_loss: 2.0615 - val_acc: 0.5879\n",
      "Epoch 693/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9396 - acc: 0.5914 - val_loss: 2.0614 - val_acc: 0.5877\n",
      "Epoch 694/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9417 - acc: 0.5907 - val_loss: 2.0617 - val_acc: 0.5879\n",
      "Epoch 695/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9417 - acc: 0.5910 - val_loss: 2.0614 - val_acc: 0.5883\n",
      "Epoch 696/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9423 - acc: 0.5908 - val_loss: 2.0612 - val_acc: 0.5879\n",
      "Epoch 697/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9432 - acc: 0.5909 - val_loss: 2.0609 - val_acc: 0.5881\n",
      "Epoch 698/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9387 - acc: 0.5908 - val_loss: 2.0604 - val_acc: 0.5877\n",
      "Epoch 699/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9412 - acc: 0.5912 - val_loss: 2.0596 - val_acc: 0.5881\n",
      "Epoch 700/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9378 - acc: 0.5910 - val_loss: 2.0593 - val_acc: 0.5879\n",
      "Epoch 701/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9361 - acc: 0.5914 - val_loss: 2.0587 - val_acc: 0.5881\n",
      "Epoch 702/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9385 - acc: 0.5913 - val_loss: 2.0585 - val_acc: 0.5881\n",
      "Epoch 703/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 1.9385 - acc: 0.5912 - val_loss: 2.0588 - val_acc: 0.5881\n",
      "Epoch 704/1000\n",
      "10/10 [==============================] - 1s 130ms/step - loss: 1.9363 - acc: 0.5909 - val_loss: 2.0592 - val_acc: 0.5883\n",
      "Epoch 705/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9442 - acc: 0.5906 - val_loss: 2.0597 - val_acc: 0.5879\n",
      "Epoch 706/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9341 - acc: 0.5914 - val_loss: 2.0601 - val_acc: 0.5883\n",
      "Epoch 707/1000\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 1.9407 - acc: 0.5909 - val_loss: 2.0612 - val_acc: 0.5879\n",
      "Epoch 708/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 1.9363 - acc: 0.5911 - val_loss: 2.0619 - val_acc: 0.5883\n",
      "Epoch 709/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9339 - acc: 0.5908 - val_loss: 2.0619 - val_acc: 0.5883\n",
      "Epoch 710/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9355 - acc: 0.5907 - val_loss: 2.0620 - val_acc: 0.5885\n",
      "Epoch 711/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9512 - acc: 0.5908 - val_loss: 2.0621 - val_acc: 0.5883\n",
      "Epoch 712/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9345 - acc: 0.5905 - val_loss: 2.0617 - val_acc: 0.5879\n",
      "Epoch 713/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9400 - acc: 0.5906 - val_loss: 2.0615 - val_acc: 0.5885\n",
      "Epoch 714/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9355 - acc: 0.5914 - val_loss: 2.0610 - val_acc: 0.5879\n",
      "Epoch 715/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9366 - acc: 0.5915 - val_loss: 2.0606 - val_acc: 0.5879\n",
      "Epoch 716/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9365 - acc: 0.5912 - val_loss: 2.0604 - val_acc: 0.5877\n",
      "Epoch 717/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9398 - acc: 0.5916 - val_loss: 2.0609 - val_acc: 0.5877\n",
      "Epoch 718/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9393 - acc: 0.5914 - val_loss: 2.0609 - val_acc: 0.5879\n",
      "Epoch 719/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9324 - acc: 0.5914 - val_loss: 2.0608 - val_acc: 0.5875\n",
      "Epoch 720/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9393 - acc: 0.5915 - val_loss: 2.0596 - val_acc: 0.5873\n",
      "Epoch 721/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9408 - acc: 0.5914 - val_loss: 2.0588 - val_acc: 0.5873\n",
      "Epoch 722/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9358 - acc: 0.5910 - val_loss: 2.0587 - val_acc: 0.5875\n",
      "Epoch 723/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9409 - acc: 0.5905 - val_loss: 2.0583 - val_acc: 0.5875\n",
      "Epoch 724/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9391 - acc: 0.5904 - val_loss: 2.0582 - val_acc: 0.5875\n",
      "Epoch 725/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9454 - acc: 0.5911 - val_loss: 2.0578 - val_acc: 0.5881\n",
      "Epoch 726/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9406 - acc: 0.5910 - val_loss: 2.0581 - val_acc: 0.5881\n",
      "Epoch 727/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9344 - acc: 0.5916 - val_loss: 2.0587 - val_acc: 0.5879\n",
      "Epoch 728/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9356 - acc: 0.5908 - val_loss: 2.0585 - val_acc: 0.5877\n",
      "Epoch 729/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9333 - acc: 0.5909 - val_loss: 2.0587 - val_acc: 0.5875\n",
      "Epoch 730/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9509 - acc: 0.5910 - val_loss: 2.0590 - val_acc: 0.5875\n",
      "Epoch 731/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9387 - acc: 0.5917 - val_loss: 2.0592 - val_acc: 0.5875\n",
      "Epoch 732/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9394 - acc: 0.5915 - val_loss: 2.0593 - val_acc: 0.5879\n",
      "Epoch 733/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9471 - acc: 0.5913 - val_loss: 2.0593 - val_acc: 0.5879\n",
      "Epoch 734/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9378 - acc: 0.5916 - val_loss: 2.0596 - val_acc: 0.5879\n",
      "Epoch 735/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9470 - acc: 0.5910 - val_loss: 2.0594 - val_acc: 0.5879\n",
      "Epoch 736/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9384 - acc: 0.5913 - val_loss: 2.0589 - val_acc: 0.5873\n",
      "Epoch 737/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9341 - acc: 0.5915 - val_loss: 2.0589 - val_acc: 0.5879\n",
      "Epoch 738/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9365 - acc: 0.5913 - val_loss: 2.0588 - val_acc: 0.5873\n",
      "Epoch 739/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9380 - acc: 0.5917 - val_loss: 2.0574 - val_acc: 0.5873\n",
      "Epoch 740/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 1.9325 - acc: 0.5917 - val_loss: 2.0568 - val_acc: 0.5875\n",
      "Epoch 741/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9427 - acc: 0.5915 - val_loss: 2.0571 - val_acc: 0.5877\n",
      "Epoch 742/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9436 - acc: 0.5909 - val_loss: 2.0573 - val_acc: 0.5870\n",
      "Epoch 743/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9315 - acc: 0.5912 - val_loss: 2.0574 - val_acc: 0.5873\n",
      "Epoch 744/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9411 - acc: 0.5919 - val_loss: 2.0576 - val_acc: 0.5870\n",
      "Epoch 745/1000\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 1.9349 - acc: 0.5912 - val_loss: 2.0569 - val_acc: 0.5873\n",
      "Epoch 746/1000\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 1.9337 - acc: 0.5916 - val_loss: 2.0567 - val_acc: 0.5875\n",
      "Epoch 747/1000\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 1.9424 - acc: 0.5917 - val_loss: 2.0563 - val_acc: 0.5875\n",
      "Epoch 748/1000\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 1.9362 - acc: 0.5914 - val_loss: 2.0566 - val_acc: 0.5873\n",
      "Epoch 749/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9319 - acc: 0.5918 - val_loss: 2.0570 - val_acc: 0.5873\n",
      "Epoch 750/1000\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 1.9340 - acc: 0.5919 - val_loss: 2.0573 - val_acc: 0.5870\n",
      "Epoch 751/1000\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 1.9402 - acc: 0.5912 - val_loss: 2.0572 - val_acc: 0.5870\n",
      "Epoch 752/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9332 - acc: 0.5917 - val_loss: 2.0572 - val_acc: 0.5864\n",
      "Epoch 753/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9336 - acc: 0.5919 - val_loss: 2.0573 - val_acc: 0.5868\n",
      "Epoch 754/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9430 - acc: 0.5913 - val_loss: 2.0577 - val_acc: 0.5868\n",
      "Epoch 755/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9428 - acc: 0.5914 - val_loss: 2.0583 - val_acc: 0.5868\n",
      "Epoch 756/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9455 - acc: 0.5915 - val_loss: 2.0577 - val_acc: 0.5868\n",
      "Epoch 757/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9336 - acc: 0.5919 - val_loss: 2.0576 - val_acc: 0.5868\n",
      "Epoch 758/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9364 - acc: 0.5913 - val_loss: 2.0575 - val_acc: 0.5868\n",
      "Epoch 759/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9331 - acc: 0.5915 - val_loss: 2.0581 - val_acc: 0.5870\n",
      "Epoch 760/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9375 - acc: 0.5913 - val_loss: 2.0583 - val_acc: 0.5873\n",
      "Epoch 761/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9340 - acc: 0.5919 - val_loss: 2.0584 - val_acc: 0.5875\n",
      "Epoch 762/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9328 - acc: 0.5917 - val_loss: 2.0586 - val_acc: 0.5875\n",
      "Epoch 763/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9344 - acc: 0.5914 - val_loss: 2.0589 - val_acc: 0.5873\n",
      "Epoch 764/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9331 - acc: 0.5911 - val_loss: 2.0588 - val_acc: 0.5873\n",
      "Epoch 765/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9340 - acc: 0.5913 - val_loss: 2.0587 - val_acc: 0.5873\n",
      "Epoch 766/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9377 - acc: 0.5914 - val_loss: 2.0585 - val_acc: 0.5873\n",
      "Epoch 767/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9400 - acc: 0.5918 - val_loss: 2.0578 - val_acc: 0.5868\n",
      "Epoch 768/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9413 - acc: 0.5918 - val_loss: 2.0575 - val_acc: 0.5866\n",
      "Epoch 769/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9386 - acc: 0.5918 - val_loss: 2.0577 - val_acc: 0.5864\n",
      "Epoch 770/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9375 - acc: 0.5920 - val_loss: 2.0585 - val_acc: 0.5864\n",
      "Epoch 771/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9341 - acc: 0.5916 - val_loss: 2.0584 - val_acc: 0.5866\n",
      "Epoch 772/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9370 - acc: 0.5921 - val_loss: 2.0585 - val_acc: 0.5866\n",
      "Epoch 773/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9314 - acc: 0.5917 - val_loss: 2.0581 - val_acc: 0.5870\n",
      "Epoch 774/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9452 - acc: 0.5917 - val_loss: 2.0579 - val_acc: 0.5866\n",
      "Epoch 775/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9352 - acc: 0.5920 - val_loss: 2.0582 - val_acc: 0.5864\n",
      "Epoch 776/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9369 - acc: 0.5919 - val_loss: 2.0592 - val_acc: 0.5864\n",
      "Epoch 777/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9323 - acc: 0.5913 - val_loss: 2.0596 - val_acc: 0.5868\n",
      "Epoch 778/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9490 - acc: 0.5917 - val_loss: 2.0592 - val_acc: 0.5866\n",
      "Epoch 779/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9426 - acc: 0.5915 - val_loss: 2.0599 - val_acc: 0.5868\n",
      "Epoch 780/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9375 - acc: 0.5914 - val_loss: 2.0601 - val_acc: 0.5868\n",
      "Epoch 781/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9344 - acc: 0.5913 - val_loss: 2.0596 - val_acc: 0.5873\n",
      "Epoch 782/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9374 - acc: 0.5918 - val_loss: 2.0587 - val_acc: 0.5866\n",
      "Epoch 783/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9365 - acc: 0.5923 - val_loss: 2.0576 - val_acc: 0.5866\n",
      "Epoch 784/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9388 - acc: 0.5913 - val_loss: 2.0570 - val_acc: 0.5868\n",
      "Epoch 785/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9320 - acc: 0.5919 - val_loss: 2.0570 - val_acc: 0.5868\n",
      "Epoch 786/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9471 - acc: 0.5914 - val_loss: 2.0566 - val_acc: 0.5866\n",
      "Epoch 787/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9347 - acc: 0.5924 - val_loss: 2.0563 - val_acc: 0.5868\n",
      "Epoch 788/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9567 - acc: 0.5914 - val_loss: 2.0560 - val_acc: 0.5870\n",
      "Epoch 789/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9348 - acc: 0.5923 - val_loss: 2.0555 - val_acc: 0.5866\n",
      "Epoch 790/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9329 - acc: 0.5912 - val_loss: 2.0558 - val_acc: 0.5873\n",
      "Epoch 791/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9334 - acc: 0.5912 - val_loss: 2.0564 - val_acc: 0.5873\n",
      "Epoch 792/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9325 - acc: 0.5917 - val_loss: 2.0574 - val_acc: 0.5875\n",
      "Epoch 793/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9327 - acc: 0.5920 - val_loss: 2.0586 - val_acc: 0.5870\n",
      "Epoch 794/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9436 - acc: 0.5913 - val_loss: 2.0595 - val_acc: 0.5870\n",
      "Epoch 795/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9376 - acc: 0.5914 - val_loss: 2.0601 - val_acc: 0.5868\n",
      "Epoch 796/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9317 - acc: 0.5914 - val_loss: 2.0601 - val_acc: 0.5866\n",
      "Epoch 797/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9291 - acc: 0.5920 - val_loss: 2.0602 - val_acc: 0.5862\n",
      "Epoch 798/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9322 - acc: 0.5913 - val_loss: 2.0605 - val_acc: 0.5860\n",
      "Epoch 799/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9305 - acc: 0.5918 - val_loss: 2.0609 - val_acc: 0.5864\n",
      "Epoch 800/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9546 - acc: 0.5918 - val_loss: 2.0613 - val_acc: 0.5862\n",
      "Epoch 801/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9325 - acc: 0.5916 - val_loss: 2.0630 - val_acc: 0.5866\n",
      "Epoch 802/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9337 - acc: 0.5914 - val_loss: 2.0640 - val_acc: 0.5868\n",
      "Epoch 803/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9334 - acc: 0.5918 - val_loss: 2.0638 - val_acc: 0.5868\n",
      "Epoch 804/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9406 - acc: 0.5914 - val_loss: 2.0619 - val_acc: 0.5868\n",
      "Epoch 805/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9356 - acc: 0.5918 - val_loss: 2.0603 - val_acc: 0.5868\n",
      "Epoch 806/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9304 - acc: 0.5915 - val_loss: 2.0597 - val_acc: 0.5870\n",
      "Epoch 807/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9358 - acc: 0.5917 - val_loss: 2.0591 - val_acc: 0.5870\n",
      "Epoch 808/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9336 - acc: 0.5920 - val_loss: 2.0590 - val_acc: 0.5870\n",
      "Epoch 809/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9395 - acc: 0.5924 - val_loss: 2.0597 - val_acc: 0.5868\n",
      "Epoch 810/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9416 - acc: 0.5921 - val_loss: 2.0591 - val_acc: 0.5870\n",
      "Epoch 811/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9405 - acc: 0.5917 - val_loss: 2.0594 - val_acc: 0.5870\n",
      "Epoch 812/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9357 - acc: 0.5918 - val_loss: 2.0597 - val_acc: 0.5870\n",
      "Epoch 813/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9373 - acc: 0.5919 - val_loss: 2.0594 - val_acc: 0.5866\n",
      "Epoch 814/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9332 - acc: 0.5927 - val_loss: 2.0588 - val_acc: 0.5870\n",
      "Epoch 815/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9352 - acc: 0.5919 - val_loss: 2.0587 - val_acc: 0.5870\n",
      "Epoch 816/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9376 - acc: 0.5920 - val_loss: 2.0580 - val_acc: 0.5868\n",
      "Epoch 817/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9281 - acc: 0.5924 - val_loss: 2.0578 - val_acc: 0.5862\n",
      "Epoch 818/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9290 - acc: 0.5924 - val_loss: 2.0574 - val_acc: 0.5862\n",
      "Epoch 819/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9349 - acc: 0.5931 - val_loss: 2.0570 - val_acc: 0.5862\n",
      "Epoch 820/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9296 - acc: 0.5916 - val_loss: 2.0571 - val_acc: 0.5866\n",
      "Epoch 821/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9357 - acc: 0.5920 - val_loss: 2.0574 - val_acc: 0.5864\n",
      "Epoch 822/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9263 - acc: 0.5921 - val_loss: 2.0579 - val_acc: 0.5866\n",
      "Epoch 823/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9364 - acc: 0.5920 - val_loss: 2.0577 - val_acc: 0.5864\n",
      "Epoch 824/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9378 - acc: 0.5916 - val_loss: 2.0578 - val_acc: 0.5866\n",
      "Epoch 825/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9414 - acc: 0.5924 - val_loss: 2.0576 - val_acc: 0.5866\n",
      "Epoch 826/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9360 - acc: 0.5925 - val_loss: 2.0567 - val_acc: 0.5866\n",
      "Epoch 827/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9422 - acc: 0.5916 - val_loss: 2.0555 - val_acc: 0.5864\n",
      "Epoch 828/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9324 - acc: 0.5923 - val_loss: 2.0550 - val_acc: 0.5866\n",
      "Epoch 829/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9321 - acc: 0.5919 - val_loss: 2.0550 - val_acc: 0.5864\n",
      "Epoch 830/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9393 - acc: 0.5920 - val_loss: 2.0547 - val_acc: 0.5866\n",
      "Epoch 831/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9381 - acc: 0.5920 - val_loss: 2.0548 - val_acc: 0.5866\n",
      "Epoch 832/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9393 - acc: 0.5924 - val_loss: 2.0554 - val_acc: 0.5873\n",
      "Epoch 833/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9307 - acc: 0.5918 - val_loss: 2.0564 - val_acc: 0.5870\n",
      "Epoch 834/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9319 - acc: 0.5926 - val_loss: 2.0569 - val_acc: 0.5870\n",
      "Epoch 835/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9330 - acc: 0.5915 - val_loss: 2.0577 - val_acc: 0.5868\n",
      "Epoch 836/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9333 - acc: 0.5919 - val_loss: 2.0583 - val_acc: 0.5870\n",
      "Epoch 837/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9364 - acc: 0.5920 - val_loss: 2.0589 - val_acc: 0.5870\n",
      "Epoch 838/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9331 - acc: 0.5922 - val_loss: 2.0592 - val_acc: 0.5870\n",
      "Epoch 839/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9335 - acc: 0.5923 - val_loss: 2.0598 - val_acc: 0.5873\n",
      "Epoch 840/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9325 - acc: 0.5921 - val_loss: 2.0599 - val_acc: 0.5868\n",
      "Epoch 841/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9344 - acc: 0.5925 - val_loss: 2.0596 - val_acc: 0.5873\n",
      "Epoch 842/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9308 - acc: 0.5922 - val_loss: 2.0593 - val_acc: 0.5868\n",
      "Epoch 843/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9320 - acc: 0.5922 - val_loss: 2.0594 - val_acc: 0.5870\n",
      "Epoch 844/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9319 - acc: 0.5920 - val_loss: 2.0591 - val_acc: 0.5873\n",
      "Epoch 845/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9380 - acc: 0.5925 - val_loss: 2.0590 - val_acc: 0.5868\n",
      "Epoch 846/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9593 - acc: 0.5920 - val_loss: 2.0594 - val_acc: 0.5866\n",
      "Epoch 847/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9370 - acc: 0.5922 - val_loss: 2.0596 - val_acc: 0.5868\n",
      "Epoch 848/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9338 - acc: 0.5922 - val_loss: 2.0598 - val_acc: 0.5868\n",
      "Epoch 849/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9331 - acc: 0.5920 - val_loss: 2.0605 - val_acc: 0.5873\n",
      "Epoch 850/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9348 - acc: 0.5920 - val_loss: 2.0602 - val_acc: 0.5870\n",
      "Epoch 851/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9297 - acc: 0.5917 - val_loss: 2.0603 - val_acc: 0.5873\n",
      "Epoch 852/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9433 - acc: 0.5923 - val_loss: 2.0605 - val_acc: 0.5870\n",
      "Epoch 853/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9313 - acc: 0.5922 - val_loss: 2.0608 - val_acc: 0.5870\n",
      "Epoch 854/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9328 - acc: 0.5921 - val_loss: 2.0606 - val_acc: 0.5873\n",
      "Epoch 855/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9357 - acc: 0.5929 - val_loss: 2.0601 - val_acc: 0.5870\n",
      "Epoch 856/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9397 - acc: 0.5922 - val_loss: 2.0598 - val_acc: 0.5866\n",
      "Epoch 857/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9378 - acc: 0.5924 - val_loss: 2.0599 - val_acc: 0.5873\n",
      "Epoch 858/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9379 - acc: 0.5923 - val_loss: 2.0602 - val_acc: 0.5870\n",
      "Epoch 859/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9304 - acc: 0.5921 - val_loss: 2.0598 - val_acc: 0.5873\n",
      "Epoch 860/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9325 - acc: 0.5924 - val_loss: 2.0591 - val_acc: 0.5877\n",
      "Epoch 861/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9290 - acc: 0.5929 - val_loss: 2.0591 - val_acc: 0.5875\n",
      "Epoch 862/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9392 - acc: 0.5922 - val_loss: 2.0593 - val_acc: 0.5873\n",
      "Epoch 863/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9375 - acc: 0.5923 - val_loss: 2.0593 - val_acc: 0.5870\n",
      "Epoch 864/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9350 - acc: 0.5929 - val_loss: 2.0594 - val_acc: 0.5870\n",
      "Epoch 865/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9296 - acc: 0.5916 - val_loss: 2.0594 - val_acc: 0.5875\n",
      "Epoch 866/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9298 - acc: 0.5924 - val_loss: 2.0594 - val_acc: 0.5877\n",
      "Epoch 867/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9296 - acc: 0.5925 - val_loss: 2.0596 - val_acc: 0.5879\n",
      "Epoch 868/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9300 - acc: 0.5919 - val_loss: 2.0597 - val_acc: 0.5879\n",
      "Epoch 869/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9311 - acc: 0.5922 - val_loss: 2.0598 - val_acc: 0.5879\n",
      "Epoch 870/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9279 - acc: 0.5926 - val_loss: 2.0603 - val_acc: 0.5875\n",
      "Epoch 871/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9253 - acc: 0.5923 - val_loss: 2.0609 - val_acc: 0.5877\n",
      "Epoch 872/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9284 - acc: 0.5924 - val_loss: 2.0615 - val_acc: 0.5877\n",
      "Epoch 873/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9291 - acc: 0.5924 - val_loss: 2.0617 - val_acc: 0.5877\n",
      "Epoch 874/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9367 - acc: 0.5920 - val_loss: 2.0614 - val_acc: 0.5877\n",
      "Epoch 875/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9351 - acc: 0.5924 - val_loss: 2.0603 - val_acc: 0.5879\n",
      "Epoch 876/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9345 - acc: 0.5920 - val_loss: 2.0602 - val_acc: 0.5875\n",
      "Epoch 877/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9280 - acc: 0.5926 - val_loss: 2.0603 - val_acc: 0.5875\n",
      "Epoch 878/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9428 - acc: 0.5922 - val_loss: 2.0600 - val_acc: 0.5877\n",
      "Epoch 879/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9281 - acc: 0.5925 - val_loss: 2.0600 - val_acc: 0.5875\n",
      "Epoch 880/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9320 - acc: 0.5924 - val_loss: 2.0601 - val_acc: 0.5877\n",
      "Epoch 881/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9313 - acc: 0.5920 - val_loss: 2.0603 - val_acc: 0.5881\n",
      "Epoch 882/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9288 - acc: 0.5924 - val_loss: 2.0609 - val_acc: 0.5879\n",
      "Epoch 883/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9354 - acc: 0.5923 - val_loss: 2.0613 - val_acc: 0.5879\n",
      "Epoch 884/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9342 - acc: 0.5917 - val_loss: 2.0615 - val_acc: 0.5879\n",
      "Epoch 885/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9312 - acc: 0.5918 - val_loss: 2.0611 - val_acc: 0.5879\n",
      "Epoch 886/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9306 - acc: 0.5915 - val_loss: 2.0615 - val_acc: 0.5877\n",
      "Epoch 887/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9384 - acc: 0.5923 - val_loss: 2.0619 - val_acc: 0.5877\n",
      "Epoch 888/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9318 - acc: 0.5928 - val_loss: 2.0609 - val_acc: 0.5881\n",
      "Epoch 889/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9439 - acc: 0.5928 - val_loss: 2.0607 - val_acc: 0.5881\n",
      "Epoch 890/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9303 - acc: 0.5928 - val_loss: 2.0609 - val_acc: 0.5881\n",
      "Epoch 891/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9401 - acc: 0.5927 - val_loss: 2.0612 - val_acc: 0.5883\n",
      "Epoch 892/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9341 - acc: 0.5927 - val_loss: 2.0609 - val_acc: 0.5883\n",
      "Epoch 893/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9467 - acc: 0.5923 - val_loss: 2.0596 - val_acc: 0.5881\n",
      "Epoch 894/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9313 - acc: 0.5923 - val_loss: 2.0591 - val_acc: 0.5879\n",
      "Epoch 895/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9347 - acc: 0.5922 - val_loss: 2.0593 - val_acc: 0.5879\n",
      "Epoch 896/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9337 - acc: 0.5925 - val_loss: 2.0596 - val_acc: 0.5881\n",
      "Epoch 897/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9355 - acc: 0.5926 - val_loss: 2.0595 - val_acc: 0.5877\n",
      "Epoch 898/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9338 - acc: 0.5927 - val_loss: 2.0598 - val_acc: 0.5875\n",
      "Epoch 899/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9247 - acc: 0.5923 - val_loss: 2.0599 - val_acc: 0.5875\n",
      "Epoch 900/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9306 - acc: 0.5920 - val_loss: 2.0599 - val_acc: 0.5879\n",
      "Epoch 901/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9365 - acc: 0.5923 - val_loss: 2.0600 - val_acc: 0.5885\n",
      "Epoch 902/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9343 - acc: 0.5927 - val_loss: 2.0602 - val_acc: 0.5873\n",
      "Epoch 903/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9262 - acc: 0.5922 - val_loss: 2.0603 - val_acc: 0.5879\n",
      "Epoch 904/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9388 - acc: 0.5921 - val_loss: 2.0599 - val_acc: 0.5879\n",
      "Epoch 905/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9418 - acc: 0.5916 - val_loss: 2.0590 - val_acc: 0.5879\n",
      "Epoch 906/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9313 - acc: 0.5927 - val_loss: 2.0587 - val_acc: 0.5881\n",
      "Epoch 907/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9317 - acc: 0.5928 - val_loss: 2.0592 - val_acc: 0.5879\n",
      "Epoch 908/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9279 - acc: 0.5923 - val_loss: 2.0596 - val_acc: 0.5879\n",
      "Epoch 909/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9290 - acc: 0.5927 - val_loss: 2.0600 - val_acc: 0.5879\n",
      "Epoch 910/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9342 - acc: 0.5923 - val_loss: 2.0601 - val_acc: 0.5881\n",
      "Epoch 911/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9361 - acc: 0.5926 - val_loss: 2.0589 - val_acc: 0.5883\n",
      "Epoch 912/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9301 - acc: 0.5925 - val_loss: 2.0582 - val_acc: 0.5881\n",
      "Epoch 913/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9365 - acc: 0.5926 - val_loss: 2.0587 - val_acc: 0.5877\n",
      "Epoch 914/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9306 - acc: 0.5922 - val_loss: 2.0591 - val_acc: 0.5881\n",
      "Epoch 915/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9313 - acc: 0.5926 - val_loss: 2.0591 - val_acc: 0.5883\n",
      "Epoch 916/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9307 - acc: 0.5927 - val_loss: 2.0592 - val_acc: 0.5879\n",
      "Epoch 917/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9248 - acc: 0.5932 - val_loss: 2.0593 - val_acc: 0.5881\n",
      "Epoch 918/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9303 - acc: 0.5925 - val_loss: 2.0592 - val_acc: 0.5883\n",
      "Epoch 919/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9322 - acc: 0.5934 - val_loss: 2.0589 - val_acc: 0.5881\n",
      "Epoch 920/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9287 - acc: 0.5933 - val_loss: 2.0592 - val_acc: 0.5879\n",
      "Epoch 921/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9301 - acc: 0.5929 - val_loss: 2.0597 - val_acc: 0.5879\n",
      "Epoch 922/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9272 - acc: 0.5930 - val_loss: 2.0595 - val_acc: 0.5879\n",
      "Epoch 923/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9413 - acc: 0.5925 - val_loss: 2.0597 - val_acc: 0.5879\n",
      "Epoch 924/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9311 - acc: 0.5927 - val_loss: 2.0602 - val_acc: 0.5879\n",
      "Epoch 925/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9329 - acc: 0.5919 - val_loss: 2.0607 - val_acc: 0.5879\n",
      "Epoch 926/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9281 - acc: 0.5924 - val_loss: 2.0610 - val_acc: 0.5879\n",
      "Epoch 927/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9322 - acc: 0.5927 - val_loss: 2.0613 - val_acc: 0.5879\n",
      "Epoch 928/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9290 - acc: 0.5926 - val_loss: 2.0615 - val_acc: 0.5881\n",
      "Epoch 929/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9339 - acc: 0.5928 - val_loss: 2.0620 - val_acc: 0.5879\n",
      "Epoch 930/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9332 - acc: 0.5928 - val_loss: 2.0624 - val_acc: 0.5879\n",
      "Epoch 931/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9266 - acc: 0.5930 - val_loss: 2.0628 - val_acc: 0.5877\n",
      "Epoch 932/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9333 - acc: 0.5927 - val_loss: 2.0626 - val_acc: 0.5877\n",
      "Epoch 933/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9349 - acc: 0.5924 - val_loss: 2.0614 - val_acc: 0.5879\n",
      "Epoch 934/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9354 - acc: 0.5924 - val_loss: 2.0603 - val_acc: 0.5879\n",
      "Epoch 935/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9348 - acc: 0.5928 - val_loss: 2.0601 - val_acc: 0.5881\n",
      "Epoch 936/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9287 - acc: 0.5927 - val_loss: 2.0605 - val_acc: 0.5879\n",
      "Epoch 937/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9302 - acc: 0.5924 - val_loss: 2.0612 - val_acc: 0.5879\n",
      "Epoch 938/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9339 - acc: 0.5929 - val_loss: 2.0616 - val_acc: 0.5885\n",
      "Epoch 939/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9391 - acc: 0.5934 - val_loss: 2.0615 - val_acc: 0.5883\n",
      "Epoch 940/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9312 - acc: 0.5923 - val_loss: 2.0609 - val_acc: 0.5879\n",
      "Epoch 941/1000\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 1.9342 - acc: 0.5925 - val_loss: 2.0588 - val_acc: 0.5883\n",
      "Epoch 942/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9384 - acc: 0.5921 - val_loss: 2.0578 - val_acc: 0.5881\n",
      "Epoch 943/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9289 - acc: 0.5926 - val_loss: 2.0579 - val_acc: 0.5881\n",
      "Epoch 944/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9322 - acc: 0.5923 - val_loss: 2.0580 - val_acc: 0.5883\n",
      "Epoch 945/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9269 - acc: 0.5932 - val_loss: 2.0581 - val_acc: 0.5883\n",
      "Epoch 946/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9292 - acc: 0.5927 - val_loss: 2.0581 - val_acc: 0.5885\n",
      "Epoch 947/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9278 - acc: 0.5927 - val_loss: 2.0585 - val_acc: 0.5883\n",
      "Epoch 948/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9296 - acc: 0.5924 - val_loss: 2.0590 - val_acc: 0.5881\n",
      "Epoch 949/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9354 - acc: 0.5930 - val_loss: 2.0590 - val_acc: 0.5881\n",
      "Epoch 950/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9329 - acc: 0.5927 - val_loss: 2.0591 - val_acc: 0.5877\n",
      "Epoch 951/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9380 - acc: 0.5926 - val_loss: 2.0592 - val_acc: 0.5881\n",
      "Epoch 952/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9366 - acc: 0.5924 - val_loss: 2.0591 - val_acc: 0.5883\n",
      "Epoch 953/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9299 - acc: 0.5934 - val_loss: 2.0592 - val_acc: 0.5881\n",
      "Epoch 954/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9263 - acc: 0.5926 - val_loss: 2.0596 - val_acc: 0.5881\n",
      "Epoch 955/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9259 - acc: 0.5925 - val_loss: 2.0600 - val_acc: 0.5881\n",
      "Epoch 956/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9291 - acc: 0.5930 - val_loss: 2.0601 - val_acc: 0.5885\n",
      "Epoch 957/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9318 - acc: 0.5932 - val_loss: 2.0601 - val_acc: 0.5885\n",
      "Epoch 958/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9266 - acc: 0.5925 - val_loss: 2.0594 - val_acc: 0.5883\n",
      "Epoch 959/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9276 - acc: 0.5931 - val_loss: 2.0588 - val_acc: 0.5883\n",
      "Epoch 960/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9314 - acc: 0.5926 - val_loss: 2.0585 - val_acc: 0.5883\n",
      "Epoch 961/1000\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.9304 - acc: 0.5926 - val_loss: 2.0586 - val_acc: 0.5881\n",
      "Epoch 962/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9307 - acc: 0.5929 - val_loss: 2.0582 - val_acc: 0.5885\n",
      "Epoch 963/1000\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 1.9349 - acc: 0.5927 - val_loss: 2.0578 - val_acc: 0.5881\n",
      "Epoch 964/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9384 - acc: 0.5930 - val_loss: 2.0572 - val_acc: 0.5883\n",
      "Epoch 965/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9314 - acc: 0.5934 - val_loss: 2.0573 - val_acc: 0.5883\n",
      "Epoch 966/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9321 - acc: 0.5922 - val_loss: 2.0573 - val_acc: 0.5881\n",
      "Epoch 967/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9335 - acc: 0.5927 - val_loss: 2.0569 - val_acc: 0.5885\n",
      "Epoch 968/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9274 - acc: 0.5931 - val_loss: 2.0572 - val_acc: 0.5885\n",
      "Epoch 969/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9323 - acc: 0.5926 - val_loss: 2.0578 - val_acc: 0.5881\n",
      "Epoch 970/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9391 - acc: 0.5929 - val_loss: 2.0571 - val_acc: 0.5879\n",
      "Epoch 971/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9299 - acc: 0.5933 - val_loss: 2.0567 - val_acc: 0.5881\n",
      "Epoch 972/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9331 - acc: 0.5922 - val_loss: 2.0566 - val_acc: 0.5881\n",
      "Epoch 973/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9303 - acc: 0.5928 - val_loss: 2.0556 - val_acc: 0.5883\n",
      "Epoch 974/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9342 - acc: 0.5934 - val_loss: 2.0555 - val_acc: 0.5881\n",
      "Epoch 975/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9269 - acc: 0.5932 - val_loss: 2.0558 - val_acc: 0.5881\n",
      "Epoch 976/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9357 - acc: 0.5929 - val_loss: 2.0559 - val_acc: 0.5879\n",
      "Epoch 977/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9297 - acc: 0.5931 - val_loss: 2.0555 - val_acc: 0.5881\n",
      "Epoch 978/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9319 - acc: 0.5931 - val_loss: 2.0557 - val_acc: 0.5885\n",
      "Epoch 979/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9310 - acc: 0.5926 - val_loss: 2.0562 - val_acc: 0.5887\n",
      "Epoch 980/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9277 - acc: 0.5921 - val_loss: 2.0570 - val_acc: 0.5885\n",
      "Epoch 981/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9250 - acc: 0.5929 - val_loss: 2.0575 - val_acc: 0.5887\n",
      "Epoch 982/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9282 - acc: 0.5930 - val_loss: 2.0577 - val_acc: 0.5887\n",
      "Epoch 983/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9291 - acc: 0.5929 - val_loss: 2.0573 - val_acc: 0.5883\n",
      "Epoch 984/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9270 - acc: 0.5935 - val_loss: 2.0569 - val_acc: 0.5879\n",
      "Epoch 985/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9317 - acc: 0.5925 - val_loss: 2.0571 - val_acc: 0.5879\n",
      "Epoch 986/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9254 - acc: 0.5929 - val_loss: 2.0574 - val_acc: 0.5879\n",
      "Epoch 987/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9367 - acc: 0.5933 - val_loss: 2.0577 - val_acc: 0.5877\n",
      "Epoch 988/1000\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.9270 - acc: 0.5928 - val_loss: 2.0581 - val_acc: 0.5881\n",
      "Epoch 989/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9274 - acc: 0.5932 - val_loss: 2.0583 - val_acc: 0.5879\n",
      "Epoch 990/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9305 - acc: 0.5926 - val_loss: 2.0588 - val_acc: 0.5877\n",
      "Epoch 991/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9253 - acc: 0.5934 - val_loss: 2.0591 - val_acc: 0.5877\n",
      "Epoch 992/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9430 - acc: 0.5928 - val_loss: 2.0602 - val_acc: 0.5879\n",
      "Epoch 993/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9426 - acc: 0.5929 - val_loss: 2.0613 - val_acc: 0.5877\n",
      "Epoch 994/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9301 - acc: 0.5931 - val_loss: 2.0617 - val_acc: 0.5877\n",
      "Epoch 995/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9295 - acc: 0.5927 - val_loss: 2.0623 - val_acc: 0.5879\n",
      "Epoch 996/1000\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.9288 - acc: 0.5934 - val_loss: 2.0620 - val_acc: 0.5877\n",
      "Epoch 997/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9308 - acc: 0.5929 - val_loss: 2.0617 - val_acc: 0.5877\n",
      "Epoch 998/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9295 - acc: 0.5927 - val_loss: 2.0609 - val_acc: 0.5879\n",
      "Epoch 999/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9299 - acc: 0.5925 - val_loss: 2.0608 - val_acc: 0.5877\n",
      "Epoch 1000/1000\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 1.9270 - acc: 0.5935 - val_loss: 2.0602 - val_acc: 0.5877\n",
      "Model: \"trans_race\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Encoder)            multiple                  144488    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             multiple                  2210      \n",
      "=================================================================\n",
      "Total params: 146,698\n",
      "Trainable params: 146,698\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "trans_race.compile(\n",
    "    optimizer=opt,\n",
    "    loss=loss,#'categorical_crossentropy', # 'sigmoid_focal_crossentropy'\n",
    "    metrics=['accuracy'], #['categorical_accuracy']\n",
    ")\n",
    "\n",
    "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath='../models/results/transformer.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "history = trans_race.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=1000,\n",
    "    verbose=True, # hide the output because we have so many epochs\n",
    "    callbacks=[model_checkpoint_callback]\n",
    ")\n",
    "print(trans_race.summary())\n",
    "# trans_race.save_weights(\"../models/results/transformer.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb3705ccb70>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot \n",
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "# for loss\n",
    "axL.plot(history.history['loss'],label=\"loss for training\")\n",
    "axL.plot(history.history['val_loss'],label=\"loss for validation\")\n",
    "axL.set_title('model loss')\n",
    "axL.set_xlabel('epoch')\n",
    "axL.set_ylabel('loss')\n",
    "axL.legend(loc='upper right')\n",
    "axR.plot(history.history['acc'],label=\"acc for training\")\n",
    "axR.plot(history.history['val_acc'],label=\"acc for validation\")\n",
    "axR.set_title('model accuracy')\n",
    "axR.set_xlabel('epoch')\n",
    "axR.set_ylabel('accuracy')\n",
    "axR.legend(loc='lower right')\n",
    "# figureの保存\n",
    "# plt.savefig(\"../models/results/trans_race_result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "preds = trans_race.predict(X_valid)\n",
    "y_pred = np.argmax(preds, axis = 2)\n",
    "y_ans = np.argmax(y_valid, axis = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def order_algorithm(preds):\n",
    "#     num_race = preds.shape[0]\n",
    "#     y_preds = np.full((num_race, 24), 25)\n",
    "#     for i in range(num_race):\n",
    "#         one_race = preds[i,:,:]\n",
    "#         init_preds = np.argmax(one_race, axis = -1)\n",
    "#         exist_horse = np.delete(one_race, np.where(init_preds == 25)[0], 0)\n",
    "#         for j in range(1,exist_horse.shape[0]+1):\n",
    "#             one_order = np.argmax(exist_horse[:,j])\n",
    "#             for k in range(one_race.shape[0]):\n",
    "#                 if np.array_equal(one_race[k], exist_horse[one_order]):\n",
    "#                     y_preds[i][k] = j\n",
    "#                     exist_horse = np.delete(exist_horse, one_order, 0)\n",
    "#                     break\n",
    "#     return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.DataFrame(preds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def order_algorithm(preds):\n",
    "#     num_race = preds.shape[0]\n",
    "#     y_preds = np.full((num_race, 24), 25)\n",
    "#     for i in range(num_race): # iterate all race\n",
    "#         one_race = preds[i,:,:] # shape = (24, 26) ,so (num of horse, num of target 0-25)\n",
    "#         init_preds = np.argmax(one_race, axis = -1)\n",
    "#         exist_horse = np.delete(one_race, np.where(init_preds == 25)[0], 0) # shape = (num of exist horse, 26)\n",
    "#         for j in range(1,exist_horse.shape[0]+1): # iterate 1-num of exist horse\n",
    "#             one_order = np.argmax(exist_horse[:,j]) # this is a target order\n",
    "#             for k in range(one_race.shape[0]): # search the horse k = (0, 23)\n",
    "#                 if np.array_equal(one_race[k], exist_horse[one_order]):\n",
    "#                     y_preds[i][k] = j\n",
    "#                     exist_horse = np.delete(exist_horse, one_order, 0)\n",
    "#                     exist_horse[:,j+1] += exist_horse[:,j]\n",
    "#                     one_race[:,j+1] += one_race[:,j]\n",
    "#                     break\n",
    "#     return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202, 24)\n",
      "[ 5  4  8  7  3  1  2  6  9 10 25 25 25 25 25 25 25 25 25 25 25 25 25 25]\n"
     ]
    }
   ],
   "source": [
    "y_preds = order_algorithm(preds)\n",
    "print(y_preds.shape)\n",
    "print(y_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202, 24, 26)\n",
      "(202, 24)\n",
      "(202, 24)\n"
     ]
    }
   ],
   "source": [
    "print(preds.shape)\n",
    "print(y_preds.shape)\n",
    "print(y_ans.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11  4  5 12  8  1  2  9  6 10 25 25 25 25 25 25 25 25 25 25 25 25 25 25]\n",
      "[ 5  4  8  7  3  1  2  6  9 10 25 25 25 25 25 25 25 25 25 25 25 25 25 25]\n",
      "[ 1  1  6  8  3  2  6  5 10  6 25 25 25 25 25 25 25 25 25 25 25 25 25 25]\n"
     ]
    }
   ],
   "source": [
    "print(y_ans[0])\n",
    "print(y_preds[0])\n",
    "print(y_pred[0])\n",
    "# print(np.mean(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  13.450495049504951\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(y_preds.shape[0]):\n",
    "    for j in range(y_preds.shape[1]):\n",
    "        if (y_preds[i][j] == y_ans[i][j]):\n",
    "            correct += 1\n",
    "accuracy = correct / len(y_pred)\n",
    "print(\"accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the distribution of prediction\n",
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "label = ['0','1','2','3','4','5','6','7','8','9',' 10',' 11',' 12',' 13',' 14',' 15',' 16',' 17',' 18',' 19',' 20',' 21',' 22',' 23',' 24']\n",
    "\n",
    "axL.hist(y_preds.flatten(), bins = 25, label = \"pred_order\")##, range = (1,21)\n",
    "axL.set_xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24])\n",
    "axL.set_xticklabels(label)\n",
    "axL.set_title('pred order distribution')\n",
    "axL.legend()\n",
    "axR.hist(y_ans.flatten(), bins = 25, label = \"ans_order\")##, range = (1,21)\n",
    "axR.set_xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24])\n",
    "axR.set_xticklabels(label)\n",
    "axR.set_title('ans order distribution')\n",
    "axR.legend()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_f:  202\n",
      "correct_first:  83\n",
      "precision:  0.41089108910891087\n"
     ]
    }
   ],
   "source": [
    "# precision = TP / (TP + FP)\n",
    "# the accuracy of predected True\n",
    "i = 0\n",
    "correct_first = 0\n",
    "all_f = 0\n",
    "increase = 0\n",
    "# X_valid_inv = standard_scale.inverse_transform(X_valid)\n",
    "# X_valid_inv_df = pd.DataFrame(X_valid_inv)\n",
    "# odds = X_test_inv_df['odds'].values\n",
    "# hit_odds = []\n",
    "# select = []\n",
    "\n",
    "for i in range(y_preds.shape[0]):\n",
    "    for j in range(y_preds.shape[1]):\n",
    "        if (y_preds[i][j] == 1):  # total nubber of predicted 1st (TP + FP)  & (pred[i][1]*odds[i] > 1.0)) | (pred[i][1]> 0.25)\n",
    "            all_f += 1\n",
    "            if (y_ans[i][j] == 1) or (y_ans[i][j] == 2) or (y_ans[i][j] == 3):\n",
    "                correct_first += 1   #　True Positive\n",
    "            \n",
    "# for i in range(len(y_ans)):\n",
    "#     if (y_preds[i] == 1):  # total nubber of predicted 1st (TP + FP)  & (pred[i][1]*odds[i] > 1.0)) | (pred[i][1]> 0.25)\n",
    "#         all_f = all_f + 1\n",
    "#         if (y_ans[i] == 1):\n",
    "#             correct_first = correct_first + 1   #　True Positive\n",
    "# #             increase += odds[i]\n",
    "# #             hit_odds.append(odds[i])\n",
    "            \n",
    "            \n",
    "            \n",
    "print(\"all_f: \", all_f)\n",
    "print(\"correct_first: \", correct_first)\n",
    "# print(\"hit odds average: \", np.array(hit_odds).mean())\n",
    "# print(\"spent money:\", all_f * 100)\n",
    "# revenue = (increase - all_f) * 100\n",
    "# retrive = increase / all_f\n",
    " \n",
    "# print(\"retrive rate: \", retrive) \n",
    "# print(\"revenue: \", revenue)\n",
    "precision = correct_first / all_f\n",
    "print(\"precision: \",precision)\n",
    "#print(\"\\n\".join(map(str,hit_odds)))\n",
    "# print(\"min: \", min(hit_odds))\n",
    "# print(\"mid: \", np.median(np.array(hit_odds)))\n",
    "# print(\"max: \", max(hit_odds))\n",
    "\n",
    "# fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "# axL.hist(hit_odds, bins = 50, label = \"hit_odds\")\n",
    "# axL.set_title('hit odds distribution')\n",
    "# axL.legend()\n",
    "# axR.hist(odds, bins = 50, label = \"odds\", range = (0,40))\n",
    "# axR.set_title('all odds distribution')\n",
    "# axR.legend()\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.2138728323699422\n"
     ]
    }
   ],
   "source": [
    "# Recall = TP / (TP + FN)\n",
    "# the accuracy of label True\n",
    "i = 0\n",
    "correct_first = 0\n",
    "all_f = 0\n",
    "# odds_f = []\n",
    "p_rate_f = []\n",
    "\n",
    "# all_f_odds = []\n",
    "\n",
    "for i in range(y_preds.shape[0]):\n",
    "    for j in range(y_preds.shape[1]):\n",
    "        if (y_ans[i][j] == 1):  # total nubber of predicted 1st (TP + FP)  & (pred[i][1]*odds[i] > 1.0)) | (pred[i][1]> 0.25)\n",
    "            all_f += 1\n",
    "            if (y_preds[i][j] == 1):\n",
    "                correct_first += 1   #　True Positive\n",
    "                \n",
    "                \n",
    "# for i in range(len(y_ans)):\n",
    "#     if (y_ans[i] == 1):  # TP + FN\n",
    "#         all_f = all_f + 1\n",
    "# #         all_f_odds.append(odds[i])\n",
    "#         if (y_preds[i] == 1):\n",
    "#             correct_first = correct_first + 1   #　TP\n",
    "#             odds_f.append(odds[i])\n",
    "#             p_rate_f.append(pred[i][1])\n",
    "\n",
    "# fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "# axL.scatter(p_rate_f, odds_f)  \n",
    "# axL.set_title('correlation odss and prediction')\n",
    "# #axL.xlabel('prediction rate first')\n",
    "# #axL.ylabel('odds')\n",
    "# axR.hist(odds_f, bins = 50, label = \"odds\")\n",
    "# axR.set_title('all first odds distribution')\n",
    "# axR.legend()\n",
    "\n",
    "# fig.show()\n",
    "Recall = correct_first / all_f\n",
    "print(\"Recall: \",Recall)\n",
    "# print(\"all_f_odds average: \", np.array(all_f_odds).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision of 3 renpuku:  0.0558993399339934\n"
     ]
    }
   ],
   "source": [
    "# 3 renpuku\n",
    "within_3 = 0\n",
    "hit = 0\n",
    "\n",
    "for i in range(y_preds.shape[0]):\n",
    "    for j in range(y_preds.shape[1]):\n",
    "        within_3 += 1\n",
    "        if (y_ans[i][j] == 1) or (y_ans[i][j] == 2) or (y_ans[i][j] == 3):  # total nubber of predicted 1st (TP + FP)  & (pred[i][1]*odds[i] > 1.0)) | (pred[i][1]> 0.25)\n",
    "            if (y_preds[i][j] == 1) or (y_preds[i][j] == 2) or (y_preds[i][j] == 3) or (y_preds[i][j] == 4):\n",
    "                hit += 1   #　True Positive\n",
    "                \n",
    "precision = hit / within_3\n",
    "print(\"precision of 3 renpuku: \", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
