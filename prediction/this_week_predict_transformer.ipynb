{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'order_algorithm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-2d76b27909a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# from utils import functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_time_series_data_weekly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_focal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder_algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'order_algorithm'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "sys.path.append('/mnt/c/Users/44yos/RacePrediction/resnet_win5')\n",
    "\n",
    "from pickle import load\n",
    "\n",
    "# from models import resnet\n",
    "from models import transformer\n",
    "\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from tensorflow.python.keras import optimizers\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "\n",
    "# from utils import functions\n",
    "from training.utils import create_time_series_data_weekly, smooth_label, categorical_focal_loss, order_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/csv/2022/May_1/5_1_data.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['race_id'] = data['race_id'].astype(str)\n",
    "data['race_round'] = data['race_round'].astype(str)\n",
    "data['order'] = data['order'].astype(str)\n",
    "data['frame_number'] = data['frame_number'].astype(str)\n",
    "data['horse_number'] = data['horse_number'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race_id               0\n",
      "race_round            0\n",
      "race_title            0\n",
      "ground_condition      0\n",
      "date                  0\n",
      "                     ..\n",
      "ground_type_芝_3       9\n",
      "ground_type_障_3       9\n",
      "horse_weight_dif_3    9\n",
      "same_jockey_3         9\n",
      "same_jockey           0\n",
      "Length: 156, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[[\"race_id\",\"race_title\",\"odds\", \"order\", \"horse_number\", \"goal_time\", \"half_order\", \"last_time\", \"horse_weight\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85, 150)\n"
     ]
    }
   ],
   "source": [
    "data = data.drop([\"horse_number\", \"half_order\", \"goal_time\" ,\"last_time\", \"horse_weight\",  \"frame_number\"], axis = 1)\n",
    "print(data.shape)#\"horse_weight_dif\",,\"pop\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 列の順序変更   \"阪神\":'place_阪神',\"東京\":'place_東京',\"福島\":'place_福島',\n",
    "data = data.rename(columns ={\"中山\":'place_中山',\"新潟\":'place_新潟',\"小倉\":'place_小倉',\"札幌\":'place_札幌',\"函館\":'place_函館',\"京都\":'place_京都',\"中京\":'place_中京'})\n",
    "data = data[['race_id','order',\"race_round\",\"ground_condition\",\"total_horse_number\",\"age\",\"burden_weight\",\"odds\",\"race_rank\",\"distance\",\"ground_type_ダ\",\"ground_type_芝\",\"circle_右\",\"circle_左\",\"weather_circumstance_小雨\",\"weather_circumstance_小雪\",\"weather_circumstance_晴\",\"weather_circumstance_曇\",\"weather_circumstance_雨\",\"weather_circumstance_雪\",\"place_中京\",\"place_中山\",\"place_京都\",\"place_函館\",\"place_小倉\",\"place_新潟\",\"place_札幌\",\"place_東京\",\"place_福島\",\"place_阪神\",\"sex_セ\",\"sex_牝\",\"sex_牡\",\"f_grass_win_rate\",\"f_dart_win_rate\",\"f_win_rate\",\"g_f_grass_win_rate\",\"g_f_dart_win_rate\",\"g_f_win_rate\",\"m_grass_win_rate\",\"m_dart_win_rate\",\"m_win_rate\",\"same_jockey\",\"whole_horse_number_1\",\"odds_1\",\"order_1\",\"burden_weight_1\",\"race_distance_1\",\"ground_condition_1\",\"goal_time_1\",\"half_order_1\",\"last_time_1\",\"horse_weight_1\",\"weather_circumstance_小雨_1\",\"weather_circumstance_小雪_1\",\"weather_circumstance_晴_1\",\"weather_circumstance_曇_1\",\"weather_circumstance_雨_1\",\"weather_circumstance_雪_1\",\"main_place_その他_1\",\"main_place_中京_1\",\"main_place_中山_1\",\"main_place_京都_1\",\"main_place_函館_1\",\"main_place_小倉_1\",\"main_place_新潟_1\",\"main_place_札幌_1\",\"main_place_東京_1\",\"main_place_福島_1\",\"main_place_阪神_1\",\"race_rank_1\",\"ground_type_ダ_1\",\"ground_type_芝_1\",\"ground_type_障_1\",\"horse_weight_dif_1\",\"same_jockey_1\",\"whole_horse_number_2\",\"odds_2\",\"order_2\",\"burden_weight_2\",\"race_distance_2\",\"ground_condition_2\",\"goal_time_2\",\"half_order_2\",\"last_time_2\",\"horse_weight_2\",\"weather_circumstance_小雨_2\",\"weather_circumstance_小雪_2\",\"weather_circumstance_晴_2\",\"weather_circumstance_曇_2\",\"weather_circumstance_雨_2\",\"weather_circumstance_雪_2\",\"main_place_その他_2\",\"main_place_中京_2\",\"main_place_中山_2\",\"main_place_京都_2\",\"main_place_函館_2\",\"main_place_小倉_2\",\"main_place_新潟_2\",\"main_place_札幌_2\",\"main_place_東京_2\",\"main_place_福島_2\",\"main_place_阪神_2\",\"race_rank_2\",\"ground_type_ダ_2\",\"ground_type_芝_2\",\"ground_type_障_2\",\"horse_weight_dif_2\",\"same_jockey_2\",\"whole_horse_number_3\",\"odds_3\",\"order_3\",\"burden_weight_3\",\"race_distance_3\",\"ground_condition_3\",\"goal_time_3\",\"half_order_3\",\"last_time_3\",\"horse_weight_3\",\"weather_circumstance_小雨_3\",\"weather_circumstance_小雪_3\",\"weather_circumstance_晴_3\",\"weather_circumstance_曇_3\",\"weather_circumstance_雨_3\",\"weather_circumstance_雪_3\",\"main_place_その他_3\",\"main_place_中京_3\",\"main_place_中山_3\",\"main_place_京都_3\",\"main_place_函館_3\",\"main_place_小倉_3\",\"main_place_新潟_3\",\"main_place_札幌_3\",\"main_place_東京_3\",\"main_place_福島_3\" ,\"main_place_阪神_3\",\"race_rank_3\",\"ground_type_ダ_3\",\"ground_type_芝_3\",\"ground_type_障_3\",\"horse_weight_dif_3\",\"same_jockey_3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85, 86)\n"
     ]
    }
   ],
   "source": [
    "# standard_scale = StandardScaler()\n",
    "no_scale_data = data[['race_id','order']]\n",
    "scale_columns = data.drop(['race_id','order'], axis=1).columns.values\n",
    "standard_scale = load(open(\"../training/standard_scale.pkl\", \"rb\"))\n",
    "data = pd.DataFrame(standard_scale.fit_transform(data[scale_columns]))\n",
    "\n",
    "# PCA\n",
    "pca = load(open(\"../training/pca.pkl\", \"rb\"))\n",
    "data = pd.DataFrame(pca.fit_transform(data))\n",
    "max_col = 84\n",
    "# print(max_col)\n",
    "data = data.loc[:, :max_col-1]\n",
    "\n",
    "data = pd.concat([data, no_scale_data], axis=1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_time_series_data_weekly(raw_data):\n",
    "#     number_of_race = raw_data.race_id.nunique()\n",
    "#     time_series_data = np.full((number_of_race, 18, 140), 0.0)#-float('inf')\n",
    "#     race_number = 0\n",
    "#     horse_number = 0\n",
    "#     for i in range(len(raw_data)):\n",
    "#         if i == 0:\n",
    "#             time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "#             horse_number += 1\n",
    "#             continue\n",
    "#         if data.iloc[i].race_id != data.iloc[i-1].race_id:\n",
    "#             race_number += 1\n",
    "#             horse_number = 0\n",
    "#             time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "#             horse_number += 1\n",
    "#         else:\n",
    "#             time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "#             horse_number += 1\n",
    "#     return time_series_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 24, 84)\n"
     ]
    }
   ],
   "source": [
    "X = create_time_series_data_weekly(data)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "X = X.astype('float32')\n",
    "print(X.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def categorical_focal_loss(alpha, gamma):\n",
    "#     \"\"\"\n",
    "#     Softmax version of focal loss.\n",
    "#     When there is a skew between different categories/labels in your data set, you can try to apply this function as a\n",
    "#     loss.\n",
    "#            m\n",
    "#       FL = ∑  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n",
    "#           c=1\n",
    "#       where m = number of classes, c = class and o = observation\n",
    "#     Parameters:\n",
    "#       alpha -- the same as weighing factor in balanced cross entropy. Alpha is used to specify the weight of different\n",
    "#       categories/labels, the size of the array needs to be consistent with the number of classes.\n",
    "#       gamma -- focusing parameter for modulating factor (1-p)\n",
    "#     Default value:\n",
    "#       gamma -- 2.0 as mentioned in the paper\n",
    "#       alpha -- 0.25 as mentioned in the paper\n",
    "#     References:\n",
    "#         Official paper: https://arxiv.org/pdf/1708.02002.pdf\n",
    "#         https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy\n",
    "#     Usage:\n",
    "#      model.compile(loss=[categorical_focal_loss(alpha=[[.25, .25, .25]], gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "#     \"\"\"\n",
    "\n",
    "#     alpha = np.array(alpha, dtype=np.float32)\n",
    "\n",
    "#     def categorical_focal_loss_fixed(y_true, y_pred):\n",
    "#         \"\"\"\n",
    "#         :param y_true: A tensor of the same shape as `y_pred`\n",
    "#         :param y_pred: A tensor resulting from a softmax\n",
    "#         :return: Output tensor.\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Clip the prediction value to prevent NaN's and Inf's\n",
    "#         epsilon = K.epsilon()\n",
    "#         y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "#         # Calculate Cross Entropy\n",
    "#         cross_entropy = -y_true * K.log(y_pred)\n",
    "\n",
    "#         # Calculate Focal Loss\n",
    "#         loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
    "\n",
    "#         # Compute mean loss in mini_batch\n",
    "#         return K.mean(K.sum(loss, axis=-1))\n",
    "\n",
    "#     return categorical_focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [0.00000000e+00, 1.21687621e+00, 1.21555368e+00, 1.23063757e+00,\n",
    "         1.24041898e+00, 1.25990362e+00, 1.26100846e+00, 1.28155834e+00,\n",
    "         1.32325490e+00, 1.37157651e+00, 1.45736210e+00, 1.56413643e+00,\n",
    "         1.73521807e+00, 1.98052140e+00, 2.27608819e+00, 2.71833648e+00,\n",
    "         3.58156912e+00, 1.49347181e+01, 2.00918164e+01, 1.25825000e+03,\n",
    "         1.43800000e+03, 2.51650000e+03, 3.35533333e+03, 4.02640000e+03,\n",
    "         5.03300000e+03, 7.53440294e-02]\n",
    "# [0., 1.22620251, 1.21236789, 1.23692263, 1.24838235, 1.27347735, 1.27233213, 1.2771175, 1.33202573, 1.35824, 1.42768248, 1.52624955, 1.69170984, 1.90080609, 2.16445691, 2.54619076, 3.30697312, 14.51111111, 19.42562929, 0.13969294]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yoshi/miniconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7cfedb978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7cfedb978>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7cfedb978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7cfedb978>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7cfedbf28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7cfedbf28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7cfedbf28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7cfedbf28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7cfedbf28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7cfedbf28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7cfedbf28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7cfedbf28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7cfedbf28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7cfedbf28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7cfedbf28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7cfedbf28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7ccf24b70>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7ccf24b70>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7ccf24b70>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7ccf24b70>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7ccf24be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7ccf24be0>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7ccf24be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7ccf24be0>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7ccf24be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7ccf24be0>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7ccf24be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7ccf24be0>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7ccf24be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7ccf24be0>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7ccf24be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7ccf24be0>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method TransRace.call of <models.transformer.TransRace object at 0x7ff7cfedb4a8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TransRace.call of <models.transformer.TransRace object at 0x7ff7cfedb4a8>>, which Python reported as:\n",
      "    def call(self, inputs, training=True):\n",
      "        inp = inputs\n",
      "\n",
      "        enc_padding_mask = self.create_masks(inp) #, tar\n",
      "        # , look_ahead_mask, dec_padding_mask\n",
      "\n",
      "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
      "\n",
      "        final_output = self.final_layer(enc_output)\n",
      "#         print(\"final output shape\", final_output.shape)\n",
      "\n",
      "        return final_output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method TransRace.call of <models.transformer.TransRace object at 0x7ff7cfedb4a8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TransRace.call of <models.transformer.TransRace object at 0x7ff7cfedb4a8>>, which Python reported as:\n",
      "    def call(self, inputs, training=True):\n",
      "        inp = inputs\n",
      "\n",
      "        enc_padding_mask = self.create_masks(inp) #, tar\n",
      "        # , look_ahead_mask, dec_padding_mask\n",
      "\n",
      "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
      "\n",
      "        final_output = self.final_layer(enc_output)\n",
      "#         print(\"final output shape\", final_output.shape)\n",
      "\n",
      "        return final_output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7cfedb978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7cfedb978>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7cfedb978>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7cfedb978>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7cfedbf28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7cfedbf28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7cfedbf28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7cfedbf28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7cfedbf28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7cfedbf28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7cfedbf28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7cfedbf28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7cfedbf28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7cfedbf28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7cfedbf28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7cfedbf28>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7ccf24b70>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7ccf24b70>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7ccf24b70>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7ccf24b70>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7ccf24be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7ccf24be0>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7ccf24be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7ccf24be0>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7ccf24be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7ccf24be0>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7ccf24be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7ccf24be0>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7ccf24be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7ccf24be0>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7ccf24be0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7ccf24be0>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    }
   ],
   "source": [
    "num_layers = 2 # hyperparameter\n",
    "d_model = max_col # 4*35 84=4*3*7\n",
    "num_heads = 28 # hyperparameter *must be a factor of d_model*\n",
    "d_ffn = 16 # hyperparameter\n",
    "pe_input = 24\n",
    "target_size = 26\n",
    "dropout_rate = 0.1 # hyperparameter\n",
    "\n",
    "model1 = transformer.TransRace(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ffn=d_ffn,\n",
    "    pe_input=pe_input,\n",
    "    target_size=target_size,\n",
    "    rate=dropout_rate,\n",
    "                              )\n",
    "opt = optimizers.Adam(decay=0.01)\n",
    "loss = categorical_focal_loss(alpha=[alpha], gamma=0.5)\n",
    "model1.compile(\n",
    "    optimizer=opt,\n",
    "    loss=loss,#'categorical_crossentropy', # 'sigmoid_focal_crossentropy'\n",
    "    metrics=['accuracy'], #['categorical_accuracy']\n",
    ")\n",
    "model1.build(input_shape=(None, X.shape[1], X.shape[2]))\n",
    "model1.load_weights(\"../models/results/transformer1.h5\")\n",
    "pred1 = model1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff770394588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff770394588>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff770394588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff770394588>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff770394908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff770394908>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff770394908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff770394908>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff770394908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff770394908>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff770394908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff770394908>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff770394908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff770394908>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff770394908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff770394908>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff77032c630>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff77032c630>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff77032c630>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff77032c630>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff77032c128>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff77032c128>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff77032c128>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff77032c128>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff77032c128>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff77032c128>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff77032c128>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff77032c128>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff77032c128>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff77032c128>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff77032c128>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff77032c128>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method TransRace.call of <models.transformer.TransRace object at 0x7ff7b5b04208>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TransRace.call of <models.transformer.TransRace object at 0x7ff7b5b04208>>, which Python reported as:\n",
      "    def call(self, inputs, training=True):\n",
      "        inp = inputs\n",
      "\n",
      "        enc_padding_mask = self.create_masks(inp) #, tar\n",
      "        # , look_ahead_mask, dec_padding_mask\n",
      "\n",
      "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
      "\n",
      "        final_output = self.final_layer(enc_output)\n",
      "#         print(\"final output shape\", final_output.shape)\n",
      "\n",
      "        return final_output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method TransRace.call of <models.transformer.TransRace object at 0x7ff7b5b04208>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TransRace.call of <models.transformer.TransRace object at 0x7ff7b5b04208>>, which Python reported as:\n",
      "    def call(self, inputs, training=True):\n",
      "        inp = inputs\n",
      "\n",
      "        enc_padding_mask = self.create_masks(inp) #, tar\n",
      "        # , look_ahead_mask, dec_padding_mask\n",
      "\n",
      "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
      "\n",
      "        final_output = self.final_layer(enc_output)\n",
      "#         print(\"final output shape\", final_output.shape)\n",
      "\n",
      "        return final_output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff770394588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff770394588>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff770394588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff770394588>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff770394908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff770394908>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff770394908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff770394908>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff770394908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff770394908>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff770394908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff770394908>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff770394908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff770394908>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff770394908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff770394908>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff77032c630>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff77032c630>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff77032c630>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff77032c630>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff77032c128>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff77032c128>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff77032c128>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff77032c128>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff77032c128>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff77032c128>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff77032c128>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff77032c128>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff77032c128>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff77032c128>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff77032c128>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff77032c128>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    }
   ],
   "source": [
    "num_layers = 2 # hyperparameter\n",
    "d_model = max_col # 4*35\n",
    "num_heads = 21 # hyperparameter *must be a factor of d_model*\n",
    "d_ffn = 16 # hyperparameter\n",
    "pe_input = 24\n",
    "target_size = 26\n",
    "dropout_rate = 0.1 # hyperparameter\n",
    "\n",
    "model2 = transformer.TransRace(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ffn=d_ffn,\n",
    "    pe_input=pe_input,\n",
    "    target_size=target_size,\n",
    "    rate=dropout_rate,\n",
    "                              )\n",
    "opt = optimizers.Adam(decay=0.01)\n",
    "loss = categorical_focal_loss(alpha=[alpha], gamma=0.5)\n",
    "model2.compile(\n",
    "    optimizer=opt,\n",
    "    loss=loss,#'categorical_crossentropy', # 'sigmoid_focal_crossentropy'\n",
    "    metrics=['accuracy'], #['categorical_accuracy']\n",
    ")\n",
    "model2.build(input_shape=(None, X.shape[1], X.shape[2]))\n",
    "model2.load_weights(\"../models/results/transformer2.h5\")\n",
    "pred2 = model2.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7682b44e0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7682b44e0>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7682b44e0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7682b44e0>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7682b4518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7682b4518>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7682b4518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7682b4518>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7682b4518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7682b4518>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7682b4518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7682b4518>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7682b4518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7682b4518>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7682b4518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7682b4518>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method TransRace.call of <models.transformer.TransRace object at 0x7ff770105a20>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TransRace.call of <models.transformer.TransRace object at 0x7ff770105a20>>, which Python reported as:\n",
      "    def call(self, inputs, training=True):\n",
      "        inp = inputs\n",
      "\n",
      "        enc_padding_mask = self.create_masks(inp) #, tar\n",
      "        # , look_ahead_mask, dec_padding_mask\n",
      "\n",
      "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
      "\n",
      "        final_output = self.final_layer(enc_output)\n",
      "#         print(\"final output shape\", final_output.shape)\n",
      "\n",
      "        return final_output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method TransRace.call of <models.transformer.TransRace object at 0x7ff770105a20>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TransRace.call of <models.transformer.TransRace object at 0x7ff770105a20>>, which Python reported as:\n",
      "    def call(self, inputs, training=True):\n",
      "        inp = inputs\n",
      "\n",
      "        enc_padding_mask = self.create_masks(inp) #, tar\n",
      "        # , look_ahead_mask, dec_padding_mask\n",
      "\n",
      "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
      "\n",
      "        final_output = self.final_layer(enc_output)\n",
      "#         print(\"final output shape\", final_output.shape)\n",
      "\n",
      "        return final_output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7682b44e0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7682b44e0>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7682b44e0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7682b44e0>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7682b4518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7682b4518>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7682b4518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7682b4518>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7682b4518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7682b4518>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7682b4518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7682b4518>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7682b4518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7682b4518>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7682b4518>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7682b4518>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    }
   ],
   "source": [
    "num_layers = 1 # hyperparameter\n",
    "d_model = max_col # 4*35\n",
    "num_heads = 21 # hyperparameter *must be a factor of d_model*\n",
    "d_ffn = 64 # hyperparameter\n",
    "pe_input = 24\n",
    "target_size = 26\n",
    "dropout_rate = 0.1 # hyperparameter\n",
    "\n",
    "model3 = transformer.TransRace(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ffn=d_ffn,\n",
    "    pe_input=pe_input,\n",
    "    target_size=target_size,\n",
    "    rate=dropout_rate,\n",
    "                              )\n",
    "opt = optimizers.Adam(decay=0.01)\n",
    "loss = categorical_focal_loss(alpha=[alpha], gamma=0.5)\n",
    "model3.compile(\n",
    "    optimizer=opt,\n",
    "    loss=loss,#'categorical_crossentropy', # 'sigmoid_focal_crossentropy'\n",
    "    metrics=['accuracy'], #['categorical_accuracy']\n",
    ")\n",
    "model3.build(input_shape=(None, X.shape[1], X.shape[2]))\n",
    "model3.load_weights(\"../models/results/transformer3.h5\")\n",
    "pred3 = model3.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7587c6320>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7587c6320>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7587c6320>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7587c6320>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7587c6160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7587c6160>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7587c6160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7587c6160>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7587c6160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7587c6160>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7587c6160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7587c6160>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7587c6160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7587c6160>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7587c6160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7587c6160>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method TransRace.call of <models.transformer.TransRace object at 0x7ff768243550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TransRace.call of <models.transformer.TransRace object at 0x7ff768243550>>, which Python reported as:\n",
      "    def call(self, inputs, training=True):\n",
      "        inp = inputs\n",
      "\n",
      "        enc_padding_mask = self.create_masks(inp) #, tar\n",
      "        # , look_ahead_mask, dec_padding_mask\n",
      "\n",
      "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
      "\n",
      "        final_output = self.final_layer(enc_output)\n",
      "#         print(\"final output shape\", final_output.shape)\n",
      "\n",
      "        return final_output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method TransRace.call of <models.transformer.TransRace object at 0x7ff768243550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TransRace.call of <models.transformer.TransRace object at 0x7ff768243550>>, which Python reported as:\n",
      "    def call(self, inputs, training=True):\n",
      "        inp = inputs\n",
      "\n",
      "        enc_padding_mask = self.create_masks(inp) #, tar\n",
      "        # , look_ahead_mask, dec_padding_mask\n",
      "\n",
      "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
      "\n",
      "        final_output = self.final_layer(enc_output)\n",
      "#         print(\"final output shape\", final_output.shape)\n",
      "\n",
      "        return final_output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7587c6320>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7587c6320>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7587c6320>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff7587c6320>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7587c6160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7587c6160>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7587c6160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7587c6160>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7587c6160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7587c6160>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7587c6160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7587c6160>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7587c6160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7587c6160>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7587c6160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff7587c6160>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    }
   ],
   "source": [
    "num_layers = 1 # hyperparameter\n",
    "d_model = max_col # 4*35\n",
    "num_heads = 21 # hyperparameter *must be a factor of d_model*\n",
    "d_ffn = 64 # hyperparameter\n",
    "pe_input = 24\n",
    "target_size = 26\n",
    "dropout_rate = 0.1 # hyperparameter\n",
    "\n",
    "model4 = transformer.TransRace(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ffn=d_ffn,\n",
    "    pe_input=pe_input,\n",
    "    target_size=target_size,\n",
    "    rate=dropout_rate,\n",
    "                              )\n",
    "opt = optimizers.Adam(decay=0.01)\n",
    "loss = categorical_focal_loss(alpha=[alpha], gamma=0.5)\n",
    "model4.compile(\n",
    "    optimizer=opt,\n",
    "    loss=loss,#'categorical_crossentropy', # 'sigmoid_focal_crossentropy'\n",
    "    metrics=['accuracy'], #['categorical_accuracy']\n",
    ")\n",
    "model4.build(input_shape=(None, X.shape[1], X.shape[2]))\n",
    "model4.load_weights(\"../models/results/transformer4.h5\")\n",
    "pred4 = model4.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff75853ba20>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff75853ba20>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff75853ba20>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff75853ba20>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff75853b160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff75853b160>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff75853b160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff75853b160>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff75853b160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff75853b160>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff75853b160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff75853b160>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff75853b160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff75853b160>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff75853b160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff75853b160>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method TransRace.call of <models.transformer.TransRace object at 0x7ff75853ba58>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TransRace.call of <models.transformer.TransRace object at 0x7ff75853ba58>>, which Python reported as:\n",
      "    def call(self, inputs, training=True):\n",
      "        inp = inputs\n",
      "\n",
      "        enc_padding_mask = self.create_masks(inp) #, tar\n",
      "        # , look_ahead_mask, dec_padding_mask\n",
      "\n",
      "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
      "\n",
      "        final_output = self.final_layer(enc_output)\n",
      "#         print(\"final output shape\", final_output.shape)\n",
      "\n",
      "        return final_output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method TransRace.call of <models.transformer.TransRace object at 0x7ff75853ba58>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method TransRace.call of <models.transformer.TransRace object at 0x7ff75853ba58>>, which Python reported as:\n",
      "    def call(self, inputs, training=True):\n",
      "        inp = inputs\n",
      "\n",
      "        enc_padding_mask = self.create_masks(inp) #, tar\n",
      "        # , look_ahead_mask, dec_padding_mask\n",
      "\n",
      "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
      "\n",
      "        final_output = self.final_layer(enc_output)\n",
      "#         print(\"final output shape\", final_output.shape)\n",
      "\n",
      "        return final_output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff75853ba20>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff75853ba20>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff75853ba20>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <models.transformer.EncoderLayer object at 0x7ff75853ba20>>, which Python reported as:\n",
      "    def call(self, x, training, mask):\n",
      "#         print(x.shape,\"mha\")\n",
      "        attn_output = self.mha(x, x, x, mask=mask)  # (batch_size, input_seq_len, d_model)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
      "\n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff75853b160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff75853b160>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff75853b160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff75853b160>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff75853b160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff75853b160>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff75853b160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff75853b160>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff75853b160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff75853b160>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff75853b160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MultiHeadAttention.split_heads of <models.transformer.MultiHeadAttention object at 0x7ff75853b160>>, which Python reported as:\n",
      "    def split_heads(self, x, batch_size):\n",
      "        \"\"\"Split the last dimension into (num_heads, depth).\n",
      "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
      "        \"\"\"\n",
      "        x = tf.reshape(x, (batch_size, self.seq_len, self.num_heads, self.depth))\n",
      "#         print(x.shape,\"split_head\")\n",
      "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    }
   ],
   "source": [
    "num_layers = 1 # hyperparameter\n",
    "d_model = max_col # 4*35\n",
    "num_heads = 21 # hyperparameter *must be a factor of d_model*\n",
    "d_ffn = 128 # hyperparameter\n",
    "pe_input = 24\n",
    "target_size = 26\n",
    "dropout_rate = 0.1 # hyperparameter\n",
    "\n",
    "model5 = transformer.TransRace(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ffn=d_ffn,\n",
    "    pe_input=pe_input,\n",
    "    target_size=target_size,\n",
    "    rate=dropout_rate,\n",
    "                              )\n",
    "opt = optimizers.Adam(decay=0.01)\n",
    "loss = categorical_focal_loss(alpha=[alpha], gamma=0.5)\n",
    "model5.compile(\n",
    "    optimizer=opt,\n",
    "    loss=loss,#'categorical_crossentropy', # 'sigmoid_focal_crossentropy'\n",
    "    metrics=['accuracy'], #['categorical_accuracy']\n",
    ")\n",
    "model5.build(input_shape=(None, X.shape[1], X.shape[2]))\n",
    "model5.load_weights(\"../models/results/transformer5.h5\")\n",
    "pred5 = model5.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 26)\n",
      "(144,)\n"
     ]
    }
   ],
   "source": [
    "log_pred1 = np.log(pred1)\n",
    "log_pred2 = np.log(pred2)\n",
    "log_pred3 = np.log(pred3)\n",
    "log_pred4 = np.log(pred4)\n",
    "log_pred5 = np.log(pred5)\n",
    "# log_pred6 = np.log(pred6)\n",
    "# log_pred7 = np.log(pred7)\n",
    "# log_pred8 = np.log(pred8)\n",
    "# log_pred9 = np.log(pred9)\n",
    "# log_pred10 = np.log(pred10)\n",
    "\n",
    "sum_pred = (pred1 + pred2 + pred3 + pred4 + pred5) * 100 / 5 #log_pred1 + log_pred2 + log_pred3 + log_pred4 + log_pred5 # + log_pred6 + log_pred7 + log_pred8 + log_pred9 + log_pred10\n",
    "pred_order = (order_algorithm(sum_pred)).flatten()\n",
    "sum_pred = sum_pred.reshape([-1, 26])\n",
    "print(sum_pred.shape)\n",
    "# pred_order = np.argmax(sum_pred, axis = -1)\n",
    "print(pred_order.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(sum_pred, columns = ['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25'])\n",
    "pred_order_df = pd.DataFrame(pred_order, columns = [\"pred\"])\n",
    "summary = labels\n",
    "summary = pd.concat([summary,pred_order_df], axis = 1)\n",
    "summary = pd.concat([summary,pred_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            race_id race_title  odds order horse_number goal_time half_order  \\\n",
      "0   202203010610&rf      喜多方特別   2.8     ?            1         ?          ?   \n",
      "1   202203010610&rf      喜多方特別   5.7     ?            2         ?          ?   \n",
      "2   202203010610&rf      喜多方特別  37.1     ?            3         ?          ?   \n",
      "3   202203010610&rf      喜多方特別  38.3     ?            4         ?          ?   \n",
      "4   202203010610&rf      喜多方特別  12.3     ?            5         ?          ?   \n",
      "5   202203010610&rf      喜多方特別   8.7     ?            6         ?          ?   \n",
      "6   202203010610&rf      喜多方特別  26.0     ?            7         ?          ?   \n",
      "7   202203010610&rf      喜多方特別  36.9     ?            8         ?          ?   \n",
      "8   202203010610&rf      喜多方特別   0.0     ?            9         ?          ?   \n",
      "9   202203010610&rf      喜多方特別  32.6     ?           10         ?          ?   \n",
      "10  202203010610&rf      喜多方特別  10.7     ?           11         ?          ?   \n",
      "11  202203010610&rf      喜多方特別  17.6     ?           12         ?          ?   \n",
      "12  202203010610&rf      喜多方特別  50.4     ?           13         ?          ?   \n",
      "13  202203010610&rf      喜多方特別   5.0     ?           14         ?          ?   \n",
      "14  202203010610&rf      喜多方特別  84.5     ?           15         ?          ?   \n",
      "15  202203010611&rf     吾妻小富士S  25.9     ?            1         ?          ?   \n",
      "16  202203010611&rf     吾妻小富士S  72.9     ?            2         ?          ?   \n",
      "17  202203010611&rf     吾妻小富士S   4.6     ?            3         ?          ?   \n",
      "18  202203010611&rf     吾妻小富士S   7.0     ?            4         ?          ?   \n",
      "19  202203010611&rf     吾妻小富士S  26.1     ?            5         ?          ?   \n",
      "\n",
      "   last_time horse_weight  pred  ...         16        17        18        19  \\\n",
      "0          ?            ?     2  ...   5.031551  1.006251  0.291906  0.179282   \n",
      "1          ?            ?     3  ...   6.213404  0.890594  0.663593  0.115639   \n",
      "2          ?            ?     5  ...   8.967546  1.182817  2.003972  0.229874   \n",
      "3          ?            ?    10  ...   6.610259  1.120148  0.677035  0.457582   \n",
      "4          ?            ?    13  ...  11.622751  1.786587  1.765171  0.904526   \n",
      "5          ?            ?    12  ...  11.816252  3.862069  2.347540  1.070039   \n",
      "6          ?            ?    11  ...  10.624472  1.334856  1.061819  0.560758   \n",
      "7          ?            ?     7  ...   7.663240  1.146833  0.920804  0.331841   \n",
      "8          ?            ?     8  ...   6.353547  1.405877  1.276673  0.439416   \n",
      "9          ?            ?    25  ...   4.256346  0.554884  0.390496  0.363151   \n",
      "10         ?            ?    25  ...   3.420818  0.298492  0.166682  0.097650   \n",
      "11         ?            ?     6  ...   6.297850  2.614011  1.879664  0.980332   \n",
      "12         ?            ?     1  ...   5.798984  1.204180  1.032045  0.485052   \n",
      "13         ?            ?     9  ...  11.232073  1.115876  0.762270  0.421227   \n",
      "14         ?            ?     4  ...   7.240899  1.000371  0.687774  0.213859   \n",
      "15         ?            ?    25  ...   2.043996  0.161391  0.113633  0.074810   \n",
      "16         ?            ?    25  ...   2.079079  0.135000  0.111117  0.050295   \n",
      "17         ?            ?    25  ...   1.071366  0.077573  0.052568  0.021108   \n",
      "18         ?            ?    25  ...   0.866426  0.093304  0.047963  0.018910   \n",
      "19         ?            ?    25  ...   0.926172  0.142393  0.081010  0.028091   \n",
      "\n",
      "          20        21        22        23        24         25  \n",
      "0   0.023490  0.045426  0.019698  0.022738  0.018261   2.332274  \n",
      "1   0.014224  0.038249  0.017180  0.013337  0.020584   0.801450  \n",
      "2   0.059576  0.081445  0.333707  0.137633  0.036062   0.406196  \n",
      "3   0.052284  0.093767  0.066541  0.070290  0.175931   4.474506  \n",
      "4   0.083866  0.031930  0.340476  0.205637  0.352628   0.311136  \n",
      "5   0.138726  0.095472  0.086800  0.115631  0.038358   3.079204  \n",
      "6   0.024844  0.046861  0.055824  0.034535  0.034875   0.479287  \n",
      "7   0.038771  0.034817  0.044112  0.027409  0.077881   0.572298  \n",
      "8   0.299849  0.189930  0.098917  0.070470  0.033864   0.653623  \n",
      "9   0.055438  0.062412  0.130125  0.032231  0.014662  12.985827  \n",
      "10  0.013417  0.020871  0.017015  0.009791  0.014038  17.311779  \n",
      "11  0.100046  0.090279  0.061523  0.054871  0.032388   1.489123  \n",
      "12  0.205438  0.064521  0.057804  0.045037  0.043043   1.845311  \n",
      "13  0.034331  0.036816  0.063147  0.035049  0.068045   1.278160  \n",
      "14  0.028353  0.069626  0.026665  0.057471  0.030546   4.142753  \n",
      "15  0.007883  0.028456  0.027875  0.030972  0.008225  72.882011  \n",
      "16  0.004932  0.017355  0.015316  0.021409  0.007028  72.683968  \n",
      "17  0.004296  0.011725  0.010863  0.010406  0.003440  76.755905  \n",
      "18  0.004307  0.011201  0.011304  0.013067  0.003051  81.145004  \n",
      "19  0.004413  0.012898  0.018536  0.027072  0.004761  82.336716  \n",
      "\n",
      "[20 rows x 36 columns]\n",
      "    pred\n",
      "0      2\n",
      "1      3\n",
      "2      5\n",
      "3     10\n",
      "4     13\n",
      "5     12\n",
      "6     11\n",
      "7      7\n",
      "8      8\n",
      "9     25\n",
      "10    25\n",
      "11     6\n",
      "12     1\n",
      "13     9\n",
      "14     4\n",
      "15    25\n",
      "16    25\n",
      "17    25\n",
      "18    25\n",
      "19    25\n"
     ]
    }
   ],
   "source": [
    "print(summary.head(20))\n",
    "print(pred_order_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary[[\"race_title\",\"odds\",\"horse_number\",\"pred\",'1','2','3']].to_csv(\"../../weekly_prediction/2022/May_1/5_1_race_pred_v2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary[[\"pred\",'1','2','3']].to_csv(\"csv/data/jan_1/1_24_race_pred_v2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary[[\"pred\",'1','2','3']].to_csv(\"csv/data/jan_1/1_24_race_pred_v2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary[[\"pred\",'1','2','3']].to_csv(\"csv/data/jan_1/1_24_race_pred_v2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary[[\"pred\",'1','2','3']].to_csv(\"csv/data/jan_1/1_24_race_pred_v2.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
