{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "sys.path.append('/mnt/c/Users/44yos/RacePrediction/resnet_win5')\n",
    "\n",
    "from pickle import load\n",
    "\n",
    "# from models import resnet\n",
    "from models import transformer\n",
    "\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/csv/2022/May_1/5_1_data.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['race_id'] = data['race_id'].astype(str)\n",
    "data['race_round'] = data['race_round'].astype(str)\n",
    "#data['total_horse_number'] = data['total_horse_number'].astype(str)\n",
    "data['order'] = data['order'].astype(str)\n",
    "data['frame_number'] = data['frame_number'].astype(str)\n",
    "data['horse_number'] = data['horse_number'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race_id               0\n",
      "race_round            0\n",
      "race_title            0\n",
      "ground_condition      0\n",
      "date                  0\n",
      "                     ..\n",
      "ground_type_芝_3       9\n",
      "ground_type_障_3       9\n",
      "horse_weight_dif_3    9\n",
      "same_jockey_3         9\n",
      "same_jockey           0\n",
      "Length: 156, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[[\"race_id\",\"race_title\",\"odds\", \"order\", \"horse_number\", \"goal_time\", \"half_order\", \"last_time\", \"horse_weight\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85, 150)\n"
     ]
    }
   ],
   "source": [
    "data = data.drop([\"horse_number\", \"half_order\", \"goal_time\" ,\"last_time\", \"horse_weight\",  \"frame_number\"], axis = 1)\n",
    "print(data.shape)#\"horse_weight_dif\",,\"pop\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 列の順序変更   \"阪神\":'place_阪神',\"東京\":'place_東京',\"福島\":'place_福島',\n",
    "data = data.rename(columns ={\"中山\":'place_中山',\"新潟\":'place_新潟',\"小倉\":'place_小倉',\"札幌\":'place_札幌',\"函館\":'place_函館',\"京都\":'place_京都',\"中京\":'place_中京'})\n",
    "data = data[['race_id','order',\"race_round\",\"ground_condition\",\"total_horse_number\",\"age\",\"burden_weight\",\"odds\",\"race_rank\",\"distance\",\"ground_type_ダ\",\"ground_type_芝\",\"circle_右\",\"circle_左\",\"weather_circumstance_小雨\",\"weather_circumstance_小雪\",\"weather_circumstance_晴\",\"weather_circumstance_曇\",\"weather_circumstance_雨\",\"weather_circumstance_雪\",\"place_中京\",\"place_中山\",\"place_京都\",\"place_函館\",\"place_小倉\",\"place_新潟\",\"place_札幌\",\"place_東京\",\"place_福島\",\"place_阪神\",\"sex_セ\",\"sex_牝\",\"sex_牡\",\"f_grass_win_rate\",\"f_dart_win_rate\",\"f_win_rate\",\"g_f_grass_win_rate\",\"g_f_dart_win_rate\",\"g_f_win_rate\",\"m_grass_win_rate\",\"m_dart_win_rate\",\"m_win_rate\",\"same_jockey\",\"whole_horse_number_1\",\"odds_1\",\"order_1\",\"burden_weight_1\",\"race_distance_1\",\"ground_condition_1\",\"goal_time_1\",\"half_order_1\",\"last_time_1\",\"horse_weight_1\",\"weather_circumstance_小雨_1\",\"weather_circumstance_小雪_1\",\"weather_circumstance_晴_1\",\"weather_circumstance_曇_1\",\"weather_circumstance_雨_1\",\"weather_circumstance_雪_1\",\"main_place_その他_1\",\"main_place_中京_1\",\"main_place_中山_1\",\"main_place_京都_1\",\"main_place_函館_1\",\"main_place_小倉_1\",\"main_place_新潟_1\",\"main_place_札幌_1\",\"main_place_東京_1\",\"main_place_福島_1\",\"main_place_阪神_1\",\"race_rank_1\",\"ground_type_ダ_1\",\"ground_type_芝_1\",\"ground_type_障_1\",\"horse_weight_dif_1\",\"same_jockey_1\",\"whole_horse_number_2\",\"odds_2\",\"order_2\",\"burden_weight_2\",\"race_distance_2\",\"ground_condition_2\",\"goal_time_2\",\"half_order_2\",\"last_time_2\",\"horse_weight_2\",\"weather_circumstance_小雨_2\",\"weather_circumstance_小雪_2\",\"weather_circumstance_晴_2\",\"weather_circumstance_曇_2\",\"weather_circumstance_雨_2\",\"weather_circumstance_雪_2\",\"main_place_その他_2\",\"main_place_中京_2\",\"main_place_中山_2\",\"main_place_京都_2\",\"main_place_函館_2\",\"main_place_小倉_2\",\"main_place_新潟_2\",\"main_place_札幌_2\",\"main_place_東京_2\",\"main_place_福島_2\",\"main_place_阪神_2\",\"race_rank_2\",\"ground_type_ダ_2\",\"ground_type_芝_2\",\"ground_type_障_2\",\"horse_weight_dif_2\",\"same_jockey_2\",\"whole_horse_number_3\",\"odds_3\",\"order_3\",\"burden_weight_3\",\"race_distance_3\",\"ground_condition_3\",\"goal_time_3\",\"half_order_3\",\"last_time_3\",\"horse_weight_3\",\"weather_circumstance_小雨_3\",\"weather_circumstance_小雪_3\",\"weather_circumstance_晴_3\",\"weather_circumstance_曇_3\",\"weather_circumstance_雨_3\",\"weather_circumstance_雪_3\",\"main_place_その他_3\",\"main_place_中京_3\",\"main_place_中山_3\",\"main_place_京都_3\",\"main_place_函館_3\",\"main_place_小倉_3\",\"main_place_新潟_3\",\"main_place_札幌_3\",\"main_place_東京_3\",\"main_place_福島_3\" ,\"main_place_阪神_3\",\"race_rank_3\",\"ground_type_ダ_3\",\"ground_type_芝_3\",\"ground_type_障_3\",\"horse_weight_dif_3\",\"same_jockey_3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85, 142)\n"
     ]
    }
   ],
   "source": [
    "# standard_scale = StandardScaler()\n",
    "no_scale_data = data[['race_id','order']]\n",
    "scale_columns = data.drop(['race_id','order'], axis=1).columns.values\n",
    "standard_scale = load(open(\"../training/standard_scale.pkl\", \"rb\"))\n",
    "data = pd.DataFrame(standard_scale.fit_transform(data[scale_columns]))\n",
    "data = pd.concat([data, no_scale_data], axis=1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_series_data(raw_data):\n",
    "    number_of_race = raw_data.race_id.nunique()\n",
    "    time_series_data = np.full((number_of_race, 18, 140), 0.0)#-float('inf')\n",
    "#     label = np.full((number_of_race, 18), 19)\n",
    "    race_number = 0\n",
    "    horse_number = 0\n",
    "    for i in range(len(raw_data)):\n",
    "        if i == 0:\n",
    "#             print(race_number)\n",
    "#             print(horse_number)\n",
    "#             print(raw_data.iloc[i].order)\n",
    "#             label[race_number][horse_number] = float(raw_data.iloc[i].order)\n",
    "            time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "            horse_number += 1\n",
    "            continue\n",
    "        if data.iloc[i].race_id != data.iloc[i-1].race_id:\n",
    "            race_number += 1\n",
    "            horse_number = 0\n",
    "#             label[race_number][horse_number] = float(raw_data.iloc[i].order)\n",
    "            time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "            horse_number += 1\n",
    "        else:\n",
    "#             label[race_number][horse_number] = float(raw_data.iloc[i].order)\n",
    "            time_series_data[race_number][horse_number] = raw_data.iloc[i].drop(['race_id','order'])\n",
    "            horse_number += 1\n",
    "    return time_series_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 18, 140)\n"
     ]
    }
   ],
   "source": [
    "X = create_time_series_data(data)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "X = X.astype('float32')\n",
    "print(X.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_focal_loss(alpha, gamma):\n",
    "    \"\"\"\n",
    "    Softmax version of focal loss.\n",
    "    When there is a skew between different categories/labels in your data set, you can try to apply this function as a\n",
    "    loss.\n",
    "           m\n",
    "      FL = ∑  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n",
    "          c=1\n",
    "      where m = number of classes, c = class and o = observation\n",
    "    Parameters:\n",
    "      alpha -- the same as weighing factor in balanced cross entropy. Alpha is used to specify the weight of different\n",
    "      categories/labels, the size of the array needs to be consistent with the number of classes.\n",
    "      gamma -- focusing parameter for modulating factor (1-p)\n",
    "    Default value:\n",
    "      gamma -- 2.0 as mentioned in the paper\n",
    "      alpha -- 0.25 as mentioned in the paper\n",
    "    References:\n",
    "        Official paper: https://arxiv.org/pdf/1708.02002.pdf\n",
    "        https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy\n",
    "    Usage:\n",
    "     model.compile(loss=[categorical_focal_loss(alpha=[[.25, .25, .25]], gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "\n",
    "    alpha = np.array(alpha, dtype=np.float32)\n",
    "\n",
    "    def categorical_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred: A tensor resulting from a softmax\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        # Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "        # Calculate Cross Entropy\n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "\n",
    "        # Calculate Focal Loss\n",
    "        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
    "\n",
    "        # Compute mean loss in mini_batch\n",
    "        return K.mean(K.sum(loss, axis=-1))\n",
    "\n",
    "    return categorical_focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [0., 1.22620251, 1.21236789, 1.23692263, 1.24838235, 1.27347735, 1.27233213, 1.2771175, 1.33202573, 1.35824, 1.42768248, 1.52624955, 1.69170984, 1.90080609, 2.16445691, 2.54619076, 3.30697312, 14.51111111, 19.42562929, 0.13969294]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 1 # hyperparameter\n",
    "d_model = 140 # 4*35\n",
    "num_heads = 20 # hyperparameter *must be a factor of d_model*\n",
    "d_ffn = 8 # hyperparameter\n",
    "pe_input = 18\n",
    "target_size = 20\n",
    "dropout_rate = 0.1 # hyperparameter\n",
    "\n",
    "model1 = transformer.TransRace(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ffn=d_ffn,\n",
    "    pe_input=pe_input,\n",
    "    target_size=target_size,\n",
    "    rate=dropout_rate,\n",
    "                              )\n",
    "loss = categorical_focal_loss(alpha=[alpha], gamma=0.5)\n",
    "model1.compile(\n",
    "    optimizer='Adam',\n",
    "    loss=loss,#'categorical_crossentropy', # 'sigmoid_focal_crossentropy'\n",
    "    metrics=['accuracy'], #['categorical_accuracy']\n",
    ")\n",
    "model1.build(input_shape=(None, X.shape[1], X.shape[2]))\n",
    "model1.load_weights(\"../models/results/transfomer1.h5\")\n",
    "pred1 = model1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask shape after creating (?, 1, 1, 18)\n",
      "mask shape after creating (?, 1, 1, 18)\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "final output shape (?, 18, 20)\n",
      "mask shape after creating (?, 1, 1, 18)\n",
      "mask shape after creating (?, 1, 1, 18)\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "final output shape (?, 18, 20)\n"
     ]
    }
   ],
   "source": [
    "num_layers = 2 # hyperparameter\n",
    "d_model = 140 # 4*35\n",
    "num_heads = 20 # hyperparameter *must be a factor of d_model*\n",
    "d_ffn = 8 # hyperparameter\n",
    "pe_input = 18\n",
    "target_size = 20\n",
    "dropout_rate = 0.1 # hyperparameter\n",
    "\n",
    "model2 = transformer.TransRace(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ffn=d_ffn,\n",
    "    pe_input=pe_input,\n",
    "    target_size=target_size,\n",
    "    rate=dropout_rate,\n",
    "                              )\n",
    "loss = categorical_focal_loss(alpha=[alpha], gamma=0.5)\n",
    "model2.compile(\n",
    "    optimizer='Adam',\n",
    "    loss=loss,#'categorical_crossentropy', # 'sigmoid_focal_crossentropy'\n",
    "    metrics=['accuracy'], #['categorical_accuracy']\n",
    ")\n",
    "model2.build(input_shape=(None, X.shape[1], X.shape[2]))\n",
    "model2.load_weights(\"../models/results/transfomer2.h5\")\n",
    "pred2 = model2.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask shape after creating (?, 1, 1, 18)\n",
      "mask shape after creating (?, 1, 1, 18)\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "final output shape (?, 18, 20)\n",
      "mask shape after creating (?, 1, 1, 18)\n",
      "mask shape after creating (?, 1, 1, 18)\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "final output shape (?, 18, 20)\n"
     ]
    }
   ],
   "source": [
    "num_layers = 3 # hyperparameter\n",
    "d_model = 140 # 4*35\n",
    "num_heads = 20 # hyperparameter *must be a factor of d_model*\n",
    "d_ffn = 8 # hyperparameter\n",
    "pe_input = 18\n",
    "target_size = 20\n",
    "dropout_rate = 0.1 # hyperparameter\n",
    "\n",
    "model3 = transformer.TransRace(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ffn=d_ffn,\n",
    "    pe_input=pe_input,\n",
    "    target_size=target_size,\n",
    "    rate=dropout_rate,\n",
    "                              )\n",
    "loss = categorical_focal_loss(alpha=[alpha], gamma=0.5)\n",
    "model3.compile(\n",
    "    optimizer='Adam',\n",
    "    loss=loss,#'categorical_crossentropy', # 'sigmoid_focal_crossentropy'\n",
    "    metrics=['accuracy'], #['categorical_accuracy']\n",
    ")\n",
    "model3.build(input_shape=(None, X.shape[1], X.shape[2]))\n",
    "model3.load_weights(\"../models/results/transfomer3.h5\")\n",
    "pred3 = model3.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask shape after creating (?, 1, 1, 18)\n",
      "mask shape after creating (?, 1, 1, 18)\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "final output shape (?, 18, 20)\n",
      "mask shape after creating (?, 1, 1, 18)\n",
      "mask shape after creating (?, 1, 1, 18)\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "final output shape (?, 18, 20)\n"
     ]
    }
   ],
   "source": [
    "num_layers = 4 # hyperparameter\n",
    "d_model = 140 # 4*35\n",
    "num_heads = 20 # hyperparameter *must be a factor of d_model*\n",
    "d_ffn = 8 # hyperparameter\n",
    "pe_input = 18\n",
    "target_size = 20\n",
    "dropout_rate = 0.1 # hyperparameter\n",
    "\n",
    "model4 = transformer.TransRace(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ffn=d_ffn,\n",
    "    pe_input=pe_input,\n",
    "    target_size=target_size,\n",
    "    rate=dropout_rate,\n",
    "                              )\n",
    "loss = categorical_focal_loss(alpha=[alpha], gamma=0.5)\n",
    "model4.compile(\n",
    "    optimizer='Adam',\n",
    "    loss=loss,#'categorical_crossentropy', # 'sigmoid_focal_crossentropy'\n",
    "    metrics=['accuracy'], #['categorical_accuracy']\n",
    ")\n",
    "model4.build(input_shape=(None, X.shape[1], X.shape[2]))\n",
    "model4.load_weights(\"../models/results/transfomer4.h5\")\n",
    "pred4 = model4.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask shape after creating (?, 1, 1, 18)\n",
      "mask shape after creating (?, 1, 1, 18)\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "final output shape (?, 18, 20)\n",
      "mask shape after creating (?, 1, 1, 18)\n",
      "mask shape after creating (?, 1, 1, 18)\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "(?, 18, 140) mha\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 18, 20, 7) split_head\n",
      "(?, 20, 18, 7) q_shape\n",
      "(?, 20, 18, 18) scaled_attention_shape\n",
      "(?, 1, 1, 18) mask shape\n",
      "final output shape (?, 18, 20)\n"
     ]
    }
   ],
   "source": [
    "num_layers = 5 # hyperparameter\n",
    "d_model = 140 # 4*35\n",
    "num_heads = 20 # hyperparameter *must be a factor of d_model*\n",
    "d_ffn = 8 # hyperparameter\n",
    "pe_input = 18\n",
    "target_size = 20\n",
    "dropout_rate = 0.1 # hyperparameter\n",
    "\n",
    "model5 = transformer.TransRace(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ffn=d_ffn,\n",
    "    pe_input=pe_input,\n",
    "    target_size=target_size,\n",
    "    rate=dropout_rate,\n",
    "                              )\n",
    "loss = categorical_focal_loss(alpha=[alpha], gamma=0.5)\n",
    "model5.compile(\n",
    "    optimizer='Adam',\n",
    "    loss=loss,#'categorical_crossentropy', # 'sigmoid_focal_crossentropy'\n",
    "    metrics=['accuracy'], #['categorical_accuracy']\n",
    ")\n",
    "model5.build(input_shape=(None, X.shape[1], X.shape[2]))\n",
    "model5.load_weights(\"../models/results/transfomer5.h5\")\n",
    "pred5 = model5.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model6 = resnet.resrace(x.shape[1], 19)\n",
    "# model6.load_weights(\"model/win5_resrace_model_best6.h5\")\n",
    "# pred6 = model6.predict(x)\n",
    "\n",
    "# model7 = resnet.resrace(x.shape[1], 19)\n",
    "# model7.load_weights(\"model/win5_resrace_model_best7.h5\")\n",
    "# pred7 = model7.predict(x)\n",
    "\n",
    "# model8 = resnet.resrace(x.shape[1], 19)\n",
    "# model8.load_weights(\"model/win5_resrace_model_best8.h5\")\n",
    "# pred8 = model8.predict(x)\n",
    "\n",
    "# model9 = resnet.resrace(x.shape[1], 19)\n",
    "# model9.load_weights(\"model/win5_resrace_model_best9.h5\")\n",
    "# pred9 = model9.predict(x)\n",
    "\n",
    "# model10 = resnet.resrace(x.shape[1], 19)\n",
    "# model10.load_weights(\"model/win5_resrace_model_best10.h5\")\n",
    "# pred10 = model10.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108, 20)\n",
      "(108,)\n"
     ]
    }
   ],
   "source": [
    "log_pred1 = np.log(pred1)\n",
    "log_pred2 = np.log(pred2)\n",
    "log_pred3 = np.log(pred3)\n",
    "log_pred4 = np.log(pred4)\n",
    "log_pred5 = np.log(pred5)\n",
    "# log_pred6 = np.log(pred6)\n",
    "# log_pred7 = np.log(pred7)\n",
    "# log_pred8 = np.log(pred8)\n",
    "# log_pred9 = np.log(pred9)\n",
    "# log_pred10 = np.log(pred10)\n",
    "\n",
    "sum_pred = log_pred1 + log_pred2 + log_pred3 + log_pred4 + log_pred5 # + log_pred6 + log_pred7 + log_pred8 + log_pred9 + log_pred10\n",
    "sum_pred = sum_pred.reshape([-1, 20])\n",
    "print(sum_pred.shape)\n",
    "pred_order = np.argmax(sum_pred, axis = -1)\n",
    "print(pred_order.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(sum_pred, columns = ['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19'])\n",
    "pred_order_df = pd.DataFrame(pred_order, columns = [\"pred\"])\n",
    "summary = labels.join(pred_order_df)\n",
    "summary = pd.concat([summary,pred_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            race_id race_title  odds order horse_number goal_time half_order  \\\n",
      "0   202203010610&rf      喜多方特別   2.8     ?            1         ?          ?   \n",
      "1   202203010610&rf      喜多方特別   5.7     ?            2         ?          ?   \n",
      "2   202203010610&rf      喜多方特別  37.1     ?            3         ?          ?   \n",
      "3   202203010610&rf      喜多方特別  38.3     ?            4         ?          ?   \n",
      "4   202203010610&rf      喜多方特別  12.3     ?            5         ?          ?   \n",
      "5   202203010610&rf      喜多方特別   8.7     ?            6         ?          ?   \n",
      "6   202203010610&rf      喜多方特別  26.0     ?            7         ?          ?   \n",
      "7   202203010610&rf      喜多方特別  36.9     ?            8         ?          ?   \n",
      "8   202203010610&rf      喜多方特別   0.0     ?            9         ?          ?   \n",
      "9   202203010610&rf      喜多方特別  32.6     ?           10         ?          ?   \n",
      "10  202203010610&rf      喜多方特別  10.7     ?           11         ?          ?   \n",
      "11  202203010610&rf      喜多方特別  17.6     ?           12         ?          ?   \n",
      "12  202203010610&rf      喜多方特別  50.4     ?           13         ?          ?   \n",
      "13  202203010610&rf      喜多方特別   5.0     ?           14         ?          ?   \n",
      "14  202203010610&rf      喜多方特別  84.5     ?           15         ?          ?   \n",
      "15  202203010611&rf     吾妻小富士S  25.9     ?            1         ?          ?   \n",
      "16  202203010611&rf     吾妻小富士S  72.9     ?            2         ?          ?   \n",
      "17  202203010611&rf     吾妻小富士S   4.6     ?            3         ?          ?   \n",
      "18  202203010611&rf     吾妻小富士S   7.0     ?            4         ?          ?   \n",
      "19  202203010611&rf     吾妻小富士S  26.1     ?            5         ?          ?   \n",
      "\n",
      "   last_time horse_weight  pred  ...         10         11         12  \\\n",
      "0          ?            ?   1.0  ... -16.779406 -17.238365 -17.828606   \n",
      "1          ?            ?   2.0  ... -15.232748 -16.012079 -15.409253   \n",
      "2          ?            ?  12.0  ... -14.337939 -12.852651 -12.596569   \n",
      "3          ?            ?  11.0  ... -13.710320 -13.187420 -13.232402   \n",
      "4          ?            ?   2.0  ... -14.010800 -15.398794 -15.597191   \n",
      "5          ?            ?   2.0  ... -13.949691 -14.829822 -15.778109   \n",
      "6          ?            ?  13.0  ... -12.873108 -13.076397 -12.417642   \n",
      "7          ?            ?  12.0  ... -13.163271 -12.600834 -12.244012   \n",
      "8          ?            ?   3.0  ... -13.641312 -13.671386 -13.138392   \n",
      "9          ?            ?  13.0  ... -13.978645 -13.582285 -12.941132   \n",
      "10         ?            ?   2.0  ... -13.639531 -14.857815 -14.422045   \n",
      "11         ?            ?  10.0  ... -12.779116 -13.252111 -13.963959   \n",
      "12         ?            ?   6.0  ... -13.894507 -13.713909 -15.199560   \n",
      "13         ?            ?   7.0  ... -13.128565 -14.110445 -13.310759   \n",
      "14         ?            ?  13.0  ... -13.570911 -12.619193 -12.949348   \n",
      "15         ?            ?   3.0  ... -13.365101 -14.222000 -14.199286   \n",
      "16         ?            ?   3.0  ... -13.347180 -13.974306 -13.913390   \n",
      "17         ?            ?   3.0  ... -13.447437 -13.417374 -13.619051   \n",
      "18         ?            ?  14.0  ... -13.003506 -12.276046 -12.430906   \n",
      "19         ?            ?  14.0  ... -13.678011 -12.764095 -11.607038   \n",
      "\n",
      "           13         14         15         16         17         18  \\\n",
      "0  -19.789137 -20.102646 -22.939768 -23.605539 -48.246334 -57.273968   \n",
      "1  -16.260679 -18.068352 -19.827211 -21.335287 -44.391178 -57.402405   \n",
      "2  -13.166414 -15.300644 -16.768669 -18.912472 -41.780945 -51.019943   \n",
      "3  -13.562227 -15.575902 -14.680672 -18.397205 -41.557426 -48.492615   \n",
      "4  -14.781151 -16.362408 -18.098616 -20.573238 -46.553154 -50.585419   \n",
      "5  -15.222537 -17.379448 -17.518787 -19.148586 -44.841400 -54.958622   \n",
      "6  -11.526061 -13.631849 -14.675695 -15.469197 -39.613445 -51.079655   \n",
      "7  -13.918674 -14.239382 -14.540305 -17.073774 -40.513905 -51.870697   \n",
      "8  -14.765818 -15.285785 -16.051691 -18.380192 -41.564445 -52.314980   \n",
      "9  -12.733459 -14.853662 -15.723334 -16.675714 -42.559525 -51.880943   \n",
      "10 -13.606054 -16.350023 -18.446159 -20.190443 -45.184280 -57.427559   \n",
      "11 -13.708867 -15.052899 -15.599566 -17.619888 -41.336468 -48.226685   \n",
      "12 -15.000431 -17.317575 -18.253582 -19.378269 -44.836014 -52.759686   \n",
      "13 -12.852387 -16.155724 -17.100800 -18.955172 -42.994316 -53.193779   \n",
      "14 -11.992362 -13.013600 -14.225623 -15.617727 -40.158024 -46.177891   \n",
      "15 -13.932155 -15.272964 -17.329962 -18.200542 -45.175690 -54.495152   \n",
      "16 -13.879009 -14.946922 -16.940144 -18.021711 -44.862160 -54.515732   \n",
      "17 -13.616175 -15.109267 -17.184565 -18.183531 -44.967770 -54.592445   \n",
      "18 -13.334449 -12.238701 -13.988462 -16.630589 -35.474056 -37.680687   \n",
      "19 -11.682222 -11.489864 -12.935049 -13.750003 -33.837837 -39.473068   \n",
      "\n",
      "           19  \n",
      "0  -33.664593  \n",
      "1  -37.550247  \n",
      "2  -43.220032  \n",
      "3  -34.564114  \n",
      "4  -17.798843  \n",
      "5  -23.345289  \n",
      "6  -41.053818  \n",
      "7  -36.486225  \n",
      "8  -26.917141  \n",
      "9  -32.821793  \n",
      "10 -34.229820  \n",
      "11 -46.937252  \n",
      "12 -36.128506  \n",
      "13 -37.729500  \n",
      "14 -38.056183  \n",
      "15 -31.865084  \n",
      "16 -34.214882  \n",
      "17 -34.808472  \n",
      "18 -47.939598  \n",
      "19 -42.315308  \n",
      "\n",
      "[20 rows x 30 columns]\n",
      "    pred\n",
      "0      1\n",
      "1      2\n",
      "2     12\n",
      "3     11\n",
      "4      2\n",
      "5      2\n",
      "6     13\n",
      "7     12\n",
      "8      3\n",
      "9     13\n",
      "10     2\n",
      "11    10\n",
      "12     6\n",
      "13     7\n",
      "14    13\n",
      "15     3\n",
      "16     3\n",
      "17     3\n",
      "18    14\n",
      "19    14\n"
     ]
    }
   ],
   "source": [
    "print(summary.head(20))\n",
    "print(pred_order_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary[[\"race_title\",\"odds\",\"horse_number\",\"pred\",'1','2','3']].to_csv(\"../../weekly_prediction/2022/May_1/5_1_race_pred.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary[[\"pred\",'1','2','3']].to_csv(\"csv/data/jan_1/1_24_race_pred_v2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary[[\"pred\",'1','2','3']].to_csv(\"csv/data/jan_1/1_24_race_pred_v2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary[[\"pred\",'1','2','3']].to_csv(\"csv/data/jan_1/1_24_race_pred_v2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary[[\"pred\",'1','2','3']].to_csv(\"csv/data/jan_1/1_24_race_pred_v2.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
